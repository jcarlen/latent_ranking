\documentclass{article}
\setlength{\oddsidemargin}{0in} 
\setlength{\textwidth}{6.5in} 
\setlength{\evensidemargin}{0in}
\setlength\parindent{24pt}
\usepackage[font = small]{caption}
\captionsetup{width=.8\textwidth}
\captionsetup{labelfont=bf}

\usepackage[natbibapa]{apacite}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{array}
\usepackage{fancyvrb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{bbm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{colortbl, xcolor}

%from jss.cls
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

\title{A Latent Space Network Modelling Approach to Ratings with  \\
Applications to Journal and Film Rating}

\author{Jane Carlen \\ \emph{University of California, Los Angeles}}

%\blind{\date{DRAFT COPY\\DO NOT CIRCULATE\\\today}{}}
%\date{}

\begin{document}
\maketitle
\thispagestyle{empty}
%\pagestyle{empty}
\setcounter{page}{0}

\abstract{We introduce a method for rating items based on network data or incomplete pairwise comparison data, employing the latent space network models developed by \citet{hoff02}, \citet{hoff03} and \citet{krivitskyetal09}. Our model estimates positions of items in latent space, along with individualized sender and receiver coefficients that capture powers of transmission for each item. Ratings are derived from the difference in sender and receiver coefficients. The method is ideal for items with nebulous similarities which can exert a range of influences on the strength of ties. Unlike existing rating methods, the latent space network model provides both a measure of uncertainty in estimates and meaningful visualization.

We show that quasi-Newton estimation methods for this model produce results on par with commonly used MCMC methods in a fraction of the time. In particular, we develop a quasi-Newton algorithm  for this problem which performs well in applications. This extends the practical use of the latent space ranking method and of a class of latent space network models more generally. We present two applications. First, we rank statistics journals and show how the method addresses weaknesses of existing ranking methods. Second, we formulate movie ratings for films of several genres and demonstrate the use of latent space positions for genre detection. Our method is implemented in \proglang{R}, using the packages \pkg{latentnet} \citep{latentnet} and \pkg{visNetwork} \citep{visNetwork}}, and supplementary code which is provided. 

\newpage

\section{Introduction} \label{Introduction}

\subsection{Rating Methods for Network Data}\label{Rating Methods}
    
The task of rating items from network or incomplete pairwise comparison data is well studied in some settings, such as ranking web pages. Perhaps the best-known ranking algorithm for network data is Google's PageRank algorithm \citep{pagerank99}. Because it is generalizable, fast, and has a guaranteed solution, it has been applied in many settings, including biology, chemistry, ecology, neuroscience, physics, sports and computer systems \citep{gleich14}. In brief, it ranks pages by the eigenvector of the dominant eigenvalue of a Markov transition matrix which describes traffic flow among web pages. The rating corresponds roughly to the equilibrium amount of time an internet user would spend on a specific web page. It is worth noting that the development of PageRank was influenced by earlier work in citation analysis. \citet{pinski76} proposed a similar eigenvalue-based method for scoring journals, with an application to ranking physics journals. 
% wiki: "The eigenvalue problem was suggested in 1976 by Gabriel Pinski and Francis Narin, who worked on scientometrics ranking scientific journals [7] and in 1977 by Thomas Saaty in his concept of Analytic Hierarchy Process which weighted alternative choices.[8]"

The main advantage of a method like PageRank over raw count-based metrics (including Impact Factor, described below) is that references from highly rated pages (or journals) are more highly valued. As \citet{pagerank99} put it in an early paper, ``we give the following intuitive description of PageRank: a page has high rank if the sum of the ranks of its backlinks is high.'' This is crucial in ranking web pages where most linking pages are not of any interest to a user. However, it is not as important when ranking fairly homogeneous catalogs of items, and in that context can lead to overemphasizing popularity, as we will illustrate in Section \ref{Example1}. 
%Also the web is HUGE but the kinds of things I consider below are smaller enough to navigate, so while losing scalability isn't great, it's not the main focus. Also, new methods being developed to make this scalable? (http://papers.nips.cc/paper/4978-a-scalable-approach-to-probabilistic-latent-space-inference-of-large-scale-networks.pdf)

Eigenfactor \citep{eigenfactor07} is a method similar to PageRank, but tailored specifically to rank journals. The main differences are 1) the data is normalized, i.e., we model the percentage of citations journals send to each other instead of raw counts, and 2) rather than affording every journal a uniform minimum weight
%which ensures a solution and makes it so that if something is pointed to by a lot of sites it will come out decently important, even if those sites aren't important
this amount is scaled to the number of articles published by each journal. (For details of the calculation see \citeauthor{eigenfactor08}.) These changes reflect the greater uniformity and much smaller scale of the journal ranking problem as compared to web-site ranking. The Eigenfactor score can be viewed as the total influence of a journal. In tandem, the Article Influence score also proposed by \citet{eigenfactor07} is a measure of the ``average influence'' of an article in a given journal. It is proportional to the Eigenfactor score divided by the number of articles published by the journal. In Section \ref{Example1} we compare our journal ranking results to rankings by these methods. 
%Eigenfactor may be more appropriate for 'subscription decisions' than rank decisions, where Article Influence may be more appropriate to "measure the average influcence of articles appearing in the same journal" ... "proportional to the Eigenfactor divided by the number of articles." i.e. now penalizing for out citations

%Also:
% HITS uses hub and authority scores. I tried running hits against latent space and page rank results(?)
% not really appropriate for the journal ranking problem

Another area where ratings on incomplete comparison networks have been thoroughly studied is in ranking sport and gaming competitors. A full discussion of available methods is beyond the scope of this paper and we refer the reader to, for example, \citet{barrow} and \citet{stefani}. An example of a network-based ranking system developed for US college football is that of \citet{parknewman05}. %The network density in that application is about 10 percent \citep{parkyook14}, and about 75 percent of matchups are between teams in the same regional conference.
Their method calculates a \textit{win} score for each team as the exponentially decreasing sum of its wins, opponents' wins, opponents' opponents' wins, etc. A \textit{loss} score is analogously calculated and the final rank of the team is its win score minus its loss score. Their measure can be viewed as an extension of Katz centrality. %the authors say it, and see 7.14 of newman. this looks the possible extension of katz centrality given there.

%Further:
%2014 paper by park and Yook? (also sports)
%ecological network?
%elections?

There is a close relationship between rating objects in networks and measuring network centrality. All of the ratings methods we reference are closely connected to an established centrality measure. In particular, the Impact Factor (described in Section \ref{Example1}) is related to degree centrality, PageRank and Eigenfactor are related to eigenvector and Katz centrality; the Park and Newman method is also related to Katz centrality. 
%ImpactFactor - degree centrality
%PageRank - eigenvector centrality/katz centrality (vs katz: http://www.sci.unich.it/~francesc/teaching/network/pagerank)
%Park and Newman - Katz centrality
%HITS - Authority and hub centrality]
In contrast, the latent space method introduced below makes allowance for the fact that influence and centrality are not synonymous. An item may not be central, but may nonetheless have the ability to influence disparate items, and this is reflected in its rating.

\subsection{Applications of Latent Space Network Models}\label{Applications}

Latent space network models have been used for various applications, but not specifically for rating, as far as we know. For example, \citet{hoffward04} used a latent space model to visualize the structure of relationships between political actors in Central Asia. %it's bilinear though
\citet{gormley07} developed a latent space model for rank data and used it to co-locate voters and candidates in an Irish parliamentary election. \citet{sewellchen15} employed such a model to dynamic network data to study network stability and the relationship between popularity and stability. They subsequently extended their model to fit dynamic clusters \citep{sewellchen16}. 

Although latent space network models have not been used to rank authors or journals, as we do in Section \ref{Example1}, they have been applied previously to analyze citation networks. For example, \citet{sarkarmoore} developed a dynamic latent space model that can track the relationships between authors and their level of influence over time, which they illustrated on NIPS co-authorship data. Latent class network models have also been used to discover communities in citation networks, for example in \citet{leichtetal}, but the addition of latent positions  adds capability to identify externally mislabeled nodes, as we demonstrate in Section \ref{Example2}.

\subsection{Overview}\label{Overview}

The latent space network rating method we introduce incorporates the following features uniquely: 1) accounting for similarity between nodes, whether implicit or through covariates; 2) providing measures of uncertainty in estimates; 3) meaningfully and easily visualizing results; 4) distinction between influence and centrality; and 5) simple implementation in \proglang{R} \citep{r}. The method is applicable to directed networks, including those derived from pairwise comparison data. We focus on the case where the edges are valued and can be reasonably modeled as Poisson-distributed.

Section \ref{Model} of the paper details the latent space network model we employ and its estimation methods. Sections \ref{Example1} and \ref{Example2} describe two applications of the model, first to ranking statistics journals using citation data and second to rating films from several genres. In both applications we discuss the value added through network visualization. Section \ref{Discussion} concludes the paper with a discussion of benefits, limitations, and possible future improvements to the model.

\section{Latent Space Network Model for Rating} \label{Model}

We denote a network of $n$ nodes by its adjacency matrix $Y = \{y_{ij}\}, 1 \leq i,j \leq n$. A dyad in the network consists of two directed edges, $Y_{ij}$ and $Y_{ji}$. The latent space models introduced by \citet{hoff02} assume that nodes in a network have implicit positions in ``social space''. Given these $d$-dimensional positions, $Z$, as well as possible covariates, $X$, and corresponding parameters $\beta$, the probability of an edge is independent of all other edges. Thus, the probability of a graph $Y$ is the product over its edges

$$P(Y|X, \beta, Z) = \prod_{i\neq j} P(y_{ij}|x_{ij}, \beta, z_i, z_j).$$

\citet{hoff03} recast the parameters with unobserved random effects,

\begin{equation}\label{eq1}
P(Y|X, \beta, \gamma) = \prod_{i\neq j} P(y_{ij}| x_{ij}, \beta, \gamma_{ij}),
\end{equation}


where here we model $\gamma$ as in the ``distance model'' of \citet{hoff03}:

$$\gamma_{ij} = a_i + b_j + \epsilon_{ij},$$

\begin{equation}\label{eq2}
\epsilon_{ij} = f(z_i,z_j) = - \lVert z_i - z_j \rVert.
\end{equation}

We consider $a_i$ and $b_j$ to be node-specific sender and receiver effects. $\lVert z_i - z_j \rVert$ is the Euclidean distance between nodal positions $z_i$ and $z_j$. (\citet{hoff02} also considered an asymmetric projection model which we do not employ.) Although the positions can be in high-dimensional space, we usually consider one to three dimensions for reasons of interpretability, visualization, or parsimony. %Didn't see max on dimension in the package. other distances are theoretically possible but the ergmm package uses Euclidean distance, see "fitting position" article, or projection distance.


Adapting machinery of the generalized linear model (GLM), let

\begin{equation}\label{eq3}
E(y_{ij}) = g^{-1}(\eta_{ij}),\text{ where }\eta_{ij} = \beta' x_{ij} - \lVert z_i - z_j \rVert + a_i + b_j.
\end{equation}

\noindent In the context of our applications, we assume $y_{ij}$ is Poisson distributed and let $g$ be the standard log link function. In some cases other distributions may be more appropriate, but here we use the Poisson because we are dealing with count data. (A binomial model with constant trials and a Bernoulli model for binary networks can also be implemented in \pkg{latentnet}.) This is in contrast to the binomial distributions of edge weights in the quasi-Stigler model described in Section~\ref{QS}. Unlike the quasi-Stigler model the estimates are not conditioned on the total weight of each dyad ($y_{ij} + y_{ji}$). 

% latentnet doesn't allow for a unique number of trials in a binomial distribution on each node, but I may code that eventually and add to latentnet. Wouldn't fix dyad totals like quasi-Stigler though, so the interpretation wouldn't exactly make sense in context.
% Could also use optim to get a quick estimate with binomial likelihood.

We note some features of the model:

\begin{itemize}

\item Increasing distance between $z_i$ and $z_j$ implies decreasing expectation of $y_{ij}$. One way to view this is as controlling for similarity between nodes. Nodes with salient similarities are likely to have fitted positions relatively close together. Some of the magnitude of their connection is attributable to their similarly, and the rest to their individual sender ("push") and receiver ("pull") effects.

\item The effect of positions on expected edge weights is symmetric, affecting both edges in a dyad equally. Positions influence the model via the total dyad weight. To condition on the total weight, as in the quasi-Stigler model, eliminates the value of estimated positions.

\item The \textit{rating} or \textit{score} of node $i$ is its receiver minus sender coefficient,

\begin{equation}
rating_i = b_i - a_i. 
\end{equation}

Its \textit{rank} is derived from its order among the ratings.

\end{itemize}

\subsection{Parameter Estimation}\label{Parameter Estimation}

In this section we discuss two methods of parameter estimation for the latent space model. The first is a Markov chain Monte Carlo method of the type employed previously (see \citet{hoff02}, \citet{hoff03} and \citet{krivitskyetal09}, for example). The second is a quasi-Newton algorithm we have developed for this model. Both methods embed the model in a Bayesian framework and may use the same parameter initialization, but the update methods differ. We discuss first the overlapping aspects of the two methods, and then the particular aspects of each method in subsections \ref{MCMC} and \ref{QN}. 

Both estimation methods embed the model in the same Bayesian framework with corresponding dependence structure. The aim is to maximize the posterior distribution of the parameters, $P(\theta|Y) \propto P(Y|\theta)P(\theta)$. We adapt Equation \ref{eq1} for the case when the random effects are as described in (\ref{eq2}), and our only nodal covariate is the intercept term, $B_0$.

\begin{equation}\label{eq4}
P(\theta|Y) \propto P(Y|\theta)P(\theta) = \prod_{i\neq j} P(y_{ij}|\beta_0, z_i, z_j, a_i, b_j)P(\theta).
\end{equation}

%"A closed form expression for the desired conditional distribution is generally unavailable" \citep{hoff03} (When? Here it is available)

We posit independent normal distributions for the components of $\gamma_i$, $i = 1,....,n$ \citep{hoff03}.

$$a_i \sim N(0, \sigma_a^2))$$
$$b_i \sim N(0, \sigma_b^2))$$
$$z_i \sim MVN_d(0, I_d*\sigma_z^2))$$ %I switched mean from \mu to 0

We expand Equation \ref{eq4}:

\begin{equation}\label{eq5}
P(Y|\theta)P(\theta) = \prod_{i\neq j} P(y_{ij}|\beta_0, z_i, z_j, a_i, b_j)P(a|\sigma_a^2)P(b|\sigma_b^2)P(z|\sigma_z^2)P(\beta_0)P(\sigma_a^2)P(\sigma_b^2)P(\sigma_z^2)
\end{equation}

Equation \ref{eq5} reflects the dependence structure of the parameters as displayed in Figure \ref{fig:dep}. As stated above we assume $y_{ij}|\beta_0, z, a, b$ is Poisson distributed with mean parameter given by Equation \ref{eq3}, where $B = B_0$ and $g^{-1} = exp$.

\begin{figure}
  \centering
    \includegraphics[width=0.3\textwidth]{dependency_struct.pdf}
    \caption{Dependence structure of the latent space sender-receiver model.}
  \label{fig:dep}  
\end{figure}

We place priors on the elements of $\beta$, $\sigma_z^2$, $\sigma_a^2$ and $\sigma_b^2$.

$$\beta \sim N(\textbf{0}, I\sigma_\beta^2)$$ %ergmm default 0,9
$$\sigma_a^2 \sim \text{Scale-inv-}\chi^2 (v_a, s^2_a))$$ %ergmm default 3,1
$$\sigma_b^2 \sim \text{Scale-inv-}\chi^2 (v_b, s^2_b))$$ %ergmm default 3,1
%documentation for rsender/rreceiver says prior for variance of those effects has scale-inverse-chi-squared distribution which is equivalent to the inverse gamma described in the papers, just reparameterized.-->
$$\sigma_z^2 \sim \text{Scale-inv-}\chi^2 (v_z, s^2_z))$$ %ergmm determined by default function below -  


The estimation algorithm of \pkg{latentnet} sets default values for the hyperparameters to generate diffuse distributions on the prior parameters. The default value for $\sigma_\beta^2$ is $9$ to allow a wide range of $\beta$ values.   %hoff03 uses 100. Is wider better? 
Low degrees of freedom ($v_a$ = $v_b$ = $3$) reflect uncertainty in the values of $\sigma_a^2$ and $\sigma_b^2$, while the default scale parameters ($s^2_a$ = $s^2_b$ = $1$) curtail them to a wide but reasonable range. The default values for $v_z$ and $s^2_z$ are $\sqrt{n}$ and $\frac{1}{8}\sqrt[\leftroot{-3}\uproot{3}d/2]{n}$. These values reflect that larger networks tend to take up more space, but as observed network size increases the influence of prior variance should decline. For discussion of the choice of hyperparameters see \citet{krivitskyetal09} and \citet{latentnet_jss}. We employ the same default values for our quasi-Newton algorithm. These values are fixed throughout the estimation process. %these defaults assume no clusters in the model

<<r more_prior_notes, eval = F, echo = F, warning = F, message= F>>=
#InitErgmm.euclidean<-function(model, d, G=0, var.mul=1/8, var=NULL, var.df.mul=1, var.df=NULL, mean.var.mul=2, mean.var=NULL, pK.mul=1, pK=NULL)

# What is the motivation for these functions? Documented somewhere?

#1. Z.var - scale parameter of the inverse chi square prior on var of z's
#  if(!("Z.var" %in% names(model[["prior"]]))) model[["prior"]][["Z.var"]]<-model[["prior"]][["Z.var.mul"]]*(network.size(model[["Yg"]])/max(1,model[["G"]]))^(2/model[["d"]])
#DEFAULT: 1/8 * #vertices / max(1, #clusters)^(2/dimension)

#2. Z.var.df - dof parameter of the inverse chi square prior on var of z's
#  if(!("Z.var.df" %in% names(model[["prior"]]))) model[["prior"]][["Z.var.df"]]<-model[["prior"]][["Z.var.df.mul"]]*sqrt(network.size(model[["Yg"]])/max(1,model[["G"]]))
#DEFAULT: 1 * sqrt(#vertices) / max(1, #clusters)
# why sqrt? based on presumed density/dependence?

#3. Z.mean.var - variance of the gaussian prior on cluster means, if necessary
# if(!("Z.mean.var" %in% names(model[["prior"]]))) model[["prior"]][["Z.mean.var"]]<-model[["prior"]][["Z.mean.var.mul"]]*model[["prior"]][["Z.var"]]*max(1,model[["G"]])^(2/model[["d"]])
#DEFAULT:  2 * Z.var * max(1, #clusters)^(2/dimension)

#4. Z.pK - parameter (#clusters) of the dirichlet prior on cluster assignment, if nec.
#if(!("Z.pK" %in% names(model[["prior"]]))) model[["prior"]][["Z.pK"]]<-model[["prior"]][["Z.pK.mul"]]*sqrt(network.size(model[["Yg"]])/max(1,model[["G"]]))
#DEFAULT:  1 * sqrt(#vertices)/max(1,#clusters)
@

Both estimation methods described below require us to supply or generate initial parameter values. The default initializations for MCMC estimation, as implemented in \pkg{latentnet}, are functions of the observed network. This may speed convergence in some cases, but in our applications in low dimension we find that random initialization can perform as well or better. Our quasi-Newton method employs random initialization on a reasonable scale, without loss of speed or performance. \newline

\emph{MCMC Initialization}:

\begin{itemize}
\item $z^{(0)}$: The positions are initialized through either multidimensional scaling (MDS) or normal draws. In the former, the geodesic distances for all dyads are computed from the binary adjacency matrix $Y_b$. Disconnected pairs are given distances of $n$. The initial value $z^{(0)}$ is then computed by multidimensional scaling, returning optimal $d$-dimensional coordinates whose Euclidean distances best approximate the geodesic distances between nodes. In the latter, $z^{(0)}$ is generated via independent draws from a normal distribution. \citet{hoff02} noted that the choice of initialization does not impact their results. 

\item $\displaystyle \sigma_z^{2(0)} = var(z^{(0)})$

\item $\displaystyle a_i^{(0)} =  logit \left( \frac{Y_{b_{i \cdot}}  + 1}{n - 1 + 2} \right) - \frac{1}{n}\sum_{j=1}^{n} logit \left( \frac{Y_{b_{i \cdot}} + 1}{n - 1 + 2} \right)$

The initialization of sender parameters shown above is derived by considering initial nodal degrees as binomially distributed with $n-1$ trials, observed success probability $\frac{Y_{b_{i \cdot}}}{n-1}$, and a uniform prior on success probability. As a reminder, $Y_b$ denotes the binary adjacency matrix and $Y_{b_{i \cdot}}$ denotes the $ith$ row sum of $Y_b$. The initialization of receiver coefficients is analogous. %n-1 is the number of possible edges on each node. the +1(num), +2(denom) is as though a^(0) is a prediction/new draw of the number of edges when edges ~ binomially with total trials n-1, observed success prob rowsums/(n-1), and with a uniform prior on the success prob (see gelman p. 36)
% See ergmm.initvals, bayes.prop

\item $\displaystyle \sigma_a^{2(0)} = var(a^{(0)})$

\item $\displaystyle b_i^{(0)} = logit \left( \frac{Y_{b_{\cdot i}}  + 1}{n - 1 + 2} \right) - \frac{1}{n}\sum_{i=1}^{n} logit \left( \frac{Y_{b_{\cdot i}} + 1}{n - 1 + 2} \right)$

\item $\displaystyle \sigma_b^{2(0)} = var(b^{(0)})$

\item $\displaystyle \beta_0^{(0)} = logit\left( \frac{1}{n(n-1)}\sum_{i,i\neq j}\mathbbm{1}(y_{ij} > \overline{y_{ij}})\right) + \frac{1}{{n \choose 2}}\sum_{i,i<j}{\lVert z^{(0)}_i - z^{(0)}_j \rVert}$ 

The initial intercept is composed of an ``edge intercept,'' a valued-network analog of graph density, plus a ``distance intercept,'' the average initial pairwise distance.

% edge intercept + distance intercept
% logit(mean(indicator(Y_ij > mean(Y_ij))))
% See ergmm.initvals 
% Yg<- model[["Yg"]]
% Ym<-getYm(Yg,model[["response"]]) #getYm returns as.matrix.network(Yg, "citations", matrix.type="adjacency") with NA's on diag.
% Ym01<-Ym>mean(Ym,na.rm=TRUE)

% pm[["beta"]]<-logit(mean(Ym01,na.rm=TRUE))+if(!is.null(pm[["Z"]]))mean(dist2(pm[["Z"]])) else 0 #edge intercept + distance intercept

\end{itemize}

\emph{Quasi-Netwon Initialization}:

\begin{itemize}
\item $z^{(0)}$: Standard normal draws by default, or multidimensional scaling as for MCMC estimation. If MDS, the positions are scaled so that the maximum coordinate size is one, which has shown to improve performance. %As long as positions are scaled, doesn't seem senstive to which of these is chosen

\item $\displaystyle a^{(0)}, b^{(0)}$: Standard normal draws.

\item $\displaystyle \sigma_a^{2(0)}, \sigma_b^{2(0)}, \sigma_z^{2(0)}$: User-supplied value, defaults to 10.

\item $\displaystyle \beta_0^{(0)} = 0$.

\end{itemize}

\subsubsection{MCMC Estimation}\label{MCMC}

With the introduction of latent space network models, \citet{hoff02} developed an MCMC estimation algorithm to fit a model without random sender and receiver effects. \citet{hoff03} added the capacity to fit random effects, and \citet{krivitskyetal09} extended the model further by allowing clustering terms. In this section we present an overview of the MCMC estimation described by those authors, and embellish it with details from the \pkg{latentnet} implementation (\citep{latentnet_jss}, \citep{latentnet}). The process returns a sample from the posterior parameter distribution and desired point estimates, such as a maximum likelihood estimate, posterior mean and mode. 
\newline



\emph{MCMC Parameter Updates}:  \newline

Before starting an MCMC chain, \pkg{latentnet} employs an intermediate non-convex optimization step. It uses the bounded quasi-Newton optimization routine of \citet{lbfgsb}, as implemented in the \code{optim} function of \proglang{R}. This returns starting values for MCMC with higher posterior likelihood than the initial values. In Section \ref{??} we compare the gain in posterior likelihood from this optimization routine to the value added from the MCMC chain and to the results of our quasi-Newton algorithm. 

After intermediate optimization, the MCMC chain runs through a suitably long burn-in period. If the automated burn-in length is insufficient, proper length can be determined by carrying out MCMC diagnostics on the results, such as those in the \code{mcmc.diagnostics} method of \pkg{latentnet}. Once starting values for the MCMC sampling are determined, parameters are updated as follows:  

%what is the ordering/grouping of parameter updates? latentnet allows for group.deltas for proposing changes to multiple parameters simultaneously. When are they updated singly vs as a group?

%how does latentnet's adaptive sampling work, e.g. how are proposal dist. parameters updated?

\begin{itemize}

\item $\sigma^2_a, \sigma^2_b, \sigma^2_z$: 

The variances of the sender, receiver, and position parameters may be sampled directly from their posterior distributions because they were assigned conjugate priors. 

$$\displaystyle \sigma^2_a|a \sim \text{Scale-inv-}\chi^2 (v_a + n, \frac{v_a s^2_a + \sum_{i=1}^n{a^2_i}}{v_a + n}))$$

$$\displaystyle \sigma^2_b|b \sim \text{Scale-inv-}\chi^2 (v_b + n, \frac{v_b s^2_b + \sum_{i=1}^n{b^2_i}}{v_b + n}))$$

$$\displaystyle \sigma^2_z|z \sim \text{Scale-inv-}\chi^2 (v_z + n*d, \frac{v_z s^2_z + \sum_{i=1}^{n*d}{z_{i,d}^2}}{v_z + n*d}))$$

\item Actor-specific parameters, $a, b, z$:

The sender, receiver and position parameters cannot be sampled directly and may be strongly correlated. They are updated for each actor in random order by Metropolis-Hastings block updates.

  \begin{enumerate}
  %could be metrpolis since symmetric proposal distribution
  
  \item Propose $z_i^*, a_i^*, b_i^*$ from symmetric proposal distributions. %s, $J(z|z^{(t)})$.
  $$z_i^* \sim MVN_d(z_i, \tau_z^2I_d)$$
  $$a_i^* \sim N(a_i, \tau_a^2)$$
  $$b_i^* \sim N(b_i, \tau_b^2)$$

  \item Accept as a block with probability $min(1, \frac {P(Y|z^*, a^*, b^*, \beta) P(z^*)P(a^*)P(b^*)} {P(Y|z, a, b, \beta) P(z)P(a)P(b)}$).

  %\item $\theta\setminus z$: Metropolis-Hastings \citep{hoff03}.

  \end{enumerate}
  
\item $\beta$, shift of random effects, position scale:

  To speed convergence, a simultaneous shift in $\beta$ and the random effects and a rescaling of the positions is proposed. The magnitude of the shifts and multiplier is proposed by: 
  $$(h_\beta, h_a, h_b, h_z) \sim MVN_4(0, \tau_{\beta,a,b,z})$$
  $$ \beta^* = \beta +h_\beta $$
  $$ a^* = a +h_a $$
  $$ b^* = b + h_b $$
  $$ z^* = exp(h_z)z $$
  $$ \sigma_z^{2*} = exp(2h_z)\sigma_z^2 $$
  
  The move is block-accepted or rejected. For discussion of the acceptance probability see Section 3.2 of \citet{krivitskyetal09}.

  \item \textit{Proposal Variances} $\tau_z^2, \tau_a^2, \tau_b^2, \tau_{\beta,a,b,z}$:
  
  The variances of the proposal distributions are set adaptively during the burn-in period to stay near a fixed acceptance rate, with a default target rate of 0.234 (\cite{latentnet_jss} following \cite{nealandroberts}).
  
\end{itemize}


%This is just basic MH so I don't need to describe until I get more detail on the prosal distribution. [In \citep{shortreed06} proposal distributions are Gaussian centered at current values, $\theta^{(t)}$ with some standard deviation, $\delta_{\beta}$ or $\delta_z$. Is this still true in latentnet? If so, and since these are symmetric, is all done with Metropolis?]

%1. Propose $\theta^*$ from a proposal distribution, $J(\theta|\theta^{(t)})$ %[individual or group?]

%2. Compute acceptance probability $r = min(1, \frac{P(Y|\theta^{*})P(\theta^{*})J(\theta^{(t)}|\theta^{*})}  {P(Y|\theta^{(t)})P(\theta^{(t)})J(\theta^{*}|\theta^{(t)})})$ %lots of cancellation here if only updating one parameter at a time

%3. Set $\theta^* = \theta^{(t+1)}$ with probability $r$, otherwise $\theta^{(t+1)} = \theta^{(t)}$


\emph{MCMC Post-processing}:  \newline
  
  The likelihood depends on positions only through their pairwise distances, and is invariant to rotations, reflections and translations of the positions. We are interested in the posterior variance in positions that comes from changing distances between points rather than distance-preserving transformations. One way to address this and stabilize our estimates is to store not the sampled positions, but a transformed set that has minimal squared distance to a set of reference positions. This is the Procrustean transformation used by \citet{hoff02}. They let $z^*_{store} = argmin_{Tz^*}tr(z_{ref} - Tz^*)^\top(z_{ref} - Tz^*)$, where $T$ is the set of distance-preserving transformations.
  
There may also be strong correlation or near non-identifiability between a node's actor-specific parameters, especially if it is poorly connected. To reduce instability in the estimate \citet{shortreed06} considered the parameter estimate that is optimal in the sense of minimizing Bayes risk with a Kullback-Leibler (KL) loss function. Their ``MKL'' estimate minimizes the posterior expectation of the KL divergence from its predictive distribution of networks to the posterior predictive distribution of networks. As such, the MKL estimate only pertains to the parameters on which networks are immediately dependent.
\begin{equation}
\theta_{MKL} = argmin_{\widetilde{Z} \widetilde{a}, \widetilde{b}, \widetilde{\beta}} \left[ E_{Z, a, b, \beta|Y_{obs}}\left[\sum_Y log(\frac{P(Y|Z, a, b, \beta)}{P(Y|\widetilde{Z} \widetilde{a}, \widetilde{b}, \widetilde{\beta})})P(Y|Z, a, b, \beta)\right]\right]
\end{equation}
The MKL positions are more stable than standard point estimates because they average over all networks. They require the posterior sample to calculate, so unlike \citet{hoff02} the sampled positions are transformed with MKL positions as reference after the sampling is complete.

<<r invchisq, eval = T, results = 'hide', echo = F, cache = T, warning = F, message = F>>=
#took these from geoR package which was crashing

rinvchisq <- function (n, df, scale = 1/df) 
{
  if ((length(scale) != 1) & (length(scale) != n)) 
    stop("scale should be a scalar or a vector of the same length as x")
  if (df <= 0) 
    stop("df must be greater than zero")
  if (any(scale <= 0)) 
    stop("scale must be greater than zero")
  return((df * scale)/rchisq(n, df = df))
}

dinvchisq <-function (x, df, scale = 1/df, log = FALSE) 
{
  if (df <= 0) 
    stop("df must be greater than zero")
  if (scale <= 0) 
    stop("scale must be greater than zero")
  nu <- df/2
  if (log) 
    return(ifelse(x > 0, nu * log(nu) - log(gamma(nu)) + 
                    nu * log(scale) - (nu + 1) * log(x) - (nu * scale/x), 
                  NA))
  else return(ifelse(x > 0, (((nu)^(nu))/gamma(nu)) * (scale^nu) * 
                       (x^(-(nu + 1))) * exp(-nu * scale/x), NA))
}
@

%Estimation is implemented in the \code{ergmm} function of \pkg{latentnet} \citep{latentnet_jss}. 

\subsubsection{Quasi-Newton Estimation}\label{QN}

While the MCMC sample is useful for interval calculations, the rankings themselves rely only on a point estimate. A variety of strategies are available for this optimization problem, but here we present a quasi-Newton method that is fast, transparent, and performs well in applications. \newline

  
\emph{Parameter Updates}:  \newline

For each parameter we aim to update it to a local maximum of the posterior parameter distribution conditioned on all others. A Gibbs sampler cycles through parameters updates for $z, \sigma_z^2, \beta, a, \sigma_a^2, b, \sigma_b^2.$ The conditional variance parameters have closed-form updates. For parameters without closed-form updates, we take a step whose size depends on the first-order partial derivative and direction depends on the unmixed second-order partial. Derivative calculations are detailed in Appendix \ref{Appendix2}. If the step crosses a zero we continue to search within smaller and smaller intervals until finding the zero up to a pre-set tolerance, or until the stepsize falls below a user-specified $\epsilon$. Otherwise, we expand the search up to a window of, at most, twice the largest position coordinate. We update the positions synchronously due to potentially strong correlations between some positions. We cycle through parameter updates for a user-supplied number of cycles, and take as our final estimate the one achieved with the highest posterior probability. Our algorithm differs from common quasi-Newton methods primarily in it adjustments to stepsize and use of only unmixed second-order partial derivatives (the diagonal of the Hessian). The steps are detailed in Algorithm \ref{QNalgorithm}. \newline

%Advantage of gradient descent:
%- can pick out nodes causing problems due to lack of connectivity by non-converging derivative
%- know for sure and easily when it's converged to local opt.
%- widely used and accesible to a wide audience

\begin{algorithm}[!]\label{QNalgorithm}
\DontPrintSemicolon
\KwData{$Y, D, runs, \epsilon$}

Set hyperparameters (see Section \ref{Parameter Estimation}) and algorithmic parameters $\bf \Delta$, $\epsilon, tol, j_{max}, runs$.\;
Initialize model parameters $z, a, b, B_0, \sigma_a^{2}, \sigma_b^{2} \sigma_z^{2}$.\;

\While{$r \leq runs$} {
  $Z = z$; $Z$ stored and used for calculations, updated after all $z_{i,d}$ are.\;
  Update $z$:\;
  \For{$i\in permute(1:n)$}{
  
    \For{$d\in permute(1:D)$}{
    
      \While{$|\frac{\partial \mathbbm{llik}}{\partial z_{i,d}}| > tol~ \text{\bf and } j \leq j_{max}$}{
      
        $z^*_{i,d} = z_{i,d} + \Delta_{z_{i,d}}*sgn(\frac{\partial \mathbbm{llik}}{\partial z_{i,d}})*-sgn(\frac{\partial^2 \mathbbm{llik}}{\partial^2 z_{i,d}}$)\;
        
          \If{  $sgn(\frac{\partial \mathbbm{llik}}{\partial z^*_{i,d}}) \neq 
                 sgn(\frac{\partial \mathbbm{llik}}{\partial z_{i,d}})$}   {
                 
              $s = seq(z_{i,d}, ~z^*_{i,d}, ~length = 11)$ \;
              $z_{i,d} = argmin(|s - z_{i,d}| :
                         sgn(\frac{\partial \mathbbm{llik}}{\partial s}) \neq
                         sgn(\frac{\partial \mathbbm{llik}}{\partial z_{i,d}}))$\;
              update $\frac{\partial \mathbbm{llik}}{\partial z_{i,d}}$ and
                     $\frac{\partial^2 \mathbbm{llik}}{\partial^2 z_{i,d}}$ \;          
              $\Delta_{z_{d,i}} = \frac{1}{10}*\Delta_{z_{d,i}}$\;
    
              \If{$\Delta_{z_{d,i}} < \epsilon$}{\bf break}
              
          } \Else{
                  
              \If{$\Delta_{z_{d,i}} \leq max(|z|)$} {$\Delta_{z_{d,i}} = 2*\Delta_{z_{d,i}}$}
              
            } \Else { 
                
                If no local max is forthcoming, look in both directions\;
                $s = seq(z_{i,d} - \Delta_{z_{d,i}},
                         z_{i,d} + \Delta_{z_{d,i}}, ~length = 101) \setminus z_{i,d}$\;
                
                $\Delta_{z_{d,i}} = |argmin(|\frac{\partial \mathbbm{llik}}{\partial s}|) - z_{i,d}|$\;
                $z_{d,i} = |argmin(|\frac{\partial \mathbbm{llik}}{\partial s}|)$\; %update lower case z (not upper case Z yet)
                update $\frac{\partial \mathbbm{llik}}{\partial z_{i,d}}$ and
                       $\frac{\partial^2 \mathbbm{llik}}{\partial^2 z_{i,d}}$ \;  
              }
      j = j+1
      } 
    } 
  }
  $\sigma^2_z = \frac{v_z*s^2_z + \sum z^2}{n*D + 2 + v_z}$\;
  Update $\beta_0$:\;
  \While{$|\frac{\partial \mathbbm{llik}}{\partial \beta_0} > tol|$}{
  
      $B^*_0 = B_0 + \Delta_{B_0}*sgn(\frac{\partial \mathbbm{llik}}{\partial \beta_0})$\;
      \If {$sgn(\frac{\partial \mathbbm{llik}}{\partial \beta_0}  \neq
          sgn(\frac{\partial \mathbbm{llik}}{\partial \beta^*_0}))$} { 
          
          $s = seq(B_0, ~B^*_0, ~length = 11)$\;
          $B_0 = argmin(|s - B_0| :
                         sgn(\frac{\partial \mathbbm{llik}}{\partial s}) \neq
                         sgn(\frac{\partial \mathbbm{llik}}{\partial B_0}))$\;
          $\Delta_{B_0} = \frac{1}{10}\Delta_{B_0}$\;
          update $\frac{\partial \mathbbm{llik}}{\partial B_0}$\;
        
      } \Else {$\Delta_{B_0} = 2 * \Delta_{B_0}$}
    }
  Update $a_i, i \in permute(1:n)$  by same process as $\beta_0$\;
  $\sigma^2_a = \frac{v_a*s^2_a + \sum a^2}{n + 2 + v_a}$\;
  Update $b$ analogous to $a$\;
  \If {$llik(\theta_{current})>$ previous max} 
    {store $\hat{\theta} = \theta$}
}
\caption{Latent Space Network Quasi-Newton Algorithm\label{QNalg}}
\end{algorithm}

\emph{Post-processing}:  \newline

We check that all first derivatives at our estimate are within the pre-set tolerance near zero. We also look for any sign that this point is non-optimal by checking that the unmixed second-order partials for the coordinates are negative. For other parameters they are always negative (see Appendix \ref{Appendix2}). In trials, we found that the few coordinates with positive second derivatives at the estimate also had extremely low degree. We highlighted these points because they lack the necessary connectivity to accurately determine several correlated parameters, i.e., sender, receiver and coordinates. Thus, the algorithm output provides a test for the necessary connectivity on a case-by-case basis. This has a similar stabilizing effect to the MKL estimates produced from the MCMC output, though with the cost of a small amount of data. \newline

The main argument against using a quasi-Newton estimation method is that it is not guaranteed to converge to a global optimum since the likelihood function is not convex. However, there are several reasons why it is often still successful in practice. First, in applications well suited to latent space network rating the search space for the positions is relatively small. This is especially true when positions are in low dimension. Second, although MCMC estimation is theoretically guaranteed to converge to the true distribution, if the search space is large and the likelihood function jagged it may face prohibitively slow mixing time and fail to converge to the global optimum. It is easier to diagnose specific points of non-convergence in the quasi-Newton method using the stored derivatives. Third, the speed of the quasi-Newton method means we can consider many initial values to increase our chance of finding the a global optimum. Lastly, we are still able to use MCMC estimation to provide a check on our fit. Using the quasi-Newton estimate to seed the MCMC chain we can generate a useful sample from the posterior and may be able to improve on our results while still reducing the overall time to fit. In our trials on networks of up to several hundred nodes, results from quasi-Newton closely approximated those from MCMC.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETUP FOR APPLICATION SECTION %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<r setup, echo = F, include=FALSE, warning = F, message= F, cache = T, results = 'hide'>>=
knitr::opts_chunk$set(echo = FALSE, results = 'hide')
library(ergm)
library(ernm)
@

<<r setup2, cache = T, echo = F, results='hide', warning=F, echo = F, message=F>>=
library(latentnet)
library(ergm.count)
#load matrix

Cmatrix <- as.matrix(read.csv("~/Documents/citation/Citation_supplement/Data/cross-citation-matrix.csv", row.names = 1)) #47 x 47 #

#Self-citations are removed: The highest raw counts are from self-citations of CSDA (486) and StMed (628), which would seem to skew any analysis of importance. The authors exclude self-citations for the Stigler method: "Both the eigenfactor and the article influence score are computed over a 5-year time period, with journal self-citations removed to eliminate possible sources of manipulation."

Cmatrix.diag = Cmatrix #store a copy before removing diag
diag(Cmatrix) = rep(0,47) #shouldn't actually matter, as.network zeros out diag

#Note the use of the tranposed Cmatrix in the next line to correspond to standard i,j entry = citation FROM i to j. Original Cmatrix has i,j indicates citation from j to i.

Cnet = as.network(t(Cmatrix), directed=T, matrix.type="a", ignore.eval=F,  
       names.eval="citations") #as valued net, see \cite{krivitsky2015}

#as a binary network
Cbinet <- as.network(t(Cmatrix))

#cited/citing
cited = rowSums(Cmatrix) #citations in
citing = colSums(Cmatrix) #citations out
cite.ratio = cited/citing

#normalized
Cmatrix.norm = t(Cmatrix)/citing #entries are % of i's citations to j (column)
Cnet.norm = as.network(Cmatrix.norm, directed=T, matrix.type="a", ignore.eval=F,
            names.eval="citations")
@

<<r setup_jrss, cache = T, echo = F, results='hide', warning=F, echo = F, message=F, dependson = "setup">>=
#code from JRSS-PR-SA-Dec-13-0008_supplement.R to the paper
journal.abbr <- rownames(Cmatrix)

Tmatrix <- Cmatrix + t(Cmatrix)
diag(Tmatrix) <- diag(Cmatrix.diag)

journals.cluster <- hclust(d = as.dist(1 - cor(Tmatrix)))
#plot(journals.cluster, sub = "", xlab = "")
#cutree(journals.cluster, h = 0.6)  
library(BradleyTerry2)

Cdata <- countsToBinomial(Cmatrix)
fit <- BTm(outcome = cbind(win1, win2), player1 = player1, player2 = player2, data = Cdata)

npairs <- NROW(Cdata)
njournals <- nlevels(Cdata$player1)
phi <- sum(residuals(fit, "pearson")^2) / (npairs - (njournals - 1))

## 3.1 Journal residuals
journal.res <- rep(NA, njournals)
res <- residuals(fit, type = "pearson")
coefs <- c(0, coef(fit)) # 0 is the coefficient of the first journal
for(i in 1:njournals){
    A <- which(Cdata$player1 == journal.abbr[i])
    B <- which(Cdata$player2 == journal.abbr[i])
    y <- c(res[A], -res[B])
    x <- c(-coefs[Cdata$player2[A]], -coefs[Cdata$player1[B]])
    journal.res[i] <- sum(y * x) / sqrt(phi * sum(x ^ 2))
}
names(journal.res) <- journal.abbr

library(qvcalc)
cov.matrix <- matrix(0, nrow = njournals, ncol = njournals)
cov.matrix[-1, -1] <- vcov(fit)
qse <- qvcalc(phi * cov.matrix, estimates = c(0, coef(fit)),
              labels = journal.abbr)
#qse[["covmat"]] = cov.matrix*phi

export.scores <- qse$qvframe$estimate
export.scores <- export.scores - mean(export.scores)
names(export.scores) <- journal.abbr

sort.id <- sort(export.scores, decreasing = TRUE, index.return = TRUE)$ix
fit.table <- data.frame(quasi = export.scores[sort.id], qse = qse$qvframe$quasiSE[sort.id])
rownames(fit.table)
rownames(fit.table)[c(1,6,20)] = c("JRSS-B", "JRSS-A", "JRSS-C")
match(rownames(fit.table),Cnet%v%"vertex.names")
fit.table2 = fit.table[order(match(rownames(fit.table),Cnet%v%"vertex.names")),]
@

<< llik_and_latent_qn, cache = T, echo = F, results='hide', warning = F, message = F, dependson = c("setup_jrss", "latent_sr2_plot")>>=

# source necessary functions:

source("./latent_ranking_repo/ls_quasi_newton.R")

# contains, llik, log-likelihood function, off by a constant: 
# contains lsqn, the latent space quasi-netwon function

# helper function
dist2 <- function(Z) {
  as.matrix(dist(Z, upper = T))
}

@

In the next section we apply latent space rating to rank a set of statistics journals. We evaluate the results and compare them to existing journal-ranking methods.

%could potentially embed users in the same network...for later

\section{Ranking Statistics Journals Using Citation Data}\label{Example1}

In this section we examine rankings of 47 statistics and probability journals. The data set we consider was gathered and analyzed by \citet{varinetal} from Journal Citation Reports \citep{JCR}. It consists of a $47 \times 47$ matrix of directed citation counts, encompassing within-network citations from 2001 to 2010. We compare latent space model rankings to several competing methods in Section \ref{Comparison}, visualize our results in Section \ref{Visualization}, and evaluate competing models and estimation methods in Section \ref{modeleval}. 

Impact Factor is the most commonly referenced journal rating measure, despite widespread criticism \citep{if_research, if_abuse, if_poor}. %It is the only such measure listed on the sidebar of a journal's Wikipedia page.
Impact Factor measures how frequently articles from a specific journal are cited. An Impact Factor of $1.0$ means that articles published by that journal in the last two years have been cited once on average \citep{JCR}. Journal Citation Reports also publish modified versions of Impact Factor that exclude journal self-citations or alter the size of the time window to one or five years. These modifications address two problems with Impact Factor, but it has additional pitfalls as a proxy measure of journal quality. %It is fair to argue that authors should not be judged at all on the journal ranking (importance by association) but we're just not addressing that here. that's for another paper. %Aston further commented that this is ``inherently dangerous if weight is given to such metrics in crucial decisions such as grant awards or promotion and tenure cases'' \citep{varincomment}. 

% The Immediacy Index only averages over citations receved in the last year, while the IF5 expands the window to five years. The IFno excludes journal self-citations \citep{varinetal}. This addresses a major criticism of the impact factor, which is that journals can artificially inflate their score, whether intentionally or not, by encouraging cross-citations in published articles. Changing the size of the window for citations balances the criticism that articles may not achieve peak influence for several or more years, a lag that depends on the publication field, with the desire to include newer journals in the rankings and measure immediate impact.
%A 5-year Impact Factor is also published --  The 5-year journal Impact Factor is the average number of times articles from the journal published in the past five years have been cited in the JCR year. It is caclulated by dividing the number of citations in the JCR year by the total number of articles published in the five previous years.
%JCR Year (Journal Citation Report Year): The year of the JCR edition displayed in the top right-hand corner of the page. Each JCR year contains one year of citation data. You select the JCR year on the Welcome page.-->

<<r cite_cor, echo = F, results = 'hide', eval = T, cache = T, dependson = c("setup", "setup_jrss")>>=
ij = t(Cmatrix)[lower.tri(t(Cmatrix), diag = F)]
ji = Cmatrix[lower.tri(Cmatrix, diag = F)]
cor(ij, ji)
@

First, Impact Factor does not normalize for article length or out-citations. Whether planned or not, there is documented reciprocity in citations between journals. (In our data the correlation in one-way citations is $0.57$.) Second, it does not account for differences in citation patterns between fields, such as mathematics papers tending to have relatively few citations \citep{if_fraction}. Third, the distribution of citations counts by article is very long-tailed, with a few articles receiving many citations and most receiving only a few. As Colquhoun wrote in the Discussion on the paper by \citet{varinetal}, ``It has been obvious for a long time that it is statistically illiterate to characterize very skew distributions by their mean. And it is statistically illiterate to present point estimates with no indication of their uncertainty'' \citep{varincomment}. %latent space ranking uses totals, not means, and has a measure of uncertainty 
%https://www.timeshighereducation.com/news/citation-averages-2000-2010-by-fields-and-years/415643.article

\subsection{The Quasi-Stigler Model} \label{QS}
 
\citet{varinetal} introduced the \textit{quasi-Stigler} model to address the criticisms above. The second criticism is not accounted for by the model, but by restricting the data to only 47 out of 110 journals of statistics and probability. The quasi-Stigler model requires that journals are fairly homogeneous and have a relatively high level of citation exchange. %"A key requirement for the methods that are described here, as well as in our view for any sensible analysis of citation data, is that the journals jointly analysed should be as homogeneous as possible. Accordingly, analyses are conducted on a subset of the journals from the statistics and probability category, among which there is a relatively high level of citation exchange."" [@varinetal]

%The percent of citations to/from other statistics journals considered is roughly 15 to 60 percent. The median fraction of sent citations lost by subsetting from statistics journals is 4%; received is 7%. <!--of the statistics and probability category of JCR-->

The model measures each journal's ```propensity to export intellectual influence''' (\citeauthor{varinetal}, quoting Stephen Stigler). The rank of journal $i$ is determined by its \textit{export score}, $u_i$, under the assumption that citations counts, $C_{ij}$, are quasi-binomially distributed as follows:

\begin{equation}\label{eq4a}
\begin{aligned}
E(C_{ij}) &= t_{ij}\pi_{ij} \\
\pi_{ij} &= logit^{-1}(u_j - u_i) \\
& = \frac{exp(u_j - u_i)}{1 + exp(u_j - u_i)} \\
var(C_{ij}) &= \phi t_{ij} \pi_{ij} (1 - \pi_{ij})
\end{aligned}
\end{equation}

% \mu_i = b_i - a_i, \text{ where } C_{ij} \text{ is quasi-binomially distributed and } E(C_{ij}) = t_{ij}\exp{(a_i+b_j)}. 

\noindent where $t_{ij}$ is the observed total number of citations between journals $i$ and $j$, i.e.,~$t_{ij} = c_{ij} + c_{ji}$. The notation here is slightly different than in \cite{varinetal} due to transposition of the citation matrix $\{c_{ij}\}$. We note as \citeauthor{varinetal} that the export scores could be obtained as estimates from a quasibinomial GLM with logit link.
% see note in code chunk about how response and weights are set in R for glm, 

<<r glm, eval = T, cache = T, echo = F, results = 'hide', dependson= c("setup", "setup_jrss", "latent_sr1"), fig.keep='none', warning=F>>=

library("extraDistr") #for rbern

Tmatrix <- Cmatrix + t(Cmatrix)

# 1. Binomial GLM ####

# build design matrix - adjusted to not repeat observations, since they should be independnet

n = 47
k = 1
x1 = matrix(0, choose(n, 2), n)
for (i in 1:(n-1)) { #sender
  for (j in (i+1):n) { 
    x1[k,i]=1;
    x1[k,j]=-1;
    k = k + 1
    }
}
x1[,1] = 0 #forces first coef to NA, later set -> 0

#response 
y = as.vector(t(Cmatrix))/as.vector(Tmatrix)
y = y[which(lower.tri(Cmatrix))]
y[is.nan(y)]=0

# model
g1 = glm(y~x1-1,family = quasibinomial(link="logit"), weights = Tmatrix[lower.tri(Cmatrix)])
g1$coefficients[1] = 0
# Note that this is one way to set up the quasibinomial glm in R: 
# "As a numerical vector with values between 0 and 1, interpreted as the proportion of successful cases (with the total number of cases given by the weights)."
# AIC NA because fit by quasi-likelihood

# Results Identical to Sigler

cor(fit.table2$quasi, g1$coefficients, use="pairwise.complete.obs")
    
plot(fit.table2$quasi, g1$coefficients, ylab = "glm", xlab = "Stigler") 
    #shifted by -.7 = fit.table2$quasi[47]

summary(y-predict(g1)*as.vector(Tmatrix))

# Overdispersion - no need to estimate seperately - now agres with paper (after removing repeats from design matrix), 1.759
summary(g1)$dispersion

## 1a. Binomial GLM sender + receiver ####

#Setting it up with "sender"" and "receiver" effects in the design matrix gives basically the same results but it's superfluous, as the "sender" and "receiver" coefficents are perfectly negatively correlated (we're really fitting the differences).

#build design matrix:
x1a = matrix(0,n^2,2*n)
for (i in 1:n) { #sender
  for (j in 1:n) { 
    if (i != j) {
      x1a[n*(i-1)+j,i]=1;
      x1a[n*(i-1)+j,j+n]=1;
    }
  }
}

#g1a = glm(y~x1a-1,family = quasibinomial(link="logit"), weights = as.vector(Tmatrix))

#identical to Stigler up to scaling and loss of 1 DOF (last value NA) :
#cor(fit.table2$quasi, g1a$coefficients[1:47]-g1a$coefficients[48:94], use="pairwise.complete.obs") #=1

#"receiver" and "sender" coefficents perfectly negatively correlated.
#cor(g1a$coefficients[1:47], g1a$coefficients[48:94], use = "complete.obs")

# 2. Poisson GLM (compare to latent.srp1) ####

#response
y2 = as.vector(t(Cmatrix))

#model
g2a = glm.fit(x1a-1, y2, family = poisson(link="log"))
# matches latent.srp1. (where no option for weights or quasi)
# latent.srp1 was the sender receiver only model with fixed effects
# what if weighted and quasi? more appropriate? should these be options of ergmm?
# remember with binomial we have the added constraint that y_ij + y_ji = 1

#  g2a
# MKL S-R fit estimates almost match (shifted) latent.srp1 estiamtes
# Are the underlying models the same, just by different fits?
# '"glm.fit" uses iteratively reweighted least squares (IWLS)' (always same output)
# ergmm uses mcmc (output differs slightly each time)

#plot(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47],
 #    g2a$coefficients[48:94] - g2a$coefficients[1:47])
#cor(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47],
#    g2a$coefficients[48:94] - g2a$coefficients[1:47], use="complete.obs") #.999

#Poisson S-R GLM also close to Stigler:
cor(fit.table2$quasi, g2a$coefficients[1:47] - g2a$coefficients[48:94], 
    use="complete.obs")
plot(fit.table2$quasi, g2a$coefficients[1:47] - g2a$coefficients[48:94])

# 3. Simulate function for quasibinomial GLM | Quasi-Stigler using beta-binomial (used in later chunks) ####

# based on a quasi-binomial glm, return a list of matrices of predicted
simulate.qs <- function(g1, nsim = 10) {
  
  P = g1$fitted.values #or P = inv.logit(x1 %*% c(0, fit$coefficients)) 
  PW = g1$prior.weights
  phi = summary(g1)$dispersion
  
  # set alpha and beta to agree with above mean and var
  mean1 = PW*P
  var1 = phi*PW*P*(1-P)
  B = (1-P)/P
  D = phi*P*(1-P)
  alpha = (PW*B - D*(1+B)^2)/(D*(1 + B)^3 - (B+B^2))
  beta = alpha*(1-P)/P
  mean2 = PW*alpha/(alpha + beta) 
  net.dyads = length(g1$fitted.values)
  var2 = PW*alpha*beta*(alpha + beta +PW)/((alpha + beta)^2 * (alpha + beta +1))
  n = ncol(g1$model$x1)
  k = 1
  x1 = matrix(0, choose(n, 2), n)
  for (i in 1:(n-1)) { #sender
    for (j in (i+1):n) { 
     x1[k,i]=1;
      x1[k,j]=-1;
     k = k + 1
     }
  }
  x1[,1] = 0 #forces first coef to NA, later set -> 0
  
  # sample
  g = matrix(0, net.dyads, nsim)
  mats = list(nsim)
  for (i in 1:nsim) {
    sample1 = rep(0, net.dyads)
    for (j in 1:net.dyads) {
      #if (PW[i]==0) {return(0)}
      if (PW[j]==1) {sample1[j] = rbern(1, P[j])}
      if (PW[j] > 1) {sample1[j] = rbbinom(1, size = PW[j], 
                                           alpha = alpha[j], beta = beta[j])}
    }
    y1 = sample1/g1$model$`(weights)`
    y1[is.nan(y1)]=0
    g = glm(y1~x1-1, family = quasibinomial(link="logit"),
                weights = g1$model$`(weights)`)$fitted.values
    mat = matrix(0, n, n)
    mat[lower.tri(mat)] = (g1$model$`(weights)`)*g
    mat = t(mat)
    mat[lower.tri(mat)] = (g1$model$`(weights)`)*(1-g)
    mats[[i]] = t(mat)
  }
  return(mats)
  
}

@

Uncertainty in the export scores is conveyed through the \textit{quasi-variance}, $qvar_i$, of each $\mu_i$. The quasi-variances are estimated to minimize the difference between the true pairwise variances, $var(\hat{\mu}_i - \hat{\mu}_j)$, and a quasi-variance approximation, $qvar_i+qvar_j$. Although they could convey exact variances, the authors preferred quasi-variances and quasi-standard errors (QSE) because they can be succinctly displayed alongside export scores.

To connect the quasi-Stigler model to the latent space model, consider the quasi-symmetry formulation of the model (see \citeauthor{varinetal} (4), with some notation changes), where the export score is expanded $\mu_i = b_i - a_i$. As in the latent space model, $a_i$ and $b_i$ are sender and receiver coefficients, respectively. If we constrain $E(C_{ij}) + E(C_{ji}) = t_{ij}$, then the expectation term reduces to $ E(C_{ij}) = t_{ij} exp(a_i + b_j).$

% need to check the last statement again?
% Should the quasi-symmetry formulation have the additional constraint explicitly that E(c_ij) + E(c_ji) = T_ij,
% i.e. exp(a_i + b_j) + exp(a_j + b_i) = 1 ?

\subsection{Comparison of Journal Rankings} \label{Comparison}

We compare the rankings from our latent space model to others discussed. Unless otherwise stated, our results are based on two-dimensional MKL parameter estimates from MCMC estimation with the initialization strategy of our quasi-Newton method. These had the highest posterior probability and likelihood of any two-dimensional estimates considered. In Section \ref{modeleval} we provide a foundation for the choice of model dimension and estimation method. The \proglang {R} code used to implement our model and PageRank is included in the supplementary material. 

%%%%%%%%%%%%%%%%%%%
% CITATION MODELS %
%%%%%%%%%%%%%%%%%%%

%random initialization of sender and receiver didn't make a difference to likelihood
<<r latent_sr0, eval = T, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE, dependson = c("setup", "setup2"), highlight = FALSE>>=

latent.srp0 = ergmm(Cnet ~ rsender + rreceiver,
                    response = "citations", family = "Poisson.log", seed = 30,
                    tofit = c("mcmc", "mkl", "procrustes", "mle"),
                    control = ergmm.control(burnin = 100000, interval = 500,
                                            sample.size = 5000, mle.maxit = 50))
@

<<r latent_sr2, eval = T, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE, dependson = c("setup", "setup2"), highlight = FALSE>>=

t1.srp2 = Sys.time()
latent.srp2 = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
                    response = "citations", family = "Poisson.log", seed = 30,
                    tofit = c("mcmc", "mkl", "procrustes", "mle"),
                    control = ergmm.control(burnin = 500000, interval = 500,
                                            sample.size = 5000, mle.maxit = 100,
                                            pilot.runs = 10))
t2.srp2 = Sys.time()
# The \code{euclidean(d=2)} argument indicates that the latent positions are two-dimensional.
# reponse argument conveys a valued network 
# checked mcmc.diagnostics and they look fine
# mcmc.diagnostics(latent.srp2)
# Raftery Lewis diagnsotic suggests 10000 burnin, but I increased it because acceptance was low and there seemed to be a transition after the burnin period
# Far fewer than the default mle.maxit = 100 is needed

@

<< latent_qn, eval = T, echo = F, results = 'hide', cache=T, cache.comments = FALSE, warning= FALSE>>=

#best trial after several initiations
set.seed(12)

t1 = Sys.time()
latent_qn = lsqn(t(Cmatrix), runs = 50, tol = .01, Z.init = "rnorm")
    #best results with these settings ^
    #better to do more initializations with fewer runs than vice versa
t2 = Sys.time()

@

<< latent_sr2_init_qn, eval = F, results = 'hide', echo = F, cache=T, dependson = "latent_qn", cache.comments = FALSE, warning= FALSE>>=

# No particular gain from using quasi-Newton to initialize beyond gain from the random initialization method of QN
t1.init.qn = Sys.time()
latent.srp2.init.qn = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
       response = "citations", family = "Poisson.log", seed = 30,
       tofit = c("mcmc", "mkl", "procrustes", "mle"),
       control = ergmm.control(burnin = 50000, interval = 500,
                               sample.size = 5000, mle.maxit = 100),
       user.start = with(latent_qn$map, list(Z=Z, sender=sender, receiver=receiver,
                         beta = beta, sender.var = sender.var, receiver.var = receiver.var,
                         Z.var = Z.var)))
t2.init.qn = Sys.time()
@

<< latent_sr2_init_r, eval = T, results = 'hide', echo = F, cache = T, cache.comments = FALSE, warning= FALSE>>=

#random initation of sender and receiver, and scaling Z helps the model fit
D = 2; N = nrow(Cmatrix)
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

t1.init.r = Sys.time()
latent.srp2.init.r = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 500000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        pilot.runs = 10),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
t2.init.r = Sys.time()
@

<< latent_sr2_init_r.sann, eval = T, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=

latent.srp2.init.r.sann = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 50000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        optim.method = "SANN"),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
@

<< latent_sr2_init_r.bfgs, eval = T, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=
latent.srp2.init.r.bfgs = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 50000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        optim.method = "BFGS"),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))

@

% PageRank does do something that other methods i talk about don't, which is weight citations by importance of the citing journal.

<< pagerank, eval = T, cache = T, echo = F, results = 'hide',dependson = c("setup", "setup_jrss", "latent_sr2"), fig.keep = 'none', message = F, warning = F>>=

# "Page rank is the best-known technique for link-based importance ranking"
# https://www.cs.umd.edu/class/spring2008/cmsc828g/Slides/node-ranking.pdf

# PageRank has been applied to citation data:
# http://arxiv.org/pdf/0901.2640.pdf <- prob most useful, good summary
# http://arxiv.org/pdf/1012.4872.pdf (damping factor)
# http://onlinelibrary.wiley.com/doi/10.1002/asi.21452/epdf (weighted page rank)

# apply page rank ####
Cgraph = igraph::graph.adjacency(t(Cmatrix), weighted=T) #transpose so that i,j indicates citation from i to j.
#get.edge.attribute(Cgraph, "weight")
Cgraph.page.rank = igraph::page.rank(Cgraph, damping = .95)$vector #"If weights arg is NULL and the graph has a weight edge attribute then that is used."
# note, EXTREMELY fast
#       page rank normalizes so doing that wouldn't change results

# compare results to other ranks ####

## to cited/citing ratio
plot(Cgraph.page.rank, cite.ratio)
cor(Cgraph.page.rank, cite.ratio) #.68

## to quasi-stigler

### scores
plot(Cgraph.page.rank, fit.table2$quasi, type = "n")
text(labels = names(Cgraph.page.rank), x = Cgraph.page.rank, y = fit.table2$quasi, cex = .5, xlab = "PageRank", ylab = "quasi-Stigler", srt = -45)
cor(Cgraph.page.rank, fit.table2$quasi) #.659 with damping at .95, and .65 with damping at .85

### ranks
temp = data.frame(row.names = names(Cgraph.page.rank))
temp[,1] = rank(Cgraph.page.rank)
temp[,2] = rank(fit.table2$quasi)
plot(temp, type = "n", xlab = "PageRank", ylab = "quasi-Stigler")
text(labels = rownames(temp), x = temp[,1], y = temp[,2], cex = .5)

### raw counts
cor(Cgraph.page.rank, colSums(t(Cmatrix))) #.985 - correlates very highly with number of  citations received
plot(Cgraph.page.rank, colSums(t(Cmatrix)))
#plot(vc2, colSums(t(Cmatrix))) how would latent space compare?
@

<<r Eigenfactor, eval = T, echo = F, cache = T, results = 'hide', fig.keep = 'none', dependson = c("latent_srp2")>>=

# http://www.eigenfactor.org/projects/journalRank/rankings.php?search=XY&year=2010&searchby=isicat&orderby=Eigenfactor
# 2010, going back five years

EF = c(0.004,0.003,0.035,0.002,0.008,0.005,0.02,0.018,0.012,0.004,0.003,0.006,0.002,0.022,0.002,0.003,0.002,0.002,0.04,0.002,0.004,0.007,0.011,0.002,0.007,0.021,0.004,0.003,0.017,0.007,0.004,0.002,0.002,0.006,0.006,0.006,0.002,0.038,0.005,0.001,0.001,0.002,0.011,0.008,0.007,0.006,0.003)

AI = c(.9, .7, 3.3, .6, 1.6, .8, 1.6, 2.4, 2.3, 1.2, .3, .3, .5, .8, .9, .6, .6, .7, 3.3, .3, .6, 1.6, .9, .5, 1.8, 4.8, 1.0, .4, .6, 1.7, .9, .9, .5, 1.4, 2.0, 1.8, .5, 1.3, 1.5, .8, .5, .4, .4, 3.4, 1.0, 1.4, 1.2)

names(EF) =  names(AI) = c('AmS','AISM','AoS','ANZS','Bern','BioJ','Bcs','Bka','Biost','CJS','CSSC','CSTM','CmpSt','CSDA','EES','Envr','ISR','JABES','JASA','JAS','JBS','JCGS','JMA','JNS','JRSS.A','JRSS.B','JRSS.C','JSCS','JSPI','JSS','JTSA','LDA','Mtka','SJS','StataJ','StCmp','Stats','StMed','SMMR','StMod','StNee','StPap','SPL','StSci','StSin','Tech','Test')

par(mfrow = c(2,2))
#vs latent 
plot(fit.table2$quasi, EF, main = as.character(round( cor(fit.table2$quasi, EF), 3)), type ="n")
text(names(EF), x = fit.table2$quasi, y = EF, cex = .6) 
#the fact that StMed scores very highly on eigenfactor is a knock against it. StMed is middle of the road in cited/citing ratio and latent score and pagerank
plot(fit.table2$quasi, AI, main = as.character(round( cor(fit.table2$quasi, AI), 3)), type ="n")
text(names(AI), x = fit.table2$quasi, y = AI, cex = .6) 

#vs page rank
#PageRank does not directly penalize out-links, even though journals which cite much more often than they are cited are less likely to be highly regarded. 

plot(Cgraph.page.rank, EF, main = as.character(round( cor(Cgraph.page.rank, EF), 3)), type ="n")
text(names(EF), x = Cgraph.page.rank, y = EF, cex = .6) 

plot(Cgraph.page.rank, AI, main = as.character(round( cor(Cgraph.page.rank, AI), 3)), type ="n")
text(names(AI), x = Cgraph.page.rank, y = AI, cex = .6) 

# compared to latent and colored by cluster
#par(mfrow = c(1,1))
#plot(vc2, AI, main = as.character(round( cor(vc2, AI), 3)), type ="n")
#text(names(AI), x = vc2, y = AI, cex = .9, col = cutree(journals.cluster, h = 0.6)+3) #shows AI seems to place "general" and "computational" publications lower as opposed to latent method (because light blue color at the bottom)

# Eigenfactor correlattion to ratio of in to out citations
cor(EF, colSums(t(Cmatrix))/rowSums(t(Cmatrix)))
@

<<r IF, eval = T, echo = F, cache = T, results = 'hide', fig.keep = 'none', warning = F, message = F >>=

#This is taken from Table 4 of varinetal, via their journal-scores.csv file in their supplement folder. BUT not all the names match up so I made the journal-scores_edited file. 

IF1 = c(0.981,
0.966,
2.94,
0.618,
1,
1.438,
1.764,
1.833,
2.769,
0.689,
NA,
0.351,
0.5,
1.089,
NA,
0.75,
0.86,
0.722,
2.063,
0.306,
1.073,
1.206,
1.01,
NA,
2.57,
3.5,
0.645,
0.469,
0.691,
NA,
0.678,
0.873,
0.584,
0.835,
NA,
1.851,
0.519,
2.328,
1.768,
0.714,
0.322,
NA,
0.443,
2.48,
0.956,
1.56,
NA)

#Same as above, but with 2010 missing values supplied from https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
IF2 = c(0.981,
0.966,
2.94,
0.618,
1,
1.438,
1.764,
1.833,
2.769,
0.689,
0.343, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.351,
0.5,
1.089,
1.645, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.75,
0.86,
0.722,
2.063,
0.306,
1.073,
1.206,
1.01,
0.455, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
2.57,
3.5,
0.645,
0.469,
0.691,
2.647, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.678,
0.873,
0.584,
0.835,
2.0, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
1.851,
0.519,
2.328,
1.768,
0.714,
0.322,
0.595, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.443,
2.48,
0.956,
1.56,
1.036) #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?

#From varinetal Table4. Filling in missing values would require JCR login
IFno = c(0.752,
0.966,
2.573,
0.582,
0.964,
1.278,
1.601,
1.686,
2.615,
0.676,
NA,
0.311,
0.5,
0.815,
NA,
0.707,
0.8,
0.667,
1.929,
0.281,
0.76,
1.137,
0.816,
NA,
2.354,
3.427,
0.566,
0.429,
0.594,
NA,
0.632,
0.836,
0.573,
0.813,
NA,
1.743,
0.519,
2.072,
1.725,
0.686,
0.305,
NA,
0.356,
2.08,
0.889,
1.387,
NA)

IF2.rank = rank(-IF2, na.last = "keep")
IFno.rank = rank(-IFno, na.last = "keep")
@

<<r HITS, eval = F, echo = F, cache = F>>=
library(igraph)

# http://stackoverflow.com/questions/29911300/how-to-get-the-hits-function-in-r-tool
HITS<-function(g,k)  { 
    adj <- g
    nodes <- dim(adj)[1] 
    auth <- c(rep(1,nodes)) 
    hub <- c(rep(1,nodes)) 
    for(i in 1:k){ 
        t_adj <- t(adj) 
        auth <- t_adj%*%hub 
        hub <- adj%*%auth 
        sum_sq_auth <- sum(auth*auth) 
        sum_sq_hub <- sum(hub*hub) 
        auth <- auth/sqrt(sum_sq_auth) 
        hub <- hub/sqrt(sum_sq_hub) 
    } 
    result <- data.frame(auth = auth,hub = hub)   
    return(result) 
}

hits <- HITS(Cmatrix, 100)
par(mfrow = c(2,2))
cor(vc2, hits$auth) #not similar
cor(vc2, hits$hub) #a bit similar
plot(vc2, hits$hub)
cor(Cgraph.page.rank, hits$auth) #a bit simlar
plot(Cgraph.page.rank, hits$auth) 
cor(Cgraph.page.rank, hits$hub) #very similar
plot(Cgraph.page.rank, hits$hub)
cor(Cgraph.page.rank, hits$auth+hits$hub) 
plot(Cgraph.page.rank, hits$auth+hits$hub) 
@

<<r summarytable, eval = T, cache = T, echo = F, results = 'asis', dependson = c("setup", "setup_jrss", "latent_sr2", "pagerank", "Eigenfactor", "IF"), warning = F, message = F >>=

# varintable <- read.csv("/Users/jac/Documents/citation/Varin_Cattelan_Firth_supplement/Data/journal-scores.csv")
# can't use this table directly it includes too many journals

summarytable_rate = round(data.frame("Latent Space" = latent.srp2$mkl$receiver -
                                       latent.srp2$mkl$sender,
                          "quasi-Stigler"= fit.table2$quasi,
                          "PageRank" = Cgraph.page.rank,
                          "Eigenfactor" = EF,
                          "Article Influence" = AI,
                          "Impact Factor" = IF2),3)

summarytable_rank = data.frame(
                          "Latent Space" = order(order(latent.srp2.init.r$mkl$receiver -
                                        latent.srp2.init.r$mkl$sender, decreasing = T)),
                          "quasi-Stigler"= order(order(fit.table2$quasi, decreasing = T)),
                          "PageRank" = order(order(Cgraph.page.rank, decreasing = T)),
                          "Eigenfactor" = order(order(EF, decreasing = T)),
                          "Article Influence" = order(order(AI, decreasing = T)),
                          "Impact Factor" = IF2.rank)

rownames(summarytable_rank) = rownames(summarytable_rate) 

library(xtable)
summary_xtable_rank <- xtable(summarytable_rank, caption = "Compared Rankings", digits = 0, label = "summarytable_rank")
   
summary_xtable_rate <-xtable(round(cor(summarytable_rate),2), caption = "Correlation Table of Journal Ratings Methods", label = "correlation_table")

col1 <- rep("\\rowcolor[gray]{0.9}", 4)
print.xtable(summary_xtable_rank, booktabs = TRUE,
             table.placement = getOption("xtable.table.placement", "!"),
             add.to.row = list(pos = as.list(c(2, 7, 25, 18)), command = col1),
             sanitize.colnames.function=function(x)gsub("\\."," ",x),
             caption.placement = "top")

print.xtable(summary_xtable_rate, sanitize.colnames.function=function(x)gsub("\\."," ",x), caption.placement = "top")   
@

Table \ref{summarytable_rank} compares journal rankings from the latent space model, quasi-Stigler model, PageRank, Eigenfactor, Article Influence, and Impact Factor.
%Explain how I fit all these/whether the data's from. Include code for latent space model?
Comparisons to other versions of the Impact Factor can be found in Table 4 of \citeauthor{varinetal} for a slightly different data set. Table \ref{summarytable_rank} presents ranks rather than ratings to facilitate comparisons across the methods. According to \citet{varinetal}, there is ``diffuse opinion'' among statisticians that the most prestigious statistics journals are, in alphabetical order, \textit{Annals of Statistics} (AoS), \textit{Biometrika} (Bka), the \textit{Journal of the American Statistical Association} (JASA) and the \textit{Journal of the Royal Statistical Society}, Series B (JRSS-B). (These journals have gray background in Table \ref{summarytable_rank}.) Accordingly, they argue that a good rating method will put them near the top.

Although PageRank ranks the ``big four'' journals highest, it places \textit{Statistics in Medicine} (StMed), \textit{Journal of Statistical Planning and Inference} (JSPI), and \textit{Computational Statistics and Data Analysis} (CSDA) in positions six through eight, much higher than most other methods. These journals have the three highest out-citation counts in our data, and are among the most prolific citers of the top four journals. However, their ratios of in- to out-citations rank 20th, 30th, and 35th. Eigenfactor behaves similarly to PageRank, with some differences reflecting its use of normalized data. (Eigenfactor is strongly correlated with PageRank, $0.91$, as shown in Table \ref{correlation_table}.) We conclude that PageRank and Eigenfactor are better measures of centrality or activity than importance, influence or prestige. 

The Impact Factor rankings show the detriment of averaging per article and not controlling for out-citations or a citation's field of origin. For example, \textit{Environmental and Ecological Statistics} (EES) ranks 14th and the \textit{Journal of Statistical Software} (JSS) ranks 4th, though they have the second- and ninth-lowest in-citation counts in our data. On the other hand, the ``big four'' journals Bka and JASA are ranked 11th and 8th. The Impact Factor ratings are most strongly correlated with Article Influence ($0.87$, see Table \ref{correlation_table}), which is also normalized by articles per journal and calculated using all citations, not just ones from the $47$ journals in our data set.  %I checked and pagerank calculated on normalized data and just the 47 jounals is pretty similar to Eigenfactor.
The two methods share some anomalous rankings, such as \textit{Statistical Science} placing 2nd and 6th respectively. We argue that the high rank for this journal is not reflective of its importance within the field. Rather, it owes to its connections to top journals (confirmed by PageRank and Eigenfactor ranking it 13th) and a relatively low number of articles published. Its ratio of in- to out-citations ranks 24th in the network. As a review journal it is more likely to dissemenate than publish cutting-edge research. The visualizations provided by the latent space model help to further explain the position of \textit{Statistical Science}, which will be revisited below.
%The journal's founding editor stated that a central goal of the journal is ``presenting the full range of contemporary statistical thought at a modest technical level accessible to the wide community of practitioners, teachers, researchers and students of statistics and probability'' \citep{degroot86}.

The Latent Space and quasi-Stigler rankings are very similar (correlation $0.99$, see Table \ref{correlation_table}) and seem to provide the best measures of influence or importance in the field. They rank the ``big four'' journals in the top four positions, the \textit{Journal of Statistical Software} 42nd, and \textit{Statistical Science} 19th and 18th respectively. On the other hand, the methodological journal \textit{Scandinavian Journal of Statistics} (SJS) is highly ranked by both, at 6th and 8th respectively. SJS states its mission as ``reporting significant and innovative original contributions to statistical methodology, both theory and applications'' \citep{SJS}. \citet{varinetal} use data from the UK Research Assessment Exercise (RAE), a periodic evaluation of UK university departments, as an external check on the quasi-Stigler rankings. They find some evidence that the quasi-Stigler model provides stronger correlation to RAE assessment of research quality than other methods. Details of that comparison, and its many caveats, are found in Section 6 of \citet{varinetal}.

\subsubsection{Comparison of Latent Space and Quasi-Stigler Model Output}\label{Comparison1}

As the quasi-Stigler and latent space models emerge as the best suited to our ranking priorities, we compare their results most closely. Figure \ref{fig:r latent2_stigler} (left) plots the compared ranks, with lighter labels for larger differences in rank. Three is the highest observed difference. However, the underlying differences in scores are very small, as shown in the right panel of Figure \ref{fig:r latent2_stigler}. Figure \ref{fig:r latent2_joy} plots the posterior distributions of latent space scores. It confirms that the small differences in rank shown in Figure \ref{fig:r latent2_stigler} are not significant given the uncertainty in the estimated scores, highlighting the importance of capturing model uncertainty.

<<r latent2_stigler, eval = T, echo = F, cache = T, warning = F, message = F, fig.height = 3, fig.width = 7, results = 'hide', dependson= c("setup", "latent_sr2"), fig.cap = 'Left: Latent space vs. quasi-Stigler rankings. Better-ranked journals are at the top right, corresponding to higher numbers. Right: Comparison of scores rather than rankings. Lighter labels means larger differences (maximum rank difference is 3).'>>=

par(mfrow = c(1,2))
par(mai = c(.8,.8,.8,.4))
# compare ranks ####

srs2 = data.frame(quasi.Stigler = fit.table2$quasi,
             latent_space = latent.srp2.init.r$mkl$receiver -latent.srp2.init.r$mkl$sender)
rownames(srs2) = Cnet%v%"vertex.names"
srs2b = srs2
srs2b[,1] = rank(srs2b[,1])
srs2b[,2] = rank(srs2b[,2])
srdiff = srs2b[,1]-srs2b[,2]; #latent space - stigler.

## plot rank comparison ####
cr = grDevices::colorRampPalette(c("black", "gray"))( 4 )
col1 = rep(cr[1], 47)
col1[abs(srs2b[,1]-srs2b[,2]) >.5 ] = cr[2]
col1[abs(srs2b[,1]-srs2b[,2]) >1.5 ] = cr[3]
col1[abs(srs2b[,1]-srs2b[,2]) >2.5 ] = cr[4]

col1 = abs(srs2b[,1]-srs2b[,2])
rc = ggplot(data = srs2b, aes(x = quasi.Stigler, y = latent_space,
                         label = rownames(srs2b), col = col1)) +
    geom_text(angle = -45, size = 2.5) + 
    scale_color_gradient(low = "black", high = "gray", guide = F) + 
    theme_bw() +
    ylab("2D Latent Space") +
    ggtitle(label = "Rank Comparison")

sc = ggplot(data = srs2, aes(x = quasi.Stigler, y = latent_space,
                         label = rownames(srs2b), col = col1)) +
    scale_color_gradient(low = "black", high = "gray", guide = F) + 
    geom_point() +
    theme_bw() +
    ylab("2D Latent Space") +
    ggtitle(label = "Score Comparison")

cowplot::plot_grid(rc, sc)
@

<<r latent2_joy, eval = T, echo = F, cache = T, warning = F, message = F, fig.pos='t!', fig.height = 5, fig.cap = 'Posterior distributions of latent space score. Most small differences in ratings are not signficiant.'>>=

library(ggplot2)
library(ggjoy)

# scores:
latent2 = latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender

# Posterior distribution of scores:
quantiles2 = apply(latent.srp2.init.r$sample$receiver - latent.srp2.init.r$sample$sender,
              2, quantile, seq(0,1,length.out = 101))
o = order(quantiles2[51,])
quantiles2 = quantiles2[,o]
tmp = data.frame(score = as.vector(quantiles2), node = as.factor(rep(1:47, each = 101)))

# Plot
ggplot(tmp, aes(x = score, y = node)) + geom_joy() + 
  ggtitle("Posterior Distributions of Latent Space Scores") +
  scale_y_discrete(name ="journal & point estimate",
                   labels= apply(cbind(rownames(Cmatrix)[o], sort(round(latent2, 2))),
                                 1, paste, collapse = "  "))

@

Uncertainty in ratings is very similar between the two models, with quasi-Stigler standard deviations being 0.04 smaller on average, and at most 0.069 smaller. Under both models, \textit{Stata Journal} (StataJ) has the largest standard error, which is due to the fact that it has by far the fewest in- and out-citations. As expected, standard error of scores and citation counts are inversely correlated. Figure \ref{fig:r latent2_centipede} shows point estimates from each model in intervals of $1.96$ estimated standard deviations in either direction. On the left, the variances of the latent space scores are calculated from a sample of 5000 draws from the posterior distribution of parameters stored during MCMC estimation. On the right, the longer intervals are calculated from standard errors extracted from the scaled covariance matrix of the model. The interval for \textit{American Statistician} (AmS) is missing because its coefficients were fixed at zero for identifiability. The interior intervals are 1.96 quasi-standard errors (QSE) in each direction. These ``comparison intervals" are analogous to those in Figure 4 of \citet{varinetal}. We see that they are smaller and more variable than the true standard error intervals. The justification by \citeauthor{varinetal} to present uncertainty through quasi-standard errors is that they can be listed alongside estimates in a table, and allow a familiar Pythagorean estimate of the standard error of a difference of two export scores.  However, in a centipede plot as shown, the true standard errors are just as compact and easy to compare, and reveal that the uncertainty in scores is roughly equivalent between the two models. 

<<latent_binomial, eval = F, echo = F, cache = T, warning = F, results = 'hide'>>=

# the pi's fit by quasi-stigler are as good as when latent component added
# the fit z's for the binomial are very close to 0 
# one way we can interpret this as: total citations exchanged
# between two journals depends on positions, but given that total, 
# the specific taken by each can be modeled with just sender/receiver

theta = optim(c(a, b, B, Z),
                   Y = t(Cmatrix), d = 2, family = "binomial", llik2,
                   method = "BFGS", est = "Y", lower = -Inf,
                   control=list(maxit = 1000, fnscale = -1))
n = nrow(Y)
a = theta$par[1:n]
b = theta$par[(n+1):(2*n)]
B = theta$par[(2*n+1)]
Z = matrix(theta$par[(2*n+2):(2*n+1+d*n)], ncol = d, nrow = n)
Z_dist = dist2(Z)
l_lambda = t(b + t(a - Z_dist)) + B
#binomial
lambda = inv.logit(l_lambda); diag(lambda) = 0
res = Tmatrix * lambda - Y; diag(res) = NA 
#plot
plot(sort(as.vector(res)), type = "l", lwd = 2)
points(sort(as.vector(g1.res)), col = "blue", type = "l", lty = 2, lwd = 2)

# [removed] poisson.c
#lambda = exp(l_lambda); diag(lambda) = 0
#Tmatrix = Y + t(Y); diag(Tmatrix) = 0
#lambda[lambda > Tmatrix] = Tmatrix[lambda > Tmatrix]
#lambda[lower.tri(lambda)] = (Tmatrix - t(lambda))[lower.tri(lambda)]
#res = (lambda - Y); diag(res) = NA 

@

The near identical rankings from these two models belie the fact that the quasi-Stigler model is conditioned on dyad totals. In contrast, the positions in the latent space model help to explain those totals. The distances resulting from the positions exert the only dyad-specific symmetric effect in the model. (We fit a binomial two-dimensional latent space model conditioned on dyad totals and found the estimated positions to be very close to zero.) %using optim, see latent_binomial chunk
A major advantage of the latent space model is that the positions can be visualized and help us to better understand the ratings.

<<r centipede_function, eval = T, cache = T, echo = F>>=
centipede.plot <- function (segs, mct = "mean", lower.limit = "std.error", upper.limit = lower.limit, 
    left.labels = NULL, right.labels = NULL, sort.segs = TRUE, 
    main = "", xlab = NA, pch = 21, vgrid = NA, hgrid = NA, gridcol = "lightgray", 
    left.labels.col = 1, right.labels.col = 1,
    mar = NA, col = par("fg"), bg = "green", cex = NULL, ...) 
{
    if (missing(segs)) {
        cat("Usage: centipede.plot(segs,...)\n\twhere segs is a dstat object")
        stop("or a matrix of midpoints and limits")
    }
    if (is.list(segs)) {
        if (all(lapply(segs, is.numeric))) 
            segs <- get.segs(segs, mct = mct, lower.limit = lower.limit, 
                upper.limit = upper.limit)
        else stop("If segs is a list, all the components must be numeric")
    }
    if (class(segs) == "dstat") {
        midpoint <- "mean"
        if (lower.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ] * segs[2, ]
        }
        if (upper.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ] * segs[2, ]
        }
        if (lower.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]
        }
        if (upper.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]
        }
        if (lower.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]/sqrt(segs[3, ])
        }
        if (upper.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]/sqrt(segs[3, ])
        }
        segs <- rbind(segs[1, ], ll, ul, segs[3, ])
    }
    segdim <- dim(segs)
    if (sort.segs) {
        seg.order <- order(segs[1, ])
        segs <- segs[, seg.order]
    }
    else seg.order <- 1:segdim[2]
    oldpar <- par("mar")
    #if (is.na(mar[1])) 
    #    mar <- c(4, 6, 1 + 2 * (nchar(main) > 0), 5)
    #par(mar = mar)
    plot(x = c(min(segs[2, ]), max(segs[3, ])), y = c(1, segdim[2]), 
        main = main, xlab = "", ylab = "", type = "n", axes = FALSE, 
        ...)
    box()
    if (!is.na(vgrid[1])) 
        abline(v = vgrid, lty = 1, col = gridcol)
    if (is.null(hgrid)) 
        abline(h = 1:segdim[2], lty = 2, col = gridcol)
    else if (!is.na(hgrid[1])) 
        abline(h = hgrid, lty = 2, col = gridcol)
    axis(1, cex.axis = cex)
    arrows(segs[2, ], 1:segdim[2], segs[3, ], 1:segdim[2], length = 0.05, 
        angle = 90, code = 3, col = col)
    points(segs[1, ], 1:segdim[2], pch = pch, col = col, bg = bg)
    if (is.null(left.labels)) {
        left.labels <- colnames(segs)
        if (is.null(left.labels)) 
            left.labels <- paste("V", seg.order, sep = "")
    }
    else left.labels <- left.labels[seg.order]
    plot.limits <- par("usr")
    mtext(left.labels, 2, line = 0.2, at = 1:segdim[2], adj = 1, 
        las = 1, cex = cex, col = left.labels.col)
    #if (is.null(right.labels)) 
    #    right.labels <- paste(round(segs[1, ], 2), "(", segs[4, 
    #        ], ")", sep = "")
    #else 
    right.labels <- right.labels[seg.order]
    mtext(right.labels, 4, line = 0.2, at = 1:segdim[2], adj = 0, 
        las = 1, cex = cex, col = right.labels.col)
    #if (is.na(xlab)) 
    #    xlab <- paste("| -", rownames(segs)[2], "-", rownames(segs)[1], 
    #        "-", rownames(segs)[3], "- |")
    if (!is.na(xlab)) 
        mtext(xlab, 1, line = 2)
    par(oldpar)
    invisible(segs)
}
@

<<r QS_sim_CI, eval = F, cache = T, echo = F, results = 'hide', warning = F>>=

# "True SE vs. Simulated 95% Intervals"
# No real discrepancy between simulated and exact, but the sampling method may be useful later, e.g. for joy plot

# Simulated CI Method ####

   # to get a predicution under the model we 
   # we need to draw from 47^2 - 47 (for the diag)
   # independent quasi-binomials with P, N as below
   # variance is d * N * P * (1-P)
   #
   # To simulate from a closely related distribution with desired mean and variance, we use the beta-binomial with appropriately set shape parameters. Results are very similar to using true SE, as shown below.

   # We generate data with the prescribed mean and overdispered variance, and repeatedly fit the export scores (for which we may use the `glm` function in the `stats` package or the `BTm` function in te `BradleyTerry2` package used by @varinetal, since the original formulaton of the Quasi-Stigler models citation counts as overdispersed binomial distributions.
   # $$E(C_{ij}) = t_{ij}\pi_{ij}),$$
   # $$var(C_{ij}) =  \phi t_{ij}\pi_{ij}(1 - \pi_{ij}),$$
   # where, given the transposed citation matrix notation in this paper
   # $\pi_{ij} = \frac{exp(u_j - u_i)}{1 + exp(u_j - u_i)} = logit^{-1}(u_j - u_i)$. 
 
# 1 Use beta-binomial - Setup #### 
      # roughly: introduces dependence in pi's which increases variance
      # we can adjust alpha and beta to achieve the QB mean and var

#using the BradleyTerry output instead of glm to ensure comparability
library(boot) #for inv.logit
library("extraDistr") #for rbern
d = phi
P = inv.logit(x1 %*% c(0, fit$coefficients)) 
#or inv.logit(x1 %*% c(0, (g1$coefficients)[2:47]))
#or P = g1$fitted.values
N = g1$prior.weights

# 2 set alpha and beta to agree with above mean and var: ####
mean1 = N*P
var1 = d*N*P*(1-P)
B = (1-P)/P
D = d*P*(1-P)
alpha = (N*B - D*(1+B)^2)/(D*(1 + B)^3 - (B+B^2))
beta = alpha*(1-P)/P
mean2 = N*alpha/(alpha + beta) 
var2 = N*alpha*beta*(alpha + beta +N)/((alpha + beta)^2 * (alpha + beta +1)) #should agree with mean1 for N >= 2
#special cases for N = 0 or 1 !!

#test comparison
#plot(dbinom(0:11, size = 11, prob = P[2]), type = "b")
#points(dbbinom(0:11, size = 11, alpha = alpha[2], beta = beta[2]), col = 2, type = "b")

# 3 simulate ####

nsim = 5000
g = matrix(0, 47, nsim)

for (i in 1:nsim) {
  sample1 = rep(0, length(N))
  for (j in 1:length(N)) {
    #if (N[i]==0) {return(0)}
    if (N[j]==1) {sample1[j] = rbern(1, P[j])}
    if (N[j] > 1) {sample1[j] = rbbinom(1, size = N[j], 
                alpha = alpha[j], beta = beta[j])}
  }
  y1 = sample1/Tmatrix[which(lower.tri(Tmatrix))]
  y1[is.nan(y1)]=0

  #fit model (can use glm or Bradley terry)
  #Ymatrix = matrix(y1, 47, 47, byrow = T)
  #rownames(Ymatrix) = colnames(Ymatrix) = rownames(Cmatrix)
  #Ydata <- countsToBinomial(Ymatrix)
  #g[,i] <- c(0,BTm(outcome = cbind(win1, win2),
  #                  player1 = player1,
  #              player2 = player2,
  #              data = Ydata)$coefficients)
  g[,i] = glm(y1~x1-1,family = quasibinomial(link="logit"), weights =   (Tmatrix)[lower.tri(Tmatrix)])$coefficients
}

g[1,] = 0

# 4 centipede plot ####

#original 
#centipede.plot(t(centipede.qse), vgrid = c(-1,0,1,2), cex = .7, 
#               right.labels = round(fit.table2$quasi, 2),
#               bg = "blue", main = "Quasi-Stigler")

#Beta-Binomial vs. SE
shift = mean(apply(g, 1, mean))
centipede.sim = cbind(apply(g, 1, mean), apply(g, 1, quantile, .025), apply(g, 1, quantile, .975)) - shift

centipede.plot(t(centipede.se), vgrid = c(-1,0,1,2), cex = .7, 
               right.labels = round(centipede.se[,1], 2), 
               left.labels = rownames(fit.table2), 
               bg = c("red"), col = c("red"),
               main = "True SE vs. Simulated 95% Intervals",
               xlim = c(-2, 2.5))
par(new=TRUE)
centipede.plot(t(centipede.sim), cex = .7,
               bg = c("green"), col = c("green"),
               left.labels = " ",
               right.labels = " ",
               xlim = c(-2, 2.5))
legend("bottomright", legend = c("SE", "SIM"), col = c("red", "green"), pch = 16)

@

<<r SE_vs_LATENT, cache = F, eval = F, echo = F, results = 'hide', warning = F, fig.keep = 'last', fig.height = 10>>=

#True SE for quasi-Stigler vs. Intevals vs. Latent Space.#Pretty comparable overall. StataJ's true SE interval is noticeably larger than others. This is likely due to the fact that it has fewer observed citations (in and out) than all other journals.

latent.srp2 <- readRDS("~/Documents/citation/Sunbelt/latent_srp2R.RDS")

#latent space centipede ####

# have to be careful with centipede plot now because
# order(centipede.se[,1]) != order(centipede.lat[,1])

# use seperate plots

par(mfrow = c(1,2))
latent.model = latent.srp2
sd2 = apply(latent.model$sample$receiver - latent.model$sample$sender, 2, sd)
latent2 = latent.model$mkl$receiver - latent.model$mkl$sender

centipede.lat = cbind(latent2, latent2 - 1.96*sd2, latent2 + 1.96*sd2) + 
    max(centipede.se[,1]) - max(latent2)
    #adding a shift so they align in the plot!
rownames(centipede.lat) = rownames(Cmatrix)

centipede.plot(t(centipede.lat), vgrid = c(-1,0,1,2), cex = .7, 
               right.labels = round(centipede.lat[,1], 2),
               left.labels = rownames(fit.table2),
               bg = "green", col = "green",
               main = "Latent Space")

centipede.plot(t(centipede.se), vgrid = c(-1,0,1,2), cex = .7, 
               right.labels = round(centipede.se[,1], 2),
               left.labels = rownames(fit.table2),
               bg = "red", col = "red",
               main = "True SE")

#keep seperate names

par(mfrow = c(1,1))
centipede1 = rbind(centipede.lat, centipede.se)
centipede1 = cbind(centipede1, c(rep(1, 47), rep(2, 47)))
centipede1 = centipede1[order(centipede1[,1]), ]
centipede.plot(t(centipede1[,1:3]), vgrid = c(-1,0,1,2), cex = .5, 
               left.labels = rownames(centipede1[,1]),
               right.labels = round(centipede1[,1], 2),
               left.labels.col = centipede1[,4],
               right.labels.col = centipede1[,4],
               bg = centipede1[,4], col = centipede1[,4],
               main = "True SE and Latent Space",
               sort.segs = F)
legend("bottomright", legend = c("SE", "LS"), col = c(2, 1), pch = 16)
@

\subsection{Visualization of Latent Space Journal Rankings} \label{Visualization}

<<r latent_sr2_plot, eval = T, cache = T, dependson= c("setup", "setup2", "latent_sr2", "setup_jrss"), echo = F, fig.width = 6, cache = T, warning=F, message=F, fig.cap= 'Estimated journal positions from the two-dimensional latent space model. Top: Point estimates with node size scaled to receiver minus sender coefficient. Bottom: Sample of positions from the model. Colouring is due to the hierarchical clustering of Varin et al. '>>=


# to correspond to visNetwork
library(scales)
alpha2 <- 0.6
color1 <- c("deepskyblue3","gold", "red", "green", "deeppink", "darkorchid4", "orange", "black")
Cnet%v%"color" = color1[cutree(journals.cluster, h = 0.6)]
write.csv(latent.srp2.init.r$mkl$Z, file = "~/Documents/citation/latent_ranking/latent_srp2_init_r_Z.csv")

# plot window
par(mai = c(.1,.1,.1,.1))
#layout(matrix(c(1,1,1,2,2,1,1,1,2,2,1,1,1,2,2), 3, 5, byrow = TRUE), respect = T)
par(mfrow = c(2,1))

#### position plot ####
vc2 <- (latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender)/2+1
col2 <- alpha(color1[cutree(journals.cluster, h = 0.6)], alpha2)
plot(latent.srp2.init.r$mkl$Z, col = col2, cex = vc2, pch = 16,
     bty="n", yaxt="n", xaxt="n")
text(latent.srp2.init.r$mkl$Z, labels = rownames(Cmatrix), cex = .5, pos = 3, offset = .3)
legend("topleft", col = alpha(color1, alpha2), pch = 16, legend=c("review", "general", "theory/method", "appl./health", "computational","eco./envir.", "JSS", "StataJ"), cex=0.7, box.col=0)

####  cloud/uncertainty plot  ####
n = 47
N = 1000
p = latent.srp2.init.r[["sample"]][["Z"]]
s = sample(5000, N)

#null plot:
plot(latent.srp2.init.r$mkl$Z, xlab = NA, ylab = NA, vertex.col=0, edge.col=0, 
           plot.vars=F, suppress.axes=T, bty="n", yaxt="n", xaxt="n", type ="n",
           xlim = c(-4,4), ylim = c(-3,5), main = NA)

#add points
for (i in 1:N) {
  points(p[s[i],,], col = col2, pch = 15, cex = .2)
}

@

Figure \ref{fig:r latent_sr2_plot} (top) shows estimated MKL positions. %shortreed06: heuristically, posterior mean graph is closer to the true than oberved graph due to averaging and use of prior info.
The size of the nodes is scaled to the estimated scores. The coloring and cluster labels for the nodes in both panels are based on the hierarchical clustering of \citet{varinetal}. Although there is no clustering term in our latent space model, the estimated positions are consistent with these clusters. (Some labels are difficult to read, but this is addressed by the dynamic plot referenced below.) The bottom panel of Figure \ref{fig:r latent_sr2_plot} shows a sample of 1000 posterior draws of positions, visualizing the uncertainty. Although individual journal positions are variable, the clusters occupy discernible areas. However, their irregular shapes caution against applying the latent space clustering model of \citet{handcock07}, in which positions given clusters have spherical Gaussian distributions. 
%I did experiment with it and it didn't fit.
The plot shows how they fit together and provides more information than discrete labels. For example the \textit{Journal of Biopharmaceutical Statistics} is most deeply embedded in the applied/health cluster, while \textit{Statistical Science} is on the border. In fact, it should be classified with the review journals it lies near. (We discuss the use of latent space visualization to identify mislabeled classes in Section \ref{Example2}.)  %Broadly speaking, as we move from left to right we go from general journals to specific application areas. The computational journals are towards the top, and the theoretical cluster is the most centrally located. 


The citation network is too dense to display network edges in a static plot. However, using the \pkg{visNetwork} package we render a dynamic plot of the citation network to explore its connections. A version of the plot is included as an html file in the supplementary material, and is available on the author's website (\url{http://www.stat.ucla.edu/~jane.carlen/pages/citation_net.html}). Because of the density of the network, edges in the dynamic visualization are only shown if they account for at least three percent of a journal's out-citations, and their width when highlighted is scaled to that percentage. Rankings based on the latent space model are reported next to the journal titles in the drop-down menu on the left and in the hover text. Coloring of highlighted edges is determined by the cluster of the originating node, the same clusters as in Figure \ref{fig:r latent_sr2_plot}.

<<r latent_sr2_interactive, eval = F, results = 'hide', dependson= c("setup", "latent_sr2", "setup_jrss", "latent2_stigler"), fig.keep='last', echo = F, fig.height=5, fig.width=6, cache = T, warning=F, message=F>>=

# setup ####

library(visNetwork)
library(igraph)
#insertSource("~/Documents/citation/latent_ranking/visIgraph.R",
#             package = "visNetwork", functions = "visIgraph")
#insertSource("~/Documents/citation/latent_ranking/visIgraphLayout.R",
#             package = "visNetwork", functions = "visIgraphLayout")


# data ####
nodes <- data.frame(id = 1:47, 
                    label = Cnet%v%"vertex.names",
                    #title = Cnet%v%"vertex.names", #for selection dropdown
                    title = paste(Cnet%v%"vertex.names", 48 - srs2b[,2]), #for tooltip
                    value = vc2, #conveys node size, on a scale from 0-1
                    #color.highlight.background = "red",
                    group = rep((cutree(journals.cluster, h = 0.6))))


edges <- data.frame(from=data.frame(as.edgelist(Cnet))$X1, 
                    to=data.frame(as.edgelist(Cnet))$X2)
                    #value = Cnet%e%'citations'

cite_igraph <- graph.data.frame(edges, directed=TRUE, vertices=nodes)
Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/latent_srp2_init_r_Z.csv")[,2:3])
Z<- -Z[ ,c(2,1)] #to preserve orientaiton in paper

#fewer edges
# Cmatrix.norm = t(Cmatrix)/citing #entries are % of i's citations to j (column); rows sum to 1; 'citing' is total out for each journal
strength = .03 #only include "strong receivers", e.g. receiver accounts for >.05 = 5% of sender's citations sent [different form the plot in `compare_clustered`]
#interesting plot with strength = .1 (100 edges remain)  
Cstrong = t(Cmatrix)
Cstrong[Cmatrix.norm<strength] = 0
#rowSums(Cstrong>0)
Cnet_strong = as.network(Cstrong, directed=T, matrix.type="a", ignore.eval=F,  names.eval="citations")

edges2 <- data.frame(from=data.frame(as.edgelist(Cnet_strong))$X1, 
                    to=data.frame(as.edgelist(Cnet_strong))$X2,
                    value = as.edgelist(Cnet_strong, as.sna.edgelist = T, attrname = "citations")[,3])

cite_igraph <- igraph::graph.data.frame(edges2, directed=TRUE, vertices=nodes)

# dynamic plot ####

visNetwork(nodes, edges2, main = "Statistics Journals", submain = "latent space positions") %>%
  visIgraphLayout(cite_igraph, layout = "layout.norm",
                  layoutMatrix = cbind(-Z[,2],Z[,1]), type = "full") %>%
  #visOptions(highlightNearest = list(enabled =TRUE, degree = 1)) %>%
  visNodes(#color = list(highlight = list(background = "green")), #not working as expected
           shape = "dot", #shape = "text"
           scaling = list(min = 3, max = 15),
           labelHighlightBold = TRUE,
           font= list(size = 20, color = "black", strokeWidth = 1)) %>%
  visOptions(highlightNearest = list(enabled = TRUE, degree = .5), #hover = TRUE doesn't work?
             nodesIdSelection = FALSE, 
             selectedBy = "title") %>% 
  visEdges(shadow = FALSE,
           #width = 1, to use width turn edge value off
           scaling = list(min = 1, max = 15),
           #selectionWidth = "function(x) {Cnet_strong%e%'citations';}",
           #scaling = list(min = 1, max = 10),
           arrows = list(to = list(enabled = FALSE, scaleFactor = 3),
                    middle = list(enabled = TRUE, scaleFactor = 1)),
           color = list(inherit = "from", highlight = "red", opacity = .03),
           hidden = F,
           smooth = list(type = "curvedCW")) #%>% visLegend()

  #turned these off bc interfering with edge inherit coloring
   # visGroups(groupname = "1", color = list(background = "blue")) %>%
   # visGroups(groupname = "2", color = list(background = "cyan")) %>%
   # visGroups(groupname = "3", color = list(background = "hotpink")) %>%
   # visGroups(groupname = "4", color = list(background = "yellow")) %>%
   # visGroups(groupname = "5", color = list(background = "grey")) %>%
   # visGroups(groupname = "6", color = list(background = "lavendar")) %>%
   # visGroups(groupname = "7", color = list(background = "red")) %>%
   # visGroups(groupname = "8", color = list(background = "green")) %>%

#saved as citation_net_revised.html        
@

Now we revisit the rankings of journals discussed in Section \ref{Comparison}. The ``big four'' journals all draw citations widely from the network but tend to cite journals fairly nearby. In contrast, StMed, JSPI and CSDA, which are ranked highly by PageRank and Eigenfactor but not the latent space model, give and receive citations from a wide range of journals. JSS, which is ranked highly by Impact Factor and Article Influence but 42nd by the latent space model, only accounts for significant out-citations from StataJ. \textit{Statistical Science} does draw citations from two top-four journals, but not at the rate of the highest-ranked journals. Its in-citations have roughly the same span as its out-citations, but generally come from theoretical or computational journals and go to theoretical or applied ones. The visualization helps to explain how research flows through the network.

\subsection{Model Evaluation} \label{modeleval}

Having illustrated the value of latent space rankings and visualization, in this section we compare competing latent space models and estimation methods. We discuss the choice of model dimension, model fit to the observed network, sensitivity to hyperpriors, and tradeoffs between estimation methods. The section concludes with a simulation study of our quasi-Newton algorithm. 

\subsubsection{Latent Dimension} \label{dim}

To choose the latent space dimension of the model we consider estimates of the integrated likelihood,

\begin{equation}
P(Y) = \int P(\theta|Y)P(\theta)d\theta.
\end{equation}

\noindent The classic Bayesian information criterion (BIC) of a model is an approximation to $-2log(P(Y))$, under the assumption that the data under the model follows an exponential family distribution.

\begin{equation}
BIC = -2log(P(Y \vert \hat{\theta})) + d \cdot log(s),
\end{equation}

\noindent where $\hat{\theta}$ is the maximum-likelihood parameter estimate, $d$ is the dimension of the parameter space, and $s$ is the effective sample size. In the case of the latent space rating model, we cannot guarantee that the MCMC-generated maximum-likelihood estimate is optimal, the parameter dimension is constrained by prior distributions, and different effective sample sizes are used in estimating various parameters. As an alternative to BIC we consider the BIC for model selection (BICM) of \citet{BICM}.

\begin{equation}
BICM = -2\hat{\ell}_{max} + \sum_{k=1}^{K}log(n_k) + (\hat{d} - K)log(n_{K}),
\end{equation}

\noindent where $K$ is the number of fixed effects in the model. We let $\hat{\ell}_{max} = log(P(Y \vert \hat{\theta}))$, where $\hat{\theta}$ is our highest-likelihood MCMC estimate. Given the consistency in estimates we have found by various methods, the error in that approximation should be small relative to differences between models. We consider the estimate of $\hat{d}$ derived by \citet{BICM}, which draws on the asymptotic distribution

\begin{equation}
\ell_{max} - \ell_t \sim Gamma(\alpha, 1),
\end{equation}

\noindent where $\ell_t$ is the log likelihood of a draw from the posterior distribution of $\theta$. The variance of this gamma distribution is $\alpha$, which we estimate by $var(\ell_t)$, and thereby estimate

\begin{equation}
\hat{d} = 2var(\ell_t).
\end{equation}

\noindent We use the MCMC sample for this estimate, so it is important that it has been thinned enough that the posterior log likehoods are independent. (This can be checked using the \code{mcmc.diagnostics} function of \pkg{latentnet}.) In addition, we note that the assumed scale parameter implies another estimate of $\alpha$ if we have a pre-existing estimate of $l_{max}$, namely $\alpha = E[l_{max} - l_t]$. This recovers the estimate of parameter dimension introduced by \citet{paramD},    %Note that this is similar to a one half variance formula in Gelman, but that formula has questioned. http://andrewgelman.com/2006/07/06/number_of_param/

\begin{equation}
p_D = -2*\hat E[l_t - l_{max}] = \hat{D}_{avg}(Y, \theta) - D(Y, \hat{\theta}), %also in gelman
\end{equation}

\noindent where deviance $D(Y,\theta) = -2log(P(Y \vert \theta))$ and  $\hat{D}_{avg}(Y)$ is calculated by averaging over the MCMC sample. \citet{paramD} use the posterior mean for the point estimate $\hat\theta$, but note that others can be justified. Discrepancy between $\hat{d}$ and $p_D$ reveals the extent to which the asymptotic gamma assumption does not fit the data. This may imply that the scale parameter is not exactly one, though it should be close. (The discrepancy can provide an estimate of how far it is from one, which was a challenge to \cite{BICM}.) Alternately, it may indicate error in our estimate of $l_{max}$ or $var(l_t)$, but if the magnitude of this error is small, as we find in this example, then we can proceed with model selection.

For the effective sample size, we are primarily concerned with the number of data points used to estimate positions, since our models only differ in the dimension of positions. Although \citet{BICM} based the effective sample size for positions in a binary network on the number of realized edges, in a valued network zero-weight edges are informative, so we use $2(n-1)$. This poses a slight problem because the sender and receiver random effects would logically halve that effective sample size to $n-1$, since they only depend on out- and in-edges respectively. However, the over-penalizaiton that results is consistent across dimensional models, so we can ignore it when comparing models.

% We can use the simulated graph stuff from sim_point_point to establish that the marginal distribution of data under the model is similar to the distribution of data given theta, which is poisson/exponential family?

<< latent_sr1_init_r, eval = T, results = 'hide', echo = F, cache = T, cache.comments = FALSE, warning= FALSE>>=

#random initation of sender and receiver, and scaling Z helps the model fit
D = 1; N = nrow(Cmatrix)
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

t1.init.r1 = Sys.time()
latent.srp1.init.r = ergmm(Cnet ~ euclidean(d = 1) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 500000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        pilot.runs = 10),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
t2.init.r1 = Sys.time()
@

<< latent_sr3_init_r, eval = T, results = 'hide', echo = F, cache = T, cache.comments = FALSE, warning= FALSE>>=

#random initation of sender and receiver, and scaling Z helps the model fit
D = 3; N = nrow(Cmatrix)
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

t1.init.r3 = Sys.time()
latent.srp3.init.r = ergmm(Cnet ~ euclidean(d = 3) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 500000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        pilot.runs = 10),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
t2.init.r3 = Sys.time()
@

<< plot_3d, eval = F, echo = F, cache = T, fig.keep = "none", results = "asis">>=

# Three-dimensional model/plot?
tmp = lsqn(t(Cmatrix), runs = 50, tol = .01, Z.init = "rnorm", D = 3)
plot_ly(data = data.frame(tmp$map$Z), x = ~X1, y = ~X2, z = ~X3, color = col2)

hist((26501.9 - lgamma.constant)- latent.srp0$sample$lpY, freq = F, breaks = 50)
curve(dgamma(x, (47*2+1)/2, 1), add = T, col = "red") #pretty good
@

<<BIC, eval = T, echo = F, cache = T, fig.keep = "none", results = "asis">>=

# BIC and BICM type estimates. Vary depending on how we estimate:
# 1. l_max: (from sample or shifted gamma assumption) - we'll stick with our MLE (pmean or mkl)
# 2. d: d_hat based on deviance vs variance of posterior log likelihood (2)
# 3. n: Constant n vs. n adjusted (for number of data points involved in estimating a parameter) (2)

n = 47
# values for table ####

lhat_0 = mean(latent.srp0$sample$lpY) 
lhat_1 = mean(latent.srp1.init.r$sample$lpY) 
lhat_2 = mean(latent.srp2.init.r$sample$lpY) 
lhat_3 = mean(latent.srp3.init.r$sample$lpY) 

#dimensions based on posterior variance
dhat_0 = 2*var(latent.srp0$sample$lpY)
dhat_1 = 2*var(latent.srp1.init.r$sample$lpY)
dhat_2 = 2*var(latent.srp2.init.r$sample$lpY)
dhat_3 = 2*var(latent.srp3.init.r$sample$lpY)

#posterior means
model = latent.srp0
pmean_0 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = matrix(0, 47, 1),
                beta = mean(model$sample$beta)) 

model = latent.srp1.init.r
pmean_1 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = apply(model$sample$Z, c(2,3), mean),
                beta = mean(model$sample$beta)) 

model = latent.srp2.init.r
pmean_2 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = apply(model$sample$Z, c(2,3), mean),
                beta = mean(model$sample$beta)) 

model = latent.srp3.init.r
pmean_3 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = apply(model$sample$Z, c(2,3), mean),
                beta = mean(model$sample$beta)) 

lmean_0 = llik(pmean_0, Z = matrix(0, 47, 1), Y = t(Cmatrix), est = "Y")
lmean_1 = llik(pmean_1, Y = t(Cmatrix), est = "Y")
lmean_2 = llik(pmean_2, Y = t(Cmatrix), est = "Y")
lmean_3 = llik(pmean_3, Y = t(Cmatrix), est = "Y")

lmax_0 = llik(latent.srp0$mkl, Z = matrix(0, 47, 1), Y = t(Cmatrix), est = "Y")
lmax_1 = llik(latent.srp1.init.r$mkl, Y = t(Cmatrix), est = "Y")
lmax_2 = llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y")
lmax_3 = llik(latent.srp3.init.r$mkl, Y = t(Cmatrix), est = "Y")

#dimensions based on deviance
pD_0 = -2*lhat_0 + 2*lmean_0
pD_1 = -2*lhat_1 + 2*lmean_1
pD_2 = -2*lhat_2 + 2*lmean_2
pD_3 = -2*lhat_3 + 2*lmean_3

# bootstrap SE
SE_0 = (2*log(2*(n-1))) * sqrt(var(sapply(1:5000, function(x)
       {var((latent.srp0$sample$lpY)[sample(5000, 5000, replace = T)])})))
SE_1 = (2*log(2*(n-1))) * sqrt(var(sapply(1:5000, function(x)
       {var((latent.srp1.init.r$sample$lpY)[sample(5000, 5000, replace = T)])})))
SE_2 = (2*log(2*(n-1))) * sqrt( var(sapply(1:5000, function(x)
       {var((latent.srp2.init.r$sample$lpY)[sample(5000, 5000, replace = T)])})))
SE_3 = (2*log(2*(n-1))) * sqrt( var(sapply(1:5000, function(x)
       {var((latent.srp3.init.r$sample$lpY)[sample(5000, 5000, replace = T)])})))

# BIC table ####

BIC.table = data.frame(row.names = c("0", "1", "2", "3"))

           BIC.table$"BIC" = c(-2*(lmax_0) + (n*2 + 3)*log(n*(n-1)),
                     -2*(lmax_1) + (n*3 + 4)*log(n*(n-1)),
                     -2*(lmax_2) + (n*4 + 4)*log(n*(n-1)),
                     -2*(lmax_3) + (n*5 + 4)*log(n*(n-1)))
                     #note: bic.ergmm function of latentnet uses the handock et al BIC which is designed for selecting number of clusters

           BIC.table$"$BICM_{\\hat{d}}$" =
                        c(-2*(lmax_0) + 3*(log(n*(n-1))) + (dhat_0 - 3)*log(2*(n-1)),
                        -2*(lmax_1) + 4*(log(n*(n-1))) + (dhat_1 - 4)*log(2*(n-1)), 
                        -2*(lmax_2)+ 4*(log(n*(n-1))) + (dhat_2 - 4)*log(2*(n-1)), 
                        -2*(lmax_3)+ 4*(log(n*(n-1))) + (dhat_3 - 4)*log(2*(n-1)))
           
            BIC.table$"$BICM_{p_D}$" = 
                      c(-2*(lmax_0) + 3*(log(n*(n-1))) + (-2*(pD_0) - 3)*log(2*(n-1)),
                      -2*(lmax_1) + 4*(log(n*(n-1))) + (-2*(pD_1) - 4)*log(2*(n-1)),
                      -2*(lmax_2) + 4*(log(n*(n-1))) + (-2*(pD_2) - 4)*log(2*(n-1)),
                      -2*(lmax_3) + 4*(log(n*(n-1))) + (-2*(pD_3) - 4)*log(2*(n-1)))
                      
           #BIC.table$"$\\hat{SE}_{mcmc}$" = c(SE_0, SE_1, SE_2, SE_3)
           
           BIC.table$"$\\hat{d}$" = c(dhat_0, dhat_1, dhat_2, dhat_3)
        
           BIC.table$"$p_D$" = c(pD_0, pD_1, pD_2, pD_3)
           
           BIC.table$"Rating Cor." =
                           as.character(round(c(cor(latent.srp0$mkl$receiver - latent.srp0$mkl$sender, 
                                latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender),
                           cor(latent.srp1.init.r$mkl$receiver - latent.srp1.init.r$mkl$sender, 
                                latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender),
                           1,
                           cor(latent.srp3.init.r$mkl$receiver - latent.srp3.init.r$mkl$sender, 
                                latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender)), 4))
           

print(xtable(BIC.table, caption = "Comparison of BIC(M), estimated parameter dimension and ratings correlation for models in zero to three dimensions. Correlations listed are to the two-dimensional ratings.", digits = 0, label = "BIC.table"), caption.placement = "top", sanitize.text.function=function(x){x})


@

<<BIC_2, eval = F, echo = F, cache = T, fig.keep = "none", results = "asis">>=

# Comparing estimates ####

#1. lmax. raftery et al assume you don't have lmax, estimate it by mean(lpY) + var(lpY). How does that compare to what we found? Pretty close.
model = latent.srp2.init.r
mean(model$sample$lpY) + var(model$sample$lpY) #-4556.06
llik(model$mkl, Y = t(Cmatrix), est = "Y")  #-4560.834

#2. How do estimates of parameter dimension compare? Does gamma assumption hold up?
# how does the resulting gamma look? with pmean estimate of lmax:
hist(lmax_2 - latent.srp2.init.r$sample$lpY, freq = F, breaks = 30)
curve(dgamma(x, d_hat2/2, 1), xlim = c(0,150), add = T, col = "red") #pretty good (1)
#with adjustment to scale parameter based on discrepancy in alpha estimates:
curve(dgamma(x, d_hat2*((lmax_2-lmean_2)/var(model$sample$lpY))/2, 1), xlim = c(0,150), add = T, col = "green") #better -> (1)
curve(dgamma(x, pD_2/2, 1), xlim = c(0,150), add = T, col = "blue") #best

#with mkl estimate of lmax
hist(llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y") - latent.srp2.init.r$sample$lpY, freq = F, breaks = 50)
curve(dgamma(x, d_hat2/2, 1), xlim = c(0,150), add = T, col = "red") #pretty good

#So either var(l_t) isn't quite right (1), scale = 1 isn't quite right (2), and/or l_max estimate isn't quite right (though this is least likely since we're pretty sure our estimate is close) (3)

#p(y|M) estimation? not enough information #### 
samp1 = rep(0,10000)
N = 47
for (i in 1:length(samp1)) {
    sender.var = rinvchisq(1, 12, 2)
    receiver.var = rinvchisq(1, 12, 2)
    Z.var = rinvchisq(1, sqrt(N), N/8)
  object = list(
    beta = rnorm(1, 0, sd = sqrt(beta.var)),
    sender = rnorm(N, 0, sqrt(sender.var)),
    receiver = rnorm(N, 0, sqrt(receiver.var)),
    Z = matrix(rnorm(N*d, 0, sqrt(Z.var)), nrow = N, ncol = d)
  )
  samp1[i] = llik(object, Y, est = "Y")
}
@

Table \ref{BIC.table} shows, from left to right, a traditional BIC estimate with parameter dimension equal to the number of parameters and sample size equal to $n*(n-1)$; BICM using $\hat{d}$; BICM using $p_D$; the $\hat{d}$ and $p_D$ estimates of parameter dimension; and the correlation in ratings from each model with the two-dimensional model as the reference. The two-dimensional model is a major improvement over the zero- and one-dimensional models by all measures. The three-dimensional model is a small improvement over the two-dimensional model. However, the correlation in ratings between the two and three-dimensional models is extremely high (0.9991). The combination of a modest decrease in BIC with increased difficulty of visualizing three-dimensional estimates led us to select the two-dimensional model. (Although we did not  BICM estimates is not of primary interest here, we did calculate a bootstrap estimate of the standard error in $BICM_{\hat{d}}$ due to MCMC sampling variation and found it to be small relative to differences between models. Variation in $BICM_{p_D}$ is even smaller as long as samples are fairly large.)

\subsubsection{Model Fit} \label{dim}

The latent space and quasi-Stigler models produce roughly equivalent journal rankings, but how well do they model the observed network? We consider several elements of the fit of our latent space model, especially as they compare to the quasi-Stigler model.

<<residuals, eval = T, cache = T, message =F, warning = F, results = 'hide', echo = F, dependson = c("glm"), fig.height = 3, fig.cap = 'Comparison of model residuals. Left: Residuals are ordered by size and only the range from -25 to 25 is shown to enhance detail. Right: Residuals are ordered by weight of the corresponding edge and a linear model is fit to each set.', fig.pos = 'H'>>=

# g1 (quasi stigler, or quassibinomial glm) ####

# under most likely graph | theta point estimate
# fitted.values for upper.tri: 2,1 3,1 ... 3,2, 4,2, ... 47,46
# this is because of how the response were listed in g1

#QSmat is the most likely graph (as matrix) under quasi-Stigler
QSmat = matrix(0, 47, 47)
QSmat[lower.tri(QSmat)] = (Tmatrix[lower.tri(Tmatrix)])*g1$fitted.values
QSmat = t(QSmat)
QSmat[lower.tri(QSmat)] = (Tmatrix[lower.tri(Tmatrix)])*(1-g1$fitted.values)
QSmat = t(QSmat)

g1.res = QSmat - t(Cmatrix); diag(g1.res) = NA

# latent model without positions ####
model = latent.srp0
R0 = predict(model) - model$model$Ym; diag(R0) = rep(NA, nrow(model$model$Ym))
#hist(R0, breaks = 100, col = "green", prob = T, ylim = c(0,.25), xlim = c(-50,50))

# latent model with 2-D positions ####
model = latent.srp2.init.r
R2 = predict(model, type = "mkl") - model$model$Ym; diag(R2) = rep(NA, nrow(model$model$Ym))
#hist(R2, breaks = 20, add = T, col = "red", prob = T, density = 30)
#Y = t(Cmatrix); diag(Y) = NA; plot(log(as.vector(R2))[order(as.vector(Y))])
#Y = predict(model, type = "mkl") ; diag(Y) = NA; plot(x = as.vector(Y), y = as.vector(R2))

# other models with positions ####
#model = latent.srp1
#R1 = predict(model, type = "mkl") - model$model$Ym; diag(R1) = rep(NA, nrow(model$model$Ym))

#model = latent.srp3
#R3 = predict(model, type = "mkl") - model$model$Ym; diag(R3) = rep(NA, nrow(model$model$Ym))

#R = data.frame(residual = c(as.vector(g1.res), as.vector(R0),
 #                           as.vector(R2), as.vector(R1), as.vector(R3)),
  #             model = as.factor(c(rep("QS", 2209), rep("D = 0", 2209), rep("D = 2", 2209), 
   #                                rep("D = 1", 2209), rep("D = 3", 2209))),
    #           edgeweight = rep(as.vector(t(Cmatrix)), 5),
     #          order = c(rank(g1.res), rank(R0), rank(R2), rank(R1), rank(R3)))

# Summary ####
summary(as.vector(g1.res))
summary(as.vector(R0))
summary(as.vector(R2))
R = data.frame(residual = c(as.vector(g1.res), as.vector(R0), as.vector(R2)),
               model = as.factor(c(rep("QS", 2209), rep("D = 0", 2209), rep("D = 2", 2209))),
               edgeweight = rep(as.vector(t(Cmatrix)), 3),
               order = c(rank(g1.res, ties.method = "random"),
                         rank(R0, ties.method = "random"),
                         rank(R2, ties.method = "random")))
# Sorted Residual Plot ####
res_vs_index = ggplot(R, aes(x = order, y = residual, color = model)) + 
  geom_hline(yintercept = 0, col = "gray") +
  geom_point(size = .4) + ylim(-25,25) +
  theme_bw() +
  theme(legend.position="none") +
  ggtitle("Ordered Residuals")

#ggplot(data = R, aes(x = residual, fill = model, col = model)) +
 # geom_histogram(binwidth = c(1), position = "identity") +
  #theme_bw() + theme(legend.position = "none") + 
  #facet_wrap(~model, scales = "free_x")

# Residuals vs. Edge Weight Plot  ####
# not surprisingly, quasi fits best bc binomial model assumes totals (Tmatrix) known
res_vs_edge = ggplot(R, aes(x = edgeweight, y = residual, color = model)) + 
  geom_hline(yintercept = 0, col = "gray") +
  geom_point(size = .5, alpha = .8) +
  geom_smooth(method = "lm", se = F, size = .5) +
  theme_bw() + 
  guides(fill = F) +
  ggtitle("Residuals vs. Edge Weight") #+ facet_wrap(~model, scales = "free_x")

# Overdispersion? ####
#Is there overdispersion in the poisson models? We plot the Pearson residuals:

data1 = data.frame(fitted2 = predict(latent.srp2.init.r)[!is.na(R2)],
                   pearson2 = as.vector(R2[!is.na(R2)]/sqrt(predict(latent.srp2.init.r)[!is.na(R2)])),
                   fitted0 = predict(latent.srp0)[!is.na(R0)],
                   pearson0 = as.vector(R0[!is.na(R0)]/sqrt(predict(latent.srp0)[!is.na(R0)])))

data1 = reshape::melt(data1, measure.vars = c("fitted0", "fitted2"))

res_vs_fitted = ggplot(data1, aes(color = variable, x = value, y = pearson0)) +
  geom_point(size = .5) + theme_bw() + ggtitle("Residuals vs. Fitted") +
  theme(legend.position = "none") +
  ylab("Pearson residual") + xlab("fitted value") + xlim(0,150) +
  scale_color_manual(values = c(hue_pal()(3)[1:2]))


# PLOT ####
cowplot::plot_grid(res_vs_index, res_vs_fitted, res_vs_edge, rel_widths = c(.75, .75, 1), nrow = 1)
# Treat 0 as missing edges? ####
#Concerned about the number of underpredictios for fitted values of zero I tried fitting the network treating zero edges as missing. The basic patter was still there though. 

#par(mfrow = c(1,2))

#plot(as.vector(predict(latent.srp2.init.r)),
 #    as.vector(R2)/sqrt(as.vector(predict(latent.srp2.init.r))), xlim = c(0,150))

#model = latent.srp2.missing
#R2m = predict(model) - model$model$Ym; diag(R2m) = rep(NA, nrow(model$model$Ym))
#plot(as.vector(predict(model)[t(Cmatrix)>0]),
 #    as.vector(R2m[t(Cmatrix)>0])/sqrt(as.vector(predict(model)[t(Cmatrix)>0])), xlim = c(0,150))


@

Figure \ref{fig:residuals} (left) displays sorted residuals based on point estimates from the two-dimensional latent space model (D = 2), a Poisson sender-receiver model with no latent positions (D = 0), and the quasi-Stigler model. The quasi-Stigler model has the smallest residuals overall, which we expect since the model is constrained by dyad totals. We include the no-position model because it helps to distinguish the impact of using a Poisson model from the effect of adding positions.

The two-dimensional latent position model is a clear improvement on the same model without positions. As a summary measure, we estimate expected posterior deviance, $E_{\theta|Y_{obs}}[-2log(P(Y_{obs}|\theta))]$, by averaging over the MCMC sample. It drops from $-52912$ to $-58766$ when we add two-dimensional positions. (The standard deviations of the posterior deviances are 19 and 13 respectively.) The right plot of Figure \ref{fig:residuals} orders residuals by weight of the underlying edge. The no-position model is systematically biased, as edges with lower citation counts are more likely to be underestimated, and heavier edges more likely to be underestimated. This pattern is evident in the other two models, but much less severe. Examining the underestimated edges with high edge weight confirms that they represent outliers relative to how much they typically cite or are cited. This may be a result of the long-tail distribution of citation counts, with a few papers that are relevant to specific journal sub-fields driving these values up. We do not have access to the itemized counts to investigate this further, but if we find a systematic cause we could add a covariate to the model to capture additional features.

<<latent_deviance, eval = T, cache = T, echo = F,  message = F, results = "hide">>=
lgamma.constant = sum(lgamma(as.vector(Cmatrix+1)))
# Expected deviance 
ED0 = mean(-2*(latent.srp0$sample$lpY + lgamma.constant))
ED2 = mean(-2*(latent.srp2.init.r$sample$lpY + lgamma.constant))
#DIC estimation ####
pd0 = ED0 + 2*llik(latent.srp0$mkl, Z = matrix(0, 47, 2), Y = t(Cmatrix), est = "Y")
      #(latnet.srp0$mcmc.mle$lpY + lgamma.constant)
pd2 = ED2 + 2*llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y")
      #(latnet.srp2$mcmc.mle$lpY + lgamma.constant)
DIC0 = pd0 + ED0
DIC2 = pd2 + ED2

sd(-2*(latent.srp0$sample$lpY + lgamma.constant))
sd(-2*(latent.srp2.init.r$sample$lpY + lgamma.constant))
@

<<residuals2, eval = F, echo = F>>=
library(reshape)
Y = data.frame(t(Cmatrix))
Y = melt(Y); colnames(Y) = c("journal", "citations")
receiver_dist = ggplot(data = Y, aes(x = journal, y = citations, color = journal)) + geom_boxplot() + theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("receiver distibutions")

Y = data.frame(Cmatrix)
Y = melt(Y); colnames(Y) = c("journal", "citations")
sender_dist = ggplot(data = Y, aes(x = journal, y = citations, color = journal)) + geom_boxplot() + theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("sender distributions")

#ggplot(Y, aes(x = citations, y = journal)) + geom_joy() + xlim(0, 50)

cowplot::plot_grid(sender_dist, receiver_dist)
@

%3). Structure, reciprocity 

We also evaluate model fit through a posterior predictive check of network reciprocity. Given the high reciprocity observed between journals and its influence on rankings, we are interested in whether this has been captured by the models. We employ a version of the reciprocity statistic defined by \citet{newman10}, adjusted for weighted networks. We consider both a normalized and unnormalized version to examine the impact of fixing dyad totals. 

\begin{equation}
reciprocity(Y) = \frac{\sum_{ij}Y_{ij}*Y_{ji}}{\sum_{ij}(\frac{Y_{ij}+Y_{ji}}{2})^2}
\end{equation}

The value is zero for a completely imbalanced network and one for a perfectly symmetrical one. The denominator controls for the influence of variation in the dyad totals. The unnormalized version removes the denominator. Figure \ref{fig:structure_reciprocity_plot} (top) shows the observed reciprocity as a vertical line and simulated reciprocity based on posterior parameter distributions for the two Poisson models (D = 0 and D = 2), and based on the point estimate for the quasi-Stigler model. To simulate networks from the quasi-Stigler model we use a beta-binomial approximation with mean and variance matching the overdispersed binomial. The left panel shows normalized reciprocity while the right panel shows unnormalized values. 

The average reciprocity estimates are very similar for the two-dimensional and quasi-Stigler models. However, the variance of the two-dimensional model is slightly larger for normalized reciprocity and much larger for unnormalized (standard deviation of 14081 vs. 3628). As a result, the two-dimensional latent space model includes the observed value in its simulated range in both cases (p-values 0.008 (left) and 0.209 (right)), showing more appropriate model uncertainty. Only a fraction of this difference is due to the use of the posterior sample instead of a point estimate. Figure \ref{fig:structure_reciprocity_plot2} compares the distributions for D = 2 shown in Figure \ref{fig:structure_reciprocity_plot} to one for draws from the MKL parameter estimate. The variance from the point estimate is a little bit smaller (p-values drop to 0 and 0.156). The no-position Poisson model provides a lower estimate of normalized reciprocity since it lacks the symmetric effect of positions, but badly underestimates the unnormalized version, as it underestimates high edge values in general. 

<<sim_point_post, eval = F, echo = F, warning = F, message = F>>=

library(intergraph)

# the simulate.ergmm function randomly picks an mcmc sample to use for simulatution, i.e. simulating from the posterior distribution. ( models with no mcmc sample, e.g only "mle" fit, can't use simulate.ergmm)

# 1 simulate from point estimate - how much less varaibility? ####

model = latent.srp2.init.r
lambda.mkl = exp(model$mkl$receiver + t(model$mkl$sender -
                dist2(model$mkl$Z)) + model$mkl$beta); diag(lambda.mkl) = 0

nsim = 50
tmp = matrix(0,2209, nsim)
# average sets of graphs for each to smooth the poisson, i.e. nsim = nsim^2
for (i in 1:nsim) {
  tmp.j = matrix(0,2209, nsim)
  for (j in 1:nsim) {
    mat = matrix(rpois(2209, lambda), ncol = 47, nrow = 47); diag(mat) = NA
    tmp.j[,j] = mat - t(Cmatrix)
  }
  tmp[,i] = rowMeans(tmp.j)
}
tmp = apply(tmp, 1, quantile, c(.025,.25,.5,.75,.975), na.rm = TRUE)
tmp = tmp[,!is.na(tmp[1,])]
tmp = tmp[,order(tmp[3,])]
plot(tmp[3,s], type = "l")
points(tmp[1,s], type = "l")
points(tmp[5,s], type = "l")

# 2 simulate from graph posterior draw - slightly more variability (same as simulate function but this is for transparency) ####

nsim = 50
tmp1 = matrix(0,2209, nsim)
for (i in 1:nsim) {
  k = sample(1:length(model$sample$lpY), 1)
  lambda = exp(model$sample$receiver[k,] +
                 t(model$sample$sender[k,] -
                  dist2(model$sample$Z[k,,])) +
                 model$sample$beta[k])
  tmp1.j = matrix(0,2209, nsim)
  for (j in 1:nsim) {
    mat = matrix(rpois(2209, lambda), ncol = 47, nrow = 47); diag(mat) = NA
    tmp1.j[,j] = mat - t(Cmatrix)
  }
  tmp1[,i] = rowMeans(tmp1.j)
}
tmp1 = apply(tmp1, 1, quantile, c(.025,.25,.5,.75,.975), na.rm = TRUE)
tmp1 = tmp1[,!is.na(tmp1[1,])]
tmp1 = tmp1[,order(tmp1[3,])]
points(tmp1[3,s], type = "l", col = "red")
points(tmp1[1,s], type = "l", col = "red")
points(tmp1[5,s], type = "l", col = "red")

@
 
<<structure_reciprocity, eval = T, echo = F, cache = T, message =F, warning = F, results = 'hide', fig.height = 4>>=

reciprocity = function(A, normalize = T) {
 if(normalize) {sum(A*t(A), na.rm = T)/sum(((A+t(A))/2)^2, na.rm = T)}
  else {sum(A*t(A), na.rm = T)}
}

nsim = 1000

# 1. unnormalized ####

Q = reciprocity(t(Cmatrix), normalize = F)

# Poisson, QS
g = simulate.qs(g1, nsim = nsim)
#g = lapply(g, round)
tmp.qs = unlist( lapply(g, reciprocity, normalize = F))

# Poisson, D = 0
sim0 = simulate(latent.srp0, nsim) #draws from the posterior
tmp0 = unlist(lapply(sim0[[2]], function(x) {reciprocity(as.sociomatrix(x, attrname = "citations"), normalize = F)}) )

# Poisson, D = 2
sim2 = simulate(latent.srp2.init.r, nsim) #draws from the posterior
tmp2 = unlist(lapply(sim2[[2]], function(x) {reciprocity(as.sociomatrix(x, attrname = "citations"), normalize = F)}) )

# Poisson, D = 2, MKL point estimate
model = latent.srp2.init.r
lambda.mkl = exp(model$mkl$receiver + t(model$mkl$sender -
                dist2(model$mkl$Z)) + model$mkl$beta); diag(lambda.mkl) = 0
tmp2mkl = rep(0, nsim)
for (i in 1:nsim) {
  mat = matrix(rpois(2209, lambda.mkl), ncol = 47, nrow = 47); diag(mat) = 0 #0 instea of NA. NA causes all mod values to go to 0
  tmp2mkl[i] = reciprocity(mat, normalize = F)
}

# the simulate.ergmm function randomly picks an mcmc sample to use for simulatution, i.e. simulating from the posterior distribution.




# 2. normalized ####
Q.norm = reciprocity(t(Cmatrix), normalize = T)

# Poisson, QS
tmp.qs.norm = unlist( lapply(g, reciprocity, normalize = T))
tmp0.norm = unlist(lapply(sim0[[2]], function(x) {reciprocity(as.sociomatrix(x, attrname = "citations"), normalize = T)}) )
tmp2.norm = unlist(lapply(sim2[[2]], function(x) {reciprocity(as.sociomatrix(x, attrname = "citations"), normalize = T)}) )
tmp2mkl.norm = rep(0, nsim)
for (i in 1:nsim) {
  mat = matrix(rpois(2209, lambda.mkl), ncol = 47, nrow = 47); diag(mat) = 0 #0 instea of NA. NA causes all mod values to go to 0
  tmp2mkl.norm[i] = reciprocity(mat, normalize = T)
}

# the simulate.ergmm function randomly picks an mcmc sample to use for simulatution, i.e. simulating from the posterior distribution.



@

<<structure_reciprocity_plot, eval = T, echo = F, cache = T, message =F, warning = F, fig.height = 2, fig.width = 6, fig.pos = 'H', fig.cap = "Comparison of simulated reciprocity distributions based on 1000 simulations" >>=

rec.data = data.frame(model = c(rep("QS", nsim), rep("D = 0", nsim), rep("D = 2", nsim)), 
                      reciprocity = c(tmp.qs, tmp0, tmp2))
recplot1 = ggplot(rec.data, aes(x = reciprocity, fill = model)) +
  geom_histogram(position = "identity", bins = 40) +
  geom_vline(xintercept = Q, color = "gray", show.legend = T) + 
  geom_text(x=Q, label="observed", y = 250, colour="gray", angle=90, vjust = -.2, size=3) +
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  theme_bw() +
  ggtitle("Reciprocity") +  labs(x = NULL)
  
rec.data = data.frame(model = c(rep("QS", nsim), rep("D = 0", nsim), rep("D = 2", nsim)), 
                      normalized_reciprocity = c(tmp.qs.norm, tmp0.norm, tmp2.norm))
recplot1.norm = ggplot(rec.data, aes(x = normalized_reciprocity, fill = model)) +
  geom_histogram(position = "identity", bins = 40) +
  geom_vline(xintercept = Q.norm, color = "gray", show.legend = T) + 
  geom_text(x=Q.norm, label="observed", y = 80, colour="gray", angle=90, vjust = -.2, size=3) +
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  theme_bw() + 
  theme(legend.position = "none") +
  ggtitle("Normalized Reciprocity") + labs(x = NULL)

cowplot::plot_grid(recplot1.norm, recplot1, rel_widths = c(.8, 1), nrow = 1)

@

<<structure_reciprocity_plot2, eval = T, echo = F, cache = T, message =F, warning = F, fig.height = 2, fig.width = 6, fig.pos = 'H', fig.cap = "Comparison of simulated reciprocity distributions for two-dimensional Poisson posterior and MKL point estimate based on 1000 simulations" >>=

# Compare modularity distribution from simulating from MKL point estimate vs. posterior distribution

# Normalized ####
rec.data = data.frame(model = c(rep("D = 2", nsim), rep("D = 2, mkl", nsim)), 
                      reciprocity = c(tmp2.norm, tmp2mkl.norm))

recplot2.norm = ggplot(rec.data, aes(x = reciprocity, fill = model)) +
  geom_histogram(position = "identity", bins = 40) +
  geom_vline(xintercept = Q.norm, color = "gray", show.legend = T) + 
  geom_text(x=Q.norm, label="observed", y = 65, colour="gray", angle=90,
            vjust = -.2, size=3) +
  theme_bw() + facet_wrap(~model, scales = "fixed") + 
  theme(legend.position = "none") + 
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  scale_fill_manual(values = c(hue_pal()(3)[2], hue_pal()(4)[4])) +
  labs(x = "Normalized Reciprocity")

# Unnormalized ####

rec.data = data.frame(model = c(rep("D = 2", nsim), rep("D = 2, mkl", nsim)), 
                      reciprocity = c(tmp2, tmp2mkl))
recplot2 = ggplot(rec.data, aes(x = reciprocity, fill = model)) +
  geom_histogram(position = "identity", bins = 40) +
  geom_vline(xintercept = Q, color = "gray", show.legend = T) + 
  geom_text(x=Q, label="observed", y = 80, colour="gray", angle=90,
            vjust = -.2, size=3) +
  theme_bw() + facet_wrap(~model, scales = "fixed") + 
  theme(legend.position = "none") +
  scale_x_discrete(limits=c(360000, 400000) )+
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  scale_fill_manual(values = c(hue_pal()(3)[2], hue_pal()(4)[4])) +
  labs(x = "Reciprocity")

cowplot::plot_grid(recplot2.norm, recplot2, rel_widths = c(1, 1), nrow = 1)

@

<<structure_reciprocity_stats, eval = F, echo = F >>=
sum(tmp2<Q)
sum(tmp2mkl<Q)
sum(tmp.qs.norm<Q.norm)
sum(tmp0.norm<Q.norm)
sum(tmp2.norm<Q.norm)
sum(tmp2mkl.norm<Q.norm)

mean(tmp.qs)
mean(tmp2)
mean(tmp2mkl)
mean(tmp.qs.norm)
mean(tmp2.norm)

sd(tmp.qs)
sd(tmp0)
sd(tmp2)
sd(tmp2mkl)
sd(tmp2.norm)
sd(tmp2mkl.norm)

@

<<structure_exp_trans, eval = F, echo = F, cache = T, message =F, warning = F, results = 'hide'>>=

#Leave out transitive structure section - the statistic doesn't seem to capture what we would be interested in regarding transitivity for these nets. Also odd that the basic model overestimates transitivity by this metric.

# Some experimental structure stats for hierarchy and transitivity.
# Probalby won't include

# hierarchy ####
# exphi is measure for each node (i) the weighted average of citations received for nodes (j) i is connected to
# weighted by normalized out-citations for i.
exphi <- function(mat) {
  v = ( mat*(1/rowSums(mat)) ) %*% colSums(mat)
  v = sort(v)
}

tmp2 = lapply(sim2[[2]], function(x) {
  exphi(as.sociomatrix(x, directed = T, attrname = "citations"))
})
qua2 = apply(matrix(unlist(tmp2), nrow = n, ncol = nsim), 1, quantile, c(.25, .95))

tmp0 = lapply(sim0[[2]], function(x) {
  exphi(as.sociomatrix(x, directed = T, attrname = "citations"))
})
qua0 = apply(matrix(unlist(tmp0), nrow = n, ncol = nsim), 1, quantile, c(.25, .95))

tmp.qs = lapply(g[[2]], function(x) {
  exphi(as.sociomatrix(x, directed = T, attrname = "citations"))
})
qua.qs = apply(matrix(unlist(tmp.qs), nrow = n, ncol = nsim), 1, quantile, c(.25, .95))

par(mfrow = c(1,3))
plot(exphi(t(Cmatrix)), type = "l", lwd = 3)
points(qua.qs[1,], type = "l", col = "blue")
points(qua.qs[2,], type = "l", col = "blue")
plot(exphi(t(Cmatrix)), type = "l", lwd = 3)
points(qua2[1,], type = "l", col = "red")
points(qua2[2,], type = "l", col = "red")
plot(exphi(t(Cmatrix)), type = "l", lwd = 3)
points(qua0[1,], type = "l", col = "green")
points(qua0[2,], type = "l", col = "green")
#points(exphi(QSmat), col = "blue", type = "l")

# transitivity ####
# exptrans measure the mean connectedness among the nodes that (i) is most connected to (determined by cutoff)
# not a great comparison since qs fixes total citations on a dyad.
exptrans <- function(Y, cutoff = 20) {
  N = nrow(Y)
  diag(Y) = rep(0, N)
  tmp = 1:N
  for(i in 1:N) {
    # mean %age inter-connection between journals strongest connected to i
     # i.e. the journals that send greatest % of their citation to i
    j = sort(Y[,i]/rowSums(Y), decreasing = TRUE)[1:cutoff]
    ind = which(Y[,i]/rowSums(Y)>=j[cutoff])
    tmp[i] = mean((Y/rowSums(Y))[ind,ind])
  }
  if (!is.null(row.names(Y))) names(tmp) = row.names(Y)
  return(tmp)
}

plot(sort(exptrans(t(Cmatrix), cutoff)), ylim = c(0,.06), type = "l")
points(sort(exptrans(predict(latent.srp0), cutoff)), col = "red", type = "l")
points(sort(exptrans(predict(latent.srp2), cutoff)), col = "green", type = "l")
points(sort(exptrans(predict(latent.srp2.h, type = "start"), cutoff)), col = "magenta", type = "l")
points(sort(exptrans(QSmat, cutoff)), col = "blue", type = "l")
@

\subsubsection{Sensitivity} \label{sensitivity}

<<hyperpriors, eval = T, cache = T, echo = F, results = 'hide', warning= F, message = F, fig.keep='none', dependson= c("llik_and_latent_qn")>>=

# Which parameters could we adjust for more realistic priors?
# We consider putting narrower priors on the variance and intercept parameters

# Adjust the models ####

N = 47; d = 2
sender.var.df = 2*3 #default 3 ** shrink sender range
receiver.var.df  = 2*3 #default 3 ** shrink receiver range
Z.var.df = 2*sqrt(N)  #default sqrt(N) ** shrink z range

prior.sender.var = .5 #default 1
prior.receiver.var = .5
prior.Z.var = N/32 #default N/8 ** shrink sigma_z range
beta.var = 1 #default 9

quantile(rinvchisq(100000, sender.var.df, prior.sender.var), probs = c(0.05, .5, .95))
quantile(rinvchisq(100000, Z.var.df, prior.Z.var), probs = c(0.05, .5, .95))

samp1 = rep(0, 500)
median1 = rep(0, length(samp1))
mean1 = rep(0, length(samp1))
N = 47
for (i in 1:length(samp1)) {
  # higher df condenses it
  # higher var moves mode from 0
  sender.var = rinvchisq(1, sender.var.df, prior.sender.var)
  receiver.var = rinvchisq(1, receiver.var.df, prior.receiver.var)
  Z.var = rinvchisq(1, Z.var.df, prior.Z.var)
  object = list(map = list(
    beta = rnorm(1, 0, sd = sqrt(beta.var)),
    sender = rnorm(N, 0, sqrt(sender.var)),
    receiver = rnorm(N, 0, sqrt(receiver.var)),
    Z = matrix(rnorm(N*d, 0, sqrt(Z.var)), nrow = N, ncol = d)
  ))
  median1[i] = median(predict.lsqn(object, type = "rpois"), na.rm = T)
  mean1[i] = mean(predict.lsqn(object, type = "rpois"), na.rm = T)
}
Z_dist = dist2(object$map$Z)
summary(median1)
summary(mean1)

set.seed(1) #best of a few attempts
latent_qn2.h = lsqn(Y = t(Cmatrix), N = 47, D = 2, runs = 50, tol = .01, Z.init = "rnorm",
     v_a = sender.var.df, v_b = receiver.var.df, v_z = Z.var.df,
     s2_a = prior.sender.var, s2_b = prior.receiver.var, s2_z = prior.Z.var,
     sigma2_B = beta.var ) 

range(latent_qn2.h$map$diff_Z); range(latent_qn2.h$map$diff_Z2)
@

<< latent_hyperpriors, eval = T, cache = T, echo = F, results = 'hide', warning= F, message = F, fig.keep='none', dependson= c("llik_and_latent_qn")>>=

latent.srp2.h = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver, 
                    response = "citations", family = "Poisson.log", seed = 30,
                    prior = ergmm.prior(sender.var.df = sender.var.df,
                                        receiver.var.df = receiver.var.df,
                                        Z.var.df = Z.var.df,
                                        sender.var = prior.sender.var,
                                        receiver.var = prior.receiver.var, 
                                        Z.var = prior.Z.var,
                                        beta.var = beta.var),
                    control = ergmm.control(burnin = 50000, interval = 500,
                                            sample.size = 5000, mle.maxit = 100,
                                            refine.user.start = F),
                    user.start = with(latent_qn2.h$map, list(Z=Z, sender=sender, receiver=receiver,
                                            beta = beta, sender.var = sender.var,
                                            receiver.var = receiver.var, Z.var = Z.var))
                    )

# Analyze ####

model = latent.srp2.init.r
sd2 = apply(model$sample$receiver - model$sample$sender, 2, sd)
latent2 = model$mkl$receiver - model$mkl$sender
centipede1 = t(cbind(latent2, latent2 - 1.96*sd2, latent2 + 1.96*sd2))
colnames(centipede1) = Cnet%v%"vertex.names"

model = latent.srp2.h
sd.h = apply(model$sample$receiver - model$sample$sender, 2, sd)
latent.h = model$mkl$receiver - model$mkl$sender
centipede.h = t(cbind(latent.h, latent.h - 1.96*sd.h, latent.h + 1.96*sd.h))
colnames(centipede.h) = Cnet%v%"vertex.names"

par(mfrow = c(1,2))
centipede.plot(centipede1, vgrid = c(-1,0,1,2), cex = .7, right.labels = round(latent2, 2),
               bg = "red", main = "Original Hyperpriors")
centipede.plot(centipede.h, vgrid = c(-1,0,1,2), cex = .7, right.labels = round(latent.h, 2),
                bg = "red", main = "Adjusted Hyperpriors")
cor(latent2, latent.h)
cor(sd.h^2, sd2^2)

@

As with any Bayesian model we must consider the sensitivity of estimates to the assumptions contained in the prior structure and hyperprior values. However, we find such a strong correlation (0.999) between the rating scores resulting from the MLE and MKL estimates that there is little reason to suspect undue influence from the priors. Still, we consider shrinking the prior intercept variance to one, and adjustments to the hyperprior degrees of freedom and variance. These adjustments narrow the prior distributions on variance parameters to more realistic ranges based on previous results. The middle 90 percent intervals drop to roughly $[0.2, 1.8]$ instead of $[0.4, 8.5]$ for sender and receiver variances, and $[0.9, 3.2]$ instead of $[2.9, 20]$ for position variance. The impact on the results is trivial. The updated estimates of scores and their variances barely change from the previous estimates (correlation $> 0.99$).

\subsubsection{Comparison of Estimation Methods} \label{estimeval}

<<compare_latent_models, eval = T, echo = F, cache=T, results='asis', cache.comments = FALSE, warning= FALSE, dependson = c("latent_sr2_init_r", "latent_sr2", "latent_qn")>>=

results.table = data.frame(row.names = c("Quasi-Newton", "optim (L-BFGS-B)", "MCMC (MKL)", "MCMC.init.r (MKL)" ))

results.table$"Time (min)" = c("$<1$", "$<1$", round(t2.srp2 - t1.srp2), round(t2.init.r - t1.init.r))

results.table$"$l(Y|\\hat{\\theta})$" = as.character(round(
                                c(llik(latent_qn$map, Y = t(Cmatrix), est = "Y"), 
                                llik(latent.srp2.init.r$burnin.start, Y = t(Cmatrix), est = "Y"),
                                #latentnet finds a conditional posterior mode using optim before burnin
                                llik(latent.srp2$mkl, Y = t(Cmatrix), est = "Y"),
                                llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y"))))
results.table$"$l(\\hat{\\theta})$" = as.character(round(
                                c(llik(latent_qn$map, Y = t(Cmatrix), est = "theta"),
                                llik(latent.srp2.init.r$burnin.start, Y = t(Cmatrix), est = "theta"),
                                with(latent.srp2$mcmc.mle, llik(latent.srp2$mkl,
                                     Y = t(Cmatrix), sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")),
                                with(latent.srp2.init.r$mcmc.mle, llik(latent.srp2.init.r$mkl,
                                     Y = t(Cmatrix), sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")))))
results.table$"$l(\\hat{\\theta}|Y)$" = as.character(as.numeric(results.table$"$l(Y|\\hat{\\theta})$") + as.numeric(results.table$"$l(\\hat{\\theta})$"))

results.table$sample = c("No", "No", "Yes", "Yes")
results.table$code = c("R", "C", "C", "C")
latent2 = latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender
results.table$"Rating Cor." = c(cor(latent2, latent_qn$map$receiver - latent_qn$map$sender),
                               cor(latent2, latent.srp2.init.r$burnin.start$receiver - 
                                            latent.srp2.init.r$burnin.start$sender),
                               cor(latent2, latent.srp2$mkl$receiver - latent.srp2$mkl$sender),
                               cor(latent2, latent.srp2.init.r$mkl$receiver - 
                                            latent.srp2.init.r$mkl$sender))

print(xtable(results.table, caption = "Estimation Methods", digits = 4, label = "results.table"), caption.placement = "top", sanitize.text.function=function(x){x})

# MCMC diagnostics ####

  # these all look fine
  # mcmc.diagnostics(latent.srp2)
  # mcmc.diagnostics(latent.srp2.init.r)
  # mcmc.diagnostics(latent.srp2.init.r.sann)

# Other estimates ####
# est = "Y"
# llik(latent.srp2.init.r$mcmc.mle, Y = t(Cmatrix), est = est)
# llik(latent.srp2.init.r$mcmc.pmode, Y = t(Cmatrix), est = est)
# with(latent.srp2.init.r.bfgs$mcmc.mle, llik(latent.srp2.init.r.bfgs$mkl, Y = t(Cmatrix), sender.var = sender.var, receiver.var = receiver.var,  Z.var = Z.var, est = est))
#llik(latent.srp2.init.r.bfgs$mcmc.mle, Y = t(Cmatrix), est = est)
#llik(latent.srp2.init.r.bfgs$mcmc.pmode, Y = t(Cmatrix), est = est)

#same as sann
#with(latent.srp2.init.r.bfgs, llik(t(Cmatrix), mkl$sender, mkl$receiver, mkl$beta, mkl$Z,  mcmc.mle$sender.var, mcmc.mle$receiver.var, mcmc.mle$Z.var,est = est))

# best mcmc.mle estimate consistently a little workse than mkl
  # with(latent.srp2$mcmc.mle, llik(t(Cmatrix), sender, receiver, beta, Z,sender.var, receiver.var, Z.var, est = est))
  # with(latent.srp2.init.r.sann$mcmc.mle, llik(t(Cmatrix), sender, receiver, beta, Z, sender.var, receiver.var, Z.var, est = est))
  # with(latent.srp2.init.r.bfgs$mcmc.mle, llik(t(Cmatrix), sender, receiver, beta, Z, sender.var, receiver.var, Z.var, est = est))
 # even worse with #(latent.srp2, llik(Y, mkl$sender, mkl$receiver, mkl$beta, mkl$Z, sum(mkl$sender^2 + v_a*s2_a^2) / (N + 2 + v_a), sum(mkl$receiver^2 + v_b*s2_b^2) / (N + 2 + v_b), sum(as.vector(mkl$Z)^2 + v_z*s2_z^2) / (N*d + 2 + v_z), est = "MAP"))

# Compare positions (with or without procrustes) ####
#plot(latent.srp2.init.r$mkl$Z, col = col2, pch = 16)
#plot(vegan::procrustes(latent.srp2.init.r$mkl$Z, latent.srp2.init.r$burnin.start$Z)$Yrot, col = col2, pch = 16)

@

Table \ref{results.table} compares the results for the two-dimensional Poisson latent space model with four different estimations techniques: 1) The quasi-Netwon method outlined in section \ref{QN}; 2) The limited-memory bounded quasi-Newton method implemented by the \code{optim} function in the \proglang{R} base package \pkg{stats}. This is the intermediate optimization method of \pkg{latentnet}; 3) MCMC estimation as implemented by \pkg{latentnet}; and 4) MCMC as in method three but with random initialization as used in quasi-Newton method one. For MCMC methods we check for convergence and appropriate MCMC interval and burn-in values using the \code{mcmc.diagnostics} function of \pkg{latentnet}. (The MCMC method of row three seems to converge based on diagnostic plots, but could potentially achieve the estimate of method four with a much longer burn-in.) For our quasi-Netwon method we check that all calculated first derivatives are approximately zero and all second derivatives are negative. The log likelihoods shown in columns two through four are up to a constant value.

We present MKL estimates from MCMC methods because they have higher posterior and graph likelihood than other point estimates, such as the posterior mode. Results in previous sections have all drawn on the MKL estimates from the MCMC estimation with random initialization (row four). The MCMC methods are much slower than the quasi-Newton methods, although they have the advantage of returning a posterior sample. The much faster estimates from the quasi-Newton methods are very highly correlated with these results (column four). For larger networks where we are primarily interested in a point estimate, the quasi-Netwon methods are probably preferable.

Timing of computations was in R 3.3.2 with a MacBookPro, 2.6 GHz Intel Core i5 processor with 8 GB 1600 MHz DDR3 memory without. The ``code'' column indicates if the main operations were performed in \proglang{R} or \proglang{C}. Although not employed here, \pkg{latentnet} does allow for parallel processing of multiple MCMC chains. The estimates should be considered ballpark for each method, as the number of iterations for the quasi-Netwon methods and control parameters for the MCMC could be augmented to achieve some speed-up. However, the process of optimizing these would mostly likely negate the time benefits. 

<<journal_simulation_study, eval = F, echo = F>>=

#tried adding precision but that didn't help.
#tried using qn estimates to start and that didn't help

trials = 50; tol = .01 
tmp = list()
for(i in 19:22) {
  s = predict.lsqn(latent_qn, type = "rpois"); diag(s) = 0
  q  = lsqn(s, runs = 50, tol = tol)$map
  tmp[[i]] = q
  print(i)
}

data0= data.frame(journal = rep(rownames(Cmatrix), 50),
                   sender = unlist(lapply(tmp, function(x) {x$sender - latent_qn$map$sender})),
                   receiver = unlist(lapply(tmp, function(x) {x$receiver - latent_qn$map$receiver})),
                   beta = rep(unlist(lapply(tmp, function(x) {x$beta - latent_qn$map$beta})), each = 47))

ggplot(data0, aes(x = journal, y = sender + receiver + beta, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))

#variance w/ sender - receiver
ggplot(data0, aes(x = journal, y = receiver - sender + beta, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))
   

trials = 300; tol = .001 #tried adding precision but that didn't help.
#fits = list()
#for(i in 1:trials) {
#  s = simulate(latent.srp2.init.r$model, par = latent.srp2.init.r$mkl)
#  m = as.sociomatrix(s, ignore.eval = F, attrname = "citations")
#  q  = lsqn(m, runs = 50, tol = tol)$map
#  fits[[i]] = q
#  print(i)
#}

# check for diffz outsize tol - bad if you don't cut those out
# fails = unlist(lapply(fits, function(x) {max(abs(x$diff_Z)) > tol}))
# fits2 = fits[!fails]

#load saved version
fits2 = readRDS("fits2.rds")



# compare. 
# shifts in sender or receiver don't matter. 
model = latent.srp2.init.r$mkl

#variance w/ sender + receiver + beta
data1 = data.frame(journal = rep(rownames(Cmatrix), sum(!fails)),
                   sender = unlist(lapply(fits2, function(x) {x$sender - model$sender})),
                   receiver = unlist(lapply(fits2, function(x) {x$receiver - model$receiver})),
                   beta = rep(unlist(lapply(fits2, function(x) {x$beta - model$beta})), each = 47))

ggplot(data1, aes(x = journal, y = sender + receiver + beta, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))

#variance w/ sender - receiver
ggplot(data1, aes(x = journal, y = receiver - sender + beta, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))
   
# positions
Zp = lapply(fits2, function(x) {vegan::procrustes(model$Z, x$Z)$Yrot })
plot(model$Z, type = "n")
text(model$Z, col = "red")
for (i in 1:length(fits2)) {
  text(vegan::procrustes(model$Z, fits[[i]]$Z)$Yrot, as.character(1:47), cex = .5)
}

trials = 30
fits.h = list()
for(i in 1:trials) {
  s = simulate(latent.srp2.init.r$model, par = latent.srp2.init.r$mkl)
  m = as.sociomatrix(s, ignore.eval = F, attrname = "citations")
  q  = lsqn(m, runs = 50, tol = .01, v_a = 1, v_b = 1, v_z = 1,
              s2_a = 10, s2_b = 10, s2_z = nrow(m), sigma2_B = 9)$map
  fits.h[[i]] = q
  print(i)
}

fails = unlist(lapply(fits.h, is.null))
fits.h = fits.h[!fails] #removes 10
fails = unlist(lapply(fits.h, function(x) {max(abs(x$diff_Z)) > tol}))
fits.h = fits.h[!fails] #removes 155
data2 = data.frame(journal = rep(rownames(Cmatrix), length(fits.h)),
                   sender = unlist(lapply(fits.h, function(x) {x$sender - model$sender})),
                   receiver = unlist(lapply(fits.h, function(x) {x$receiver - model$receiver})),
                   beta = rep(unlist(lapply(fits.h, function(x) {x$beta - model$beta})), each = 47))

gsender = ggplot(data2, aes(x = journal, y = sender, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))

greceiver = ggplot(data2, aes(x = journal, y = receiver, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))

grate = ggplot(data2, aes(x = journal, y = receiver - sender + beta, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))

cowplot::plot_grid(gsender, greceiver, grate, nrow = 3)
#variance w/ sender - receiver
ggplot(data2, aes(x = journal, y = receiver - sender + beta, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1))

# positions
plot(model$Z, type = "n")
text(model$Z, col = "red")
for (i in 1:trials) {
  text(vegan::procrustes(model$Z, fits.h[[i]]$Z)$Yrot, as.character(1:47), cex = .5)
}
@

%If data for related fields such as economics and applied math becomes available (which it is not yet), we could employ such a covariate strategy between fields, or the clustering model of \citet{handcock07} if the network structure allows.  % In the next application we fit a latent space model to movie data that spans genres and different levels of connectivity.
%%The average computational journal as categorized by \citet{varinetal} has 419 out-citation in the network while the average ecological journal has only 115. A covariate could be added to the model to augment the expected counts within or between subfields.
%The value of journals that cite heavily outside of statistics, such as the \textit{Stata Journal} which cites many economic journals, could also be downweighted.


\section{Movie Rating and Genre Identification}\label{Example2}

In this section we apply latent space rating to a set of films with viewer-supplied star ratings. Film rating systems based on viewer ratings are biased by the fact that viewers do not choose movies to rate randomly, and they may skew high or low in their ratings. In addition, average ratings obscure the information provided by the volume of ratings for different films. The latent space rating method efficiently addresses those challenges. The concomitant visualizations aid in genre detection and enhance our ability to explore and compare related films.  Through a dynamic network plot we increase the amount of data we can present. Movie ratings are extremely subjective, so without knowing detailed individual preferences the ability to explore a network of films may be more useful than finding ``correct'' ratings. We compare our results to user star averages. 

%See full code in movie_example.R

%1M Benchmark Data Set
%http://files.grouplens.org/datasets/movielens/ml-1m-README.txt
%Permalink: http://grouplens.org/datasets/movielens/1m/

\subsection{Data}\label{Data2}

Our data came from the MovieLens data set \citep{movielens}, collected by the GroupLens Research Project at the University of Minnesota. It consisted of about one million ratings of 3,952 movies from approximately 6,000 users who joined MovieLens in 2000. The data were released in 2003. Ratings were on a one-to-five integer scale with five being the best. Each included user supplied at least 20 ratings.

To convert the data into a network format we aggregated differences in individual users' ratings to form a $3952 \times 3952$ ratings-difference matrix. To be explicit, entry $i, j$ in the matrix represented the sum of positive values of $rating(j) - rating(i)$ over users who rated both movies. (If $rating(i) > rating(j)$ the difference was added to entry $j,i$.) For example, if a user prefers movie $j$ to $i$ by one point, then one is added to entry $i, j$: $i$ ``sends'' a point to $j$. The corresponding network is positive-valued and directed. 
%note the data isn't quite coints, as the differences range between 1 and 4, but most are 1 or 2. Overdispersed?

To illustrate certain points without too much computational burden we restricted ourselves to a subset of the MovieLens data, retaining only movies assigned genre ``Action'', ``Crime'', ``Western'' or some combination therein. (Only two of the possible combinations are present in the data.) We removed a small number of isolates before modeling. The resulting network has 128 nodes and 11,393 edges.

\subsection{Latent Space Model}\label{Model2}

%full movie net is saved as movie_net.rds
<<r movie_data, eval = T, echo = F, cache = T>>=

movie = readRDS("~/Documents/citation/latent_ranking/movie_output.RDS")
movie_net_acw = movie$movie_net_acw
movie_net_acw_full = movie$movie_net_acw_full
movie_net_watch_full = movie$movie_net_watch_full
vc.acw = movie$vc.acw
latent_acw2 = movie$latent_acw2

acw_mat = as.sociomatrix(movie_net_acw_full, attrname = "ratings_diff", ignore.eval = F)
rownames(acw_mat) = movie_net_acw_full%v%"titles"
@

<<r movie_model, cache = T, eval = T, echo = F, results = 'hide', highlight = F, warning = F>>=

t1.acw2 = Sys.time()
latent_acw2 = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender, 
             response = "ratings_diff", family = "Poisson.log",
             control = control.ergmm(pilot.runs = 4,  burnin = 500000, 
             interval = 500, sample.size = 5000), seed = 123)
t2.acw2 = Sys.time()

@

% Settings too low, mcmc diagnostics bad
<<r movie_model_b, cache = T, eval = F, echo = F, results = 'hide', highlight = F, warning = F>>=

#try shorter burnin
t1.acw2b = Sys.time()
latent_acw2b = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender, 
             response = "ratings_diff", family = "Poisson.log",
             control = control.ergmm(pilot.runs = 8,  burnin = 100000, 
             interval = 100, sample.size = 1000), seed = 123)
t2.acw2b = Sys.time()

@

%even with a very long burning (5mill, took about 6hrs with interval 500, sample 10000) the final map only increases by a few over initial direct optimization

<<r movie_model_qn, cache = T, eval = T, echo = F, results = "hide", fig.keep = "none">>=

N = nrow(acw_mat)

#Default Hyperpriors: ####
set.seed(1)
t1.acw.qn = Sys.time()
acw.qn = lsqn(acw_mat, runs = 100, tol = .01, Z.init = "rnorm")
t2.acw.qn = Sys.time()
range(acw.qn$map$diff_Z); range(acw.qn$map$diff_Z2); acw.qn$map$llik #471435.1

#Adjust Hyperpriors: ####
set.seed(5)
t1.acw.qn.h = Sys.time()
acw.qn.h = lsqn(acw_mat, runs = 100, tol = .01, Z.init = "rnorm",
              v_a = 6, v_b  = 6, #default 3 
              v_z = sqrt(N)/4,  #default sqrt(N)
              s2_a = 1, s2_b = 1, #default 1
              s2_z = 1, #default N/8 ** shrink sigma_z range
              sigma2_B = 3) #default 9)
t2.acw.qn.h = Sys.time()
#quantile(rinvchisq(100000, 3, 1), probs = c(0.05, .5, .95)) #.8, 2
#quantile(rinvchisq(100000, sqrt(N)/4, 1), probs = c(0.05, .5, .95)) #1.3 
range(acw.qn.h$map$diff_Z); range(acw.qn.h$map$diff_Z2); acw.qn.h$map$llik 

# Evaluate
est = "MAP"
llik(acw.qn$map, Y = acw_mat, est = est)
llik(acw.qn.h$map, Y = acw_mat, est = est)
llik(latent_acw2$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2$mkl, Y = acw_mat, est = est)

xlim = c(-4,4); 
plot(acw.qn$map$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16, xlim = xlim)
plot(acw.qn.h$map$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16, xlim = xlim)
plot(latent_acw2$mkl$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16, xlim = xlim)
plot(latent_acw2$mcmc.mle$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16, xlim = xlim)


@

<<r movie_model_r, eval = T, echo = F, results = 'hide', highlight = F, cache = T, dependson = c("movie_data"), warning = F>>=

M = acw_mat
N = nrow(M); n = N; D= 2; d = 2; 
v_z = sqrt(N); s2_z = N/16
Z_dist = dist2(M)
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
sigma2_z = var(as.vector(Z))
a = rnorm(N)
b = rnorm(N)
sigma2_a = var(a)
sigma2_b = var(b)

t1.acw2.r = Sys.time()
latent_acw2.r = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender,
                      response = "ratings_diff", family = "Poisson.log",
                      control = control.ergmm(pilot.runs = 8,  burnin = 500000,
                      interval = 500, sample.size = 5000), seed = 123,
                      user.start = list(Z = Z, sender = a, receiver = b, 
                                        sender.var = sigma2_a,
                                        receiver.var = sigma2_b,
                                        Z.var = sigma2_z))
t2.acw2.r = Sys.time()
@

<<r movie_model_init_qn, eval = T, echo = F, results = 'hide', highlight = F, cache = T, dependson = c("movie_data"), warning = F>>=

t1.acw2.init.qn = Sys.time()
latent_acw2.init.qn = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender,
                      response = "ratings_diff", family = "Poisson.log",
                      control = control.ergmm(pilot.runs = 8,  burnin = 500000,
                      interval = 500, sample.size = 5000), seed = 123,
                      user.start = with(acw.qn.h$map, list(Z=Z, sender=sender, receiver=receiver,
                                             beta = beta, sender.var = sender.var,
                                             receiver.var = receiver.var, Z.var = Z.var))
                      )
t2.acw2.init.qn = Sys.time()
@

<<r movie_model_h, eval = T, echo = F, results = 'hide', highlight = F, cache = T, dependson = c("movie_data"), warning = F>>=

#adjustment to hyperpriors from default

t1.acw2.h = Sys.time()
latent_acw2.h = ergmm(  movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender, 
                        response = "ratings_diff", family = "Poisson.log", seed = 123,
             
                        prior = ergmm.prior(sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                        sender.var = 1,
                                        receiver.var = 1, 
                                        Z.var = 1,
                                        beta.var = 3),
                        
                        control = control.ergmm(pilot.runs = 8,  burnin = 500000, 
                        interval = 500, sample.size = 5000, mle.maxit = 100)

                       )
  
t2.acw2.h = Sys.time()


@

%for comparison tried without positions and all high-count films get pulled to the center, as expected
<<movie_model_noRE, eval = F, echo = F, warning = F, message = F>>=
M = acw_mat
N = nrow(M); n = N; D= 2; d = 2; 
v_z = sqrt(N); s2_z = N/16
Z_dist = dist2(M)
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
sigma2_z = var(as.vector(Z))

t1 = Sys.time()
latent_acw2.noRE = ergmm(movie_net_acw_full ~ euclidean(d=2),
                      response = "ratings_diff", family = "Poisson.log",
                      control = control.ergmm(pilot.runs = 8,  burnin = 500000,
                      interval = 500, sample.size = 5000), seed = 123,
                      user.start = list(Z = Z, Z.var = sigma2_z), verbose = 3)
t2 = Sys.time()
@

<<r movie_model_eval, eval = T, echo = F, message = F, warning = F, dependson = ("movie_data")>>=
library(latentnet)

#mcmc.diagnostics(latent_acw2)
#mcmc.diagnostics(latent_acw2b)
#mcmc.diagnostics(latent_acw2.r)
#mcmc.diagnostics(latent_acw2.h)

#par(mfrow = c(1,2))
#plot(latent_acw2$mkl$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16)
#plot(latent_acw2.h$mkl$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16)

@

By modeling the network of differences in ratings we capture the tendency for a movie to be frequently and consistently rated above movies that draw an overlapping audience. As above, we apply the latent space Poisson model in two dimensions to facilitate visualization. Results are presented in Table \ref{movie.table}. The methods presented in the first four rows are the same as in Table \ref{results.table}. The MKL estimates are again are the best of the estimates produced by the MCMC estimation. For comparison, we include in rows five and six the MCMC MLE and MCMC posterior mode, the maximum likelihood and maximum posterior probability sample iterations. Although the fit from MCMC estimation is not sensitive to initialization method in this example, %default, random, and quasi-Newton initialization all about the same
the time is somewhat affected. In addition, the bounded quasi-Netwon method (row two) achieves better results in less time (about two minutes instead of ten) when using the same initialization as  our quasi-Netwon method.

As above we consider adjustments to the hyperpriors that narrow the range of prior variance distributions and find the impact on ratings to be trivial (row 7). However, the hyperprior adjustments do benefit the speed of fit and the visualization, by limiting the amount that very low-connectivity nodes drift from the rest. These results are subsequently used for visualization and analysis. For our our quasi-Newton method the use of more restrictive priors improves algorithm performance even when the output is evaluated on the scale of more diffuse priors, so this estimate is show in row one of the table. The calculations in columns three and four of Table \ref{movie.table} assume consistent diffuse priors.

<<r movie_table, echo = F, results = "asis", dependson = c("movie_model", "movie_model_r", "movie_model_qn", "movie_model_h")>>=
library(xtable)

movie.table = data.frame(row.names = c("quasi-Newton",
                                       "optim (L-BFGS-B)",
                                       "MCMC (MKL)",
                                       "MCMC (MLE)",
                                       "MCMC (P.mode)",
                                       "MCMC.init.r (MKL)",
                                       "MCMC.h (MKL)"))
movie.table$"Time (min)" = c(as.character(round(difftime(t2.acw.qn.h, t1.acw.qn.h, units = "mins"), 1)), "2",  
                             as.character(round(difftime(t2.acw2, t1.acw2, units = "mins"))), "-", "-",
                             as.character(round(difftime(t2.acw2.r, t1.acw2.r, units = "mins"))),
                             as.character(round(difftime(t2.acw2.h, t1.acw2.h, units = "mins"))))
movie.table$"$l(Y|\\hat{\\theta})$" = as.character(round(
                                c(llik(acw.qn.h$map, Y = acw_mat, est = "Y"), 
                                llik(latent_acw2.r$burnin.start, Y = acw_mat, est = "Y"),
                                llik(latent_acw2$mkl, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.r$mkl, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.h$mkl, Y = acw_mat, est = "Y"))))

movie.table$"$l(\\hat{\\theta})$" = as.character(round(
                                c(llik(acw.qn.h$map, Y = acw_mat, est = "theta"),
                                llik(latent_acw2.r$burnin.start, Y = acw_mat, est = "theta"),
                                with(latent_acw2$mcmc.mle, llik(latent_acw2$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")),
                                llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = "theta"),
                                llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = "theta"),
                                with(latent_acw2.r$mcmc.mle, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")),
                                with(latent_acw2.r$mcmc.mle, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")))))

movie.table$"$l(\\hat{\\theta}|Y)$" = as.character(as.numeric(movie.table$"$l(Y|\\hat{\\theta})$") + as.numeric(movie.table$"$l(\\hat{\\theta})$"))

movie.table$sample = c("No", "No", "Yes", "-", "-", "Yes", "Yes")
movie.table$code = c("R", "C", "C", "-", "-","C", "C")

acw2 = latent_acw2.r$mkl$receiver - latent_acw2.r$mkl$sender
movie.table$"Rating Cor." = c(cor(acw2, acw.qn.h$map$receiver - acw.qn.h$map$sender),
                               cor(acw2, latent_acw2$burnin.start$receiver - 
                                            latent_acw2$burnin.start$sender), 
                               cor(acw2, latent_acw2$mkl$receiver - latent_acw2$mkl$sender),
                               cor(acw2, latent_acw2$mcmc.mle$receiver - latent_acw2$mcmc.mle$sender),
                               cor(acw2, latent_acw2$mcmc.pmode$receiver - latent_acw2$mcmc.pmode$sender),
                               cor(acw2, latent_acw2.r$mkl$receiver - 
                                            latent_acw2.r$mkl$sender),
                               cor(acw2, latent_acw2.h$mkl$receiver - 
                                            latent_acw2.h$mkl$sender)
                             )

print(xtable(movie.table, caption = "Estimation Methods", digits = 4, label = "movie.table"),
      sanitize.text.function=function(x){x})
@

<<r movie_results_likelihoods, eval = F, echo = F, results = "hide">>=

cor(latent_acw2.r$mkl$receiver - latent_acw2.r$mkl$sender, latent_acw2.h$mkl$receiver - latent_acw2.h$mkl$sender)
est = "Y"
llik(acw.qn$map, Y = acw_mat, est = est)
llik(acw.qn.h$map, Y = acw_mat, est = est)

llik(latent_acw2$burnin.start, Y = acw_mat, est = est) 
llik(latent_acw2.r$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.h$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$burnin.start, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.mle, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.pmode, Y = acw_mat, est = est)

llik(latent_acw2$mkl, Y = acw_mat, est = est) #second best
llik(latent_acw2.r$mkl, Y = acw_mat, est = est)
llik(latent_acw2.h$mkl, Y = acw_mat, est = est) #best
llik(latent_acw2.init.qn$mkl, Y = acw_mat, est = est)

est = "MAP"
llik(acw.qn$map, Y = acw_mat, est = est)
llik(acw.qn.h$map, Y = acw_mat, est = est)

llik(latent_acw2$burnin.start, Y = acw_mat, est = est) #worst
llik(latent_acw2.r$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.h$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$burnin.start, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.mle, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.pmode, Y = acw_mat, est = est)

with(latent_acw2$mcmc.mle, llik(latent_acw2$mkl, Y = acw_mat, est = est, #best
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))
with(latent_acw2.r$mcmc.mle, llik(latent_acw2$mkl, Y = acw_mat, est = est,
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))
with(latent_acw2.h$mcmc.mle, llik(latent_acw2.h$mkl, Y = acw_mat, est = est,
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))
with(latent_acw2.init.qn$mcmc.mle, llik(latent_acw2.init.qn$mkl, Y = acw_mat, est = est,
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))

#compare p(theta) under adjusted hyperpriors
tmp = as.character(round(
                                c(llik(acw.qn.h$map, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                        prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                llik(latent_acw2.r$burnin.start, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                               prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                with(latent_acw2$mcmc.mle, llik(latent_acw2$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                               prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3)),
                                llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                             prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                              prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                with(latent_acw2.r$mcmc.mle, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                                prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3)),
                                with(latent_acw2.r$mcmc.mle, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                               prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3)))))

@
% larger movie network

<< latent_watch_init_r, eval = F, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=

W = as.sociomatrix(movie_net_watch_full, ignore.eval = F, attrname = "ratings_diff")
#random initation of sender and receiver, and scaling Z helps the model fit
D = 2; N = nrow(W)
Z_dist = dist2(t(W))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

#I tried this with 50k, 500, 5k and the sampling step took >  24 hours, stopped it
#mle.maxit 50 is probably enough

latent.watch.init.r = ergmm(movie_net_watch_full ~ euclidean(d = 2) + rsender + rreceiver,
                              response = "ratings_diff", family = "Poisson.log", seed = 30,
                              tofit = c("mcmc", "mkl", "procrustes", "mle"),
                              control = ergmm.control(burnin = 50000, interval = 50,
                                                      sample.size = 1000,
                                                      mle.maxit = 100),
                              user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                sender.var = var(a),
                                                receiver.var = var(b),
                                                Z.var = var(as.vector(Z))))
@

<< latent_watch_qn, eval = F, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=

W = as.sociomatrix(movie_net_watch_full, ignore.eval = F, attrname = "ratings_diff")
N = nrow(W)

#default hyperpriors ####
set.seed(1)
t1.watch.qn = Sys.time()
latent_watch = lsqn(W, runs = 100, tol = .01, Z.init = "MDS") #MDS may be slightly better than rnorm in this example, though difference could just be due to randomness/seed. more runs can add a tiny improvement.
t2.watch.qn = Sys.time()
range(latent_watch$map$diff_Z); range(latent_watch$map$diff_Z2) #only one non~0 diff_Z value, but it has no out-edges and only three in-edges with values 1,1,2 
t2.watch.qn - t1.watch.qn  #22 min (19 with another trial, Z.init = "rnorm") #may get slighlty better with more runs. 
llik(latent_watch$map, Y = W, est = "Y") #10480510

#adjusted hyperpriors ####
set.seed(1)
t1.watch.qn.h = Sys.time()
latent_watch.h = lsqn(W, runs = 150, tol = .01, Z.init = "MDS", #may get a tiny bit better with more runs. 
              v_a = 3, v_b  = 3, #default 3 
              v_z = sqrt(N)/8,  #default sqrt(N)
            s2_a = 1, s2_b = 1, #default 1
              s2_z = 1, #default N/8 ** shrink sigma_z range
              sigma2_B = 3) #default 9)
t2.watch.qn.h = Sys.time() #~22 mins with 100 runs
range(latent_watch.h$map$diff_Z); range(latent_watch.h$map$diff_Z2)
llik(latent_watch.h$map, Y = W, est = "Y") #10480611
#quantile(rinvchisq(100000, sqrt(N)/8, 1), probs = c(0.05, .5, .95))
#quantile(rinvchisq(100000, 3, 1), probs = c(0.05, .5, .95))

@

<< latent_watch_mle, eval = F, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=

W = as.sociomatrix(movie_net_watch_full, ignore.eval = F, attrname = "ratings_diff")
#random initation of sender and receiver, and scaling Z helps the model fit
D = 2; N = nrow(W)
Z_dist = dist2(t(W))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

#default hyperprior, r.init ####
t1.latent.watch.mle = Sys.time() #about 75 minutes for optim to find mpe - could lower mle.maxit
latent.watch.mle = ergmm(movie_net_watch_full ~ euclidean(d = 2) + rsender + rreceiver,
                              response = "ratings_diff", family = "Poisson.log", seed = 30,
                              tofit = c("mle"),
                              control = ergmm.control(mle.maxit = 200),
                              user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                sender.var = var(a),
                                                receiver.var = var(b),
                                                Z.var = var(as.vector(Z))), verbose = 3)
t2.latent.watch.mle = Sys.time()
llik(latent.watch.mle$start, Y = W, est = "Y")

#hyperprior adjustment, r.init ####
t1.latent.watch.mle.h = Sys.time() #about 45 minutes with 100 maxit
latent.watch.mle.h = ergmm(movie_net_watch_full ~ euclidean(d = 2) + rsender + rreceiver,
                              response = "ratings_diff", family = "Poisson.log", seed = 30,
                              tofit = c("mle"),
                              control = ergmm.control(mle.maxit = 100),
                              user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                sender.var = var(a),
                                                receiver.var = var(b),
                                                Z.var = var(as.vector(Z))),
                                 prior = ergmm.prior(sender.var.df = 3,
                                        receiver.var.df = 3,
                                        Z.var.df = sqrt(N/8),
                                        sender.var = 1,
                                        receiver.var = 1, 
                                        Z.var = 1,
                                        beta.var = 3),verbose = 3)
t2.latent.watch.mle.h = Sys.time()
llik(latent.watch.mle.h$start, Y = W, est = "Y")
@

There is a strong correlation of $0.95$ (or $0.976$ when weighted by the log of co-review counts) between a film's average rating and its ratings from the latent space model (receiver minus sender coefficient).
%?($x$ unweighted, $x$ when weighted by the logged counts of ratings).
Figure \ref{fig:r movie_compare} shows this correlation, plotting the latent space scores against the average ratings and labeling points by review counts for each film. Although not pictured, the standard deviations in the ratings are fairly consistent. They range from 0.13 to 0.26 (95th percentile) with a median of 0.14. The high-end outliers have very few co-review counts, and only points with few co-reviews deviate strongly from the overall linear trend. We take a closer look at a couple of the films that deviate from the trend but have more than $20$ reviews, highlighted in green. The corresponding films are, from left to right, \textit{Shaft in Africa (1973)} and \textit{Assassination (1987)}. These films have a potential ``cult'' following based on their lead actors, Richard Roundtree (as Shaft) and Charles Bronson respectively. That may boost some of the user reviews, but when comparative reviews are considered in the latent space model the derived rating is lower.
%also #79 with 32 reviews, Wanted: Dead or Alice (1987). Based on a three-season television show that launched steve mcqueen's career. 

<<r movie_compare, eval = T, echo = F, dependson = c("movie_data", "setup"), cache = T, message = FALSE, warning = FALSE, fig.height = 6, results = "hide", fig.cap= "Latent space model scores vs. average ratings. The plotting characters are the review counts for each film. The red points are strong outliers to the linear trend, have very few reviews, and are excluded in calculating the best fit line. The green points are discussed in the paper." >>=

library(ggplot2)

par(mai = c(.8,.8,.5,.8))
stars1 = movie_net_acw_full%v%"stars"
vc.acw = latent_acw2.h$mkl$receiver - latent_acw2.h$mkl$sender
# highlight deviant - include 79 too? Wanted dead or alive is the other 32 count
col1 = rep("black", length(stars1)); col1[c(78, 122)] = "green"; col1[ (movie_net_acw_full%v%"counts") < 5] = "red"
data1 = data.frame(stars = stars1, rating = vc.acw,
                   counts = movie_net_acw_full%v%"counts",
                   color = col1)
# to add trend line without outliers
x <- which(movie_net_acw_full%v%"counts"<5)
lm1 = lm(vc.acw[-x] ~ stars1[-x])

ggplot(data1, aes(x = stars, y = rating, color = color)) + 
  geom_abline(slope = lm1$coefficients[2], intercept = lm1$coefficients[1], col = "gray") +
  geom_text(aes(label=counts), size = 3) +
  theme_bw() +
  ggtitle("Ratings Compared") +
  xlab("Average Star Rating") + 
  ylab("2-D Latent Space Rating") +
  theme(legend.position = "none") +
  scale_colour_manual(values = c("black", "green", "red"))

#check unweighted and weighted correlations
cor(stars1 , vc.acw) #.942
#weighted correlation even higher
boot:::corr(cbind(stars1, vc.acw),
     w = log((movie_net_acw_full%v%"counts"))) #.975

@

<<r movie_uncertainty, eval = T, cache = T, echo = F, results = "hide">>=
movie_sd = apply(latent_acw2.h$sample$receiver - latent_acw2.h$sample$sender, 2, sd)
quantile(movie_sd, probs = c(0,.25,.5,.75,.90, .95, .97, 1))
(movie_net_acw_full%v%"counts")[which(movie_sd > .28)]
sd2[40]
@

It is somewhat surprising to find such a strong correlation between the two ratings methods. This may be due in part to homogeneity in the reviewer pool, which reduces the sources of bias discussed above. MovieLens was organized by a university and all reviewers in our data joined in 2000. The top four occupations of the reviewers are (in decreasing order): college/grad student, other/not specified, executive/managerial and academic/educator. This is certainly not representative of the population at large. The impact of our difference-based network model over average ratings may be more evident when reviewers are more heterogeneous. 

%Because the reviews are strongly correlated - almost all within a third of a point so overall impression the same - and we have no ground truth to compare to
We next consider the additional insight gained through latent space model positions and visualization. An interactive plot of the model output is available on the author's website (\url{http://www.stat.ucla.edu/~jane.carlen/pages/movie_net.html}). Each film is displayed as a node whose position is estimated by the latent space model. The nodes are colored by genre with size scaled to latent space model rating. For comparison, the average star ratings are listed next to the movie titles in the drop-down menu and when hovering over a node. For clear visualization, the nodes are limited to displaying their (at most) seven strongest out-edges. In-degree is not limited in the plot. Uncertainty in positions estimates is displayed in Figure \ref{fig:r movie_cloud}.
%An edge's arrow indicates the direction from lower to higher-rated film.

<<r movie_plot_setup, eval = T, echo = F, results = "hide", dependson = c("movie_model_r", "movie_model_h")>>=
write.csv(latent_acw2.h$mkl$Z, "~/Documents/citation/latent_ranking/movie_net_acwh_Z.csv")
write.csv(latent_acw2.r$mkl$Z, "~/Documents/citation/latent_ranking/movie_net_acwr_Z.csv")
@

<<r movie_plot, eval = F, echo = F, fig.keep = "none", dependson = c("movie_model_r", "movie_model_h")>>=

#not evaluated here. stored as movie_neth.html
Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwh_Z.csv")[,2:3])

#to align with other picture
Z = -Z[,2:1]
movie_net = movie_net_acw #to see all edges use movie_net_acw_full
  
nodes <- data.frame(id = 1:length(movie_net%v%"titles"), 
                    label = "",
                    #label = movie_net_acw%v%"vertex.names",
                    title = paste(movie_net%v%"titles", round(movie_net%v%"stars",2)), #for selection dropdown
                    value = vc.acw, 
                    group = movie_net%v%"genres") #conveys node size, on a scale from 0-1

edges <- data.frame(from=data.frame(as.edgelist(movie_net))$X1, 
                    to=data.frame(as.edgelist(movie_net))$X2)
                    #value = movie_net_acw%e%"avg_diff")

acw_igraph <- igraph::graph.data.frame(edges, directed=TRUE, vertices=nodes)
  
visNetwork(nodes, edges, main = "Movie Network", submain = "latent space positions") %>%
  visIgraphLayout(acw_igraph, layout = "layout.norm",
                  layoutMatrix = Z, type = "full") %>%
  #visOptions(highlightNearest = list(enabled =TRUE, degree = 1)) %>%
  visNodes(#color = list(highlight = list(background = "black")),
           shape = "dot", #shape = "text"
           scaling = list(min = 5, max = 30, label = list(max=20)),
           labelHighlightBold = TRUE,
           borderWidth = .5,
           borderWidthSelected = 2,
           font= list(size = 20, color = "black", strokeWidth = 1)) %>%
  visEdges(shadow = FALSE,
           scaling = list(min = 1, max = 10),
           #selectionWidth = "function(x) {acw_avg_diff}",
           hidden = FALSE,
           #scaling = list(min = 1, max = 10),
           arrows = list(to = list(enabled = FALSE, scaleFactor = 5),
                         middle = list(enabled = TRUE, scaleFactor = 1)),
           color = list(inherit = TRUE, highlight = "red", opacity = 0.04), #, color = "white"
           smooth = list(type = "curvedCW")) %>%
  visOptions(highlightNearest = list(enabled = T, degree = 0.5),
             nodesIdSelection = FALSE, 
             selectedBy = "title") %>%
  visLegend(enabled = TRUE, useGroups=TRUE) %>%
  visInteraction(selectConnectedEdges = TRUE)

@

The movies clearly cluster by genre even though genre was not a term in the model. Even those movies with hybrid genres are placed roughly between their two component genres. However, there are a few films that reside outside of their genre cluster. In those cases the plot can highlight incomplete or incorrect classification. For example, the film \textit{Coogan's Bluff (1968)} is categorized as a crime film by MovieLens, but its latent position is among actions and westerns. The Internet Movie Database (\textit{IMDb}) entry for this film describes it as ``An Arizona deputy goes to New York City to escort a fugitive back into custody,'' and the lead role is played by action/crime/western star Clint Eastwood \citep{coogan}. This film has heavy action and western influences which its latent position reveals. Another example of misclassification is the two ``action'' films \textit{The Kid (1921)} and \textit{Minnie and Moskowitz (1971)}, positioned between crimes and westerns. Their genres listed on \citet{imdb} are comedy/drama/family, and comedy/drama/romance, respectively. Neither is well-classified as an action film, and their positions reflect this, though without comic films in the data the correct positions are lost.

The continuous positions from the latent space model are more precise identifiers of movie type than discrete cluster labels. Like the journal citation network visualized in Section \ref{Example1}, the clusters are discernible but irregularly shaped and blend into each other. The positions aid in identifying sub-genres, which may be valuable for recommendation systems. For example, the westerns that tail into the crime cluster, \textit{Unforgiven (1992)}, \textit{Tombstone(1993)}, and \textit{Dead Man (1995)} reveal a "modern western" subgenre. These films are much newer than the median year of westerns in the data (1968).

If no genre labels are given the fit positions provide rich input for a clustering algorithm. To illustrate, we apply k-means clustering with three clusters which we expect to correspond roughly to the action, crime and western films (\citealt{kmeans}, \citealt{r}). We use pair-counting to measure accuracy, with the caveat that not all pre-assigned genres are correct, as discussed above. Films originally labeled with two genres are considered a match if assigned to either of those genres. Table \ref{class.table} compares the k-means output to pre-assigned genre labels. Of the 128 films in the network, 105 are assigned to matching labels. The most common change in classification is from action to crime. The comparison is visualized in Figure \ref{fig:movie_kmeans}. Most films that switch labels are positioned near the boundary of the three classes, indicating that a single label is probably insufficient.

<<movie_kmeans, cache = T, echo = F, fig.height = 3, fig.width = 7, results = "asis", fig.cap = "Film network colored by k-means class with shape determined by pre-assigned genre. Centers of the k-means classes are labeled 1-3.", dependson = c("movie_model_r"), message = F>>=

library(stringr)

#as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwr_Z.csv")[,2:3])
fit.Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwr_Z.csv")[,2:3])
G = 3
fit.Kr = kmeans(fit.Z, centers =  G) #no variation in fit among the algorithms
data1 = data.frame(x = -fit.Z[,1], y = fit.Z[,2], kmeans_class = as.factor(fit.Kr$cluster), assigned_genre = as.factor(movie_net_acw_full%v%"genres"))
data2 = data.frame(x = fit.Kr$centers[,1], y = fit.Kr$centers[,2])
kmeans_r = ggplot(data1, aes(x = x, y = y, color = kmeans_class, shape = assigned_genre)) +
  geom_point(size= 3) +
    geom_point(data = data2, aes(x, y), inherit.aes = F, shape = c("1","2","3"), size = 3) + 
  scale_color_manual(values = rainbow(G, alpha = .7)) +
  theme_void()# + xlim(-3,3) + ylim(-2,2)

fit.Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwh_Z.csv")[,2:3])
G = 3
fit.Kh = kmeans(fit.Z, centers =  G) #no variation in fit among the algorithms
data1 = data.frame(x = fit.Z[,1], y = fit.Z[,2], kmeans_class = as.factor(fit.Kh$cluster), assigned_genre = as.factor(movie_net_acw_full%v%"genres"))
names(data1)[3:4] = c("k-means class", "assigned genre")
data2 = data.frame(x = fit.Kh$centers[,1], y = fit.Kh$centers[,2])
kmeans_h = ggplot(data1, aes(x = x, y = y, color = `k-means class`, shape = `assigned genre`)) +
  geom_point(size= 3) +
    geom_point(data = data2, aes(x, y), inherit.aes = F, shape = c("1","2","3"), size = 5) + 
  scale_color_manual(values = rainbow(G, alpha = .7)[c(1,2,3)]) +
  theme_void() + 
  theme(legend.text = element_text(size = 10), legend.title = element_text(size = 10)) +
  #ggtitle("Comparison of k-means clusters to pre-assigned genres")

kmeans_h

# pair counting
# let Action|Crime match action or crime. let Action|Western match action or wester
# see which matches which
kmeans.labels = as.factor(fit.Kh$cluster)
assigned.group  = aggregate(fit.Z, by = list(movie_net_acw_full%v%"genres"), mean)[c(1,4,5),]
kmeans.group = aggregate(fit.Z, by = list(kmeans.labels), mean)
assigned.group = assigned.group[order(assigned.group[,1])] #inspect this visually
levels(kmeans.labels) = assigned.group$Group.1[rank(-kmeans.group$V1)]
#sum(is.na(str_match(kmeans.labels, movie_net_acw_full%v%"genres")))
#table((movie_net_acw_full%v%"genres")[is.na(str_match(kmeans.labels, movie_net_acw_full%v%"genres"))])
table1 = t(table(kmeans.labels, movie_net_acw_full%v%"genres"))
print(xtable(table1[,c(2,1,3)], caption = "Classification of films by pre-assigned genres (row) vs. k-means cluster (column)", digits = 0, label = "class.table"))

# westerns by year
west_years = sort(sapply((movie_net_acw_full%v%"titles")[movie_net_acw_full%v%"genres"=="Western"], str_extract, "[0-9]{4}"))
crime_years = sort(sapply((movie_net_acw_full%v%"titles")[movie_net_acw_full%v%"genres"=="Crime"], str_extract, "[0-9]{4}"))
action_years = sort(sapply((movie_net_acw_full%v%"titles")[movie_net_acw_full%v%"genres"=="Action"], str_extract, "[0-9]{4}+"))
#median(as.numeric(west_years)); median(as.numeric(crime_years)); median(as.numeric(action_years)); 

@

%I tried  applying two-group and three group cluster models in latentnet. 2-group is not good (two central clusters, one with larger variance). 3-group hasn't run successfully yet on the full network and the results on the thinned network weren't very helpful. I may need to use the genres as inital labels, but would lose the "initial label" benefit. 
<<group_models, eval = F, echo = F>>=
#Even with initial postions the cluster model returns error.
latent_acw3 = ergmm(movie_net_acw_full ~ euclidean(d=2, G = 3) + rreceiver + rsender, 
             response = "ratings_diff", family = "Poisson.log", tofit = "mle",
             prior = ergmm.prior(Z.k = fit.K$cluster, Z.mean = fit.K$centers,
                                 Z.var = rep(.5,3)),
             control = control.ergmm(pilot.runs = 4,  burnin = 500000, 
             interval = 500, sample.size = 5000), seed = 123, verbose = 3)
=plot(latent_acw3$burnin.start$mkl$Z)
@

<< pagerank_movie, eval = F, cache = T, echo = F, results = 'hide', dependson = c("setup", "setup_jrss", "pagerank", "movie_data"), fig.keep = 'none', message = F, warning = F>>=

# apply page rank ####
acw_igraph = igraph::graph.adjacency(acw_mat, weighted=T)
#acw_igraph = igraph::graph.adjacency(log(acw_mat+1), weighted=T) <- stronger correlation with starts if take log first
movie.page.rank = igraph::page.rank(acw_igraph, damping = .95)$vector

# compare results to other ranks ####

## points in
cor(movie.page.rank, colSums(movie_rating_acw)) #.845

## to cited/citing ratio (cited is points in)
cor(movie.page.rank, colSums(movie_rating_acw)/rowSums(movie_rating_acw), use = "complete.obs") #.871 - better when normalized

## number of ratings
cor(movie.page.rank, movie_net_acw%v%"counts") #.64
plot(movie.page.rank, movie_net_acw%v%"counts", pch = "x")
cor(movie.page.rank, movie_net_acw%v%"stars") #.64
plot(movie.page.rank, movie_net_acw%v%"stars", pch = "x")
#text(movie.page.rank, movie_net_acw%v%"stars", labels = movie_net_acw%v%"titles", cex = .5, srt = 90)

### latent ranking scores
cor(movie.page.rank, vc.acw) #.66
plot(movie.page.rank, vc.acw, type = "n")
text(labels = movie_net_acw%v%"titles", movie.page.rank, vc.acw, cex = .5, srt = 90)

# takeaway: most page ranks extremely small (this is probably desiriable in web search results, but not here)
@

% collaborative filtering to generate individualized network plots?

\section{Discussion}\label{Discussion}

In this paper we have introduced a method for rating objects based on relational data using a latent space network model. Unlike standard network-based ranking methods such as PageRank, it is not a variant on a centrality measure. Instead, it weighs the relative in- and out-flow of an object while controlling for its position in space -- a position that is determined by its network ties. It is thereby an appropriate measure of an object's prestige within the network. In addition, the model provides estimates of uncertainty in the rankings that allow us to determine the significance of ranking differences. The quasi-Stigler model returns similar estimates of uncertainty in ratings, but as a network-generating model it underestimates uncertainty because it is conditioned on dyad totals.

The latent space ranking method is especially advantageous when elements to be rated have some underlying, but difficult to measure, similarities. In our applications these were the statistics journals with one or more type (theoretical, computational, etc.) and films of one or more genre. If additionally the sub-groups have characteristically different levels of activity we may be able to account for that using indicator covariates. Unfortunately, we are not able to obtain and present citation data for related fields of journals (e.g. economics, applied mathematics) to examine this further in the context of academic journals.

% \section{Data Expansion} - ON HOLD, this would take a lot of data scraping work and/or consent/release from JCR. Varin et al don't have it, said the data they did share was hard to negotiate.
% - \citet{varinetal} Data comes from 2010 Web of Science published by Thomson Reuters. Want to expand to include all of Statistics and Probability (?) and Mathematics, Applied ( 253 journals) and Agricultural, Economics and Policy (? 17 journals)
% Note JCR can give related journals, e.g.  http://admin-apps.webofknowledge.com/JCR/JCR?RQ=RELATED_JOURNALS&rank=6&journal=CAN+J+AGR+ECON&query_new=true, and this could be another use of the network model

Another type of heterogeneity that may bias our results is when some nodes in the network are very poorly connected or even isolated. We saw in the MovieLens example that films with very few reviews are most likely to deviate from an overall ratings correlation. One way to combat this is to make prior variance distributions less diffuse, which stabilizes the position estimates for low-connectivity without sacrificing model fit, in our experience. To further combat this we could incorporate a film's average star rating as a prior expectation for its receiver coefficient. % [implementable in ergmm or only prior mean 0?] , while keeping the sender coefficients at prior mean zero
Although this is not currently implemented in \pkg{latentnet} it could be incorporated in the future.

The latent space model is also valuable for its meaningful visualization of network data, which facilitates user exploration of the network itself. One could use the dynamic plots we present to find journals to submit articles to or films to watch that are similar to a given one.
%i checked a standard plot and it's OK but not nearly as good:
%plot(movie_net_acw, vertex.col = as.numeric(as.factor(movie_net_acw%v%"genres")), cex = 1.3, edge.col ="grey")
While the latent space rankings are very similar to those of the quasi-Stigler model, the visualization helps us better understand what is driving each journal's rating. 

Furthermore, the visualizations allow for nuanced genre detection. Though our models do not include a term to capture genre, the latent space positions have proven to cluster in agreement with pre-assigned genres. In the case where labels are not provided, we showed that k-means clustering on the estimated positions can produce good results. In preliminary comparisons this technique has performed better than clustering algorithms on the raw data, but further work is needed to establish these results.

% vs. an out-of-the-box clustering method on the (directed) graph, which does worse:
<<movie_louvain, eval = F, echo = F, results = "hide">>=

# vs. an out-of-the-box clustering method on the (directed) graph
tmp = igraph::cluster_louvain(igraph::graph.adjacency(acw_mat+t(acw_mat), weighted=T, mode = "undirected"), weights = NULL) #undirected only
plot(latent_acw2.h$mkl$Z, col = tmp$membership, pch = as.numeric(as.factor(movie_net_acw_full%v%"genres")))
legend("topright", cex = .7, pch = 1:5, legend = levels(as.factor(movie_net_acw_full%v%"genres")))
# this method groups most action and crime films as one group, recognizes the westerns, and has a third group of mostly action and action|crime that's between those two.

@

The speed of the quasi-Newton algorithm enables us to fit larger networks than were practical with MCMC estimation. We were able to fit an expanded film network of 510 nodes in roughly 20 minutes, whereas it would take over 24 hours with MCMC estimation. (the quasi-Newton optimization implemented by \pkg{latentnet} takes roughly double as long as our quasi-Newton method in this case.)
Porting the body of our code to \proglang{C++} using \pkg{Rcpp} would further increase the speed. However, we still face the scaling problem of the number of edges growing $O(n^2)$. One way to address this is to parallelize the updates to positions, which is straightforward since the algorithm updates positions synchronously. An area of future work is to develop a model with a local dependence structure. For many large networks we expect that the positions and random effects for each node depend primarily on a neighborhood around the node and a few high-activity nodes with wide reach. This would reduce the storage space and number number of calculations needed for each update step, assuming we can develop a simple method for identifying the nodal neighborhoods. The number of edges to consider would grow only $O(n)$ in the case that mean degree were not an increasing function of graph size, which is a realistic assumption of many social networks.

Recently, variational Bayesian inference methods have been developed for latent space network models and implemented in the \pkg{R} package \pkg{VBLPCM}. Variational Bayesian inference is another strategy to greatly reduce computation time for the model. However, the algorithms developed currently only apply to binary networks, not valued networks of the type discussed here. Given the speed of quasi-Newton estimation and uncertainty in the level of bias introduced by variational techniques we did not undertake to expand the variational methods to valued networks. Another line of work would be to implement that extension and compare the speed and results to the optimization methods described here.

\break

<<variational_bayes, echo = F, eval = F, results = 'hide'>>=

library(VBLPCM)
v.start<-vblpcmstart(movie_net_acw_full,G=3,model="rsocial",LSTEPS=1e4)
v.start<-vblpcmstart(movie_net_acw_full,model="rsocial",LSTEPS=1e4)
v.fit<-vblpcmfit(v.start,STEPS=50)
plot(v.fit)
plot(gof(v.fit,GOF=~degree))
Z = v.fit$V_z[!isolates_acw_thin,]

@

\begin{appendix}

\section{Appendix: Additional Plots}\label{Appendix1}

<<r latent2_centipede, eval = T, echo = F, cache = T, warning = F, message = F, results = 'hide', dependson= c("setup", "latent_sr2", "setup_jrss", "latent2_stigler"), fig.height = 6, fig.pos = "H", fig.cap = "Visualizing uncertainty in latent space scores (left) and quasi-Stigler scores (right). The error bars are +/- 1.96 estimates of standard deviation (left) and quasi-standard error (right). Comparison of ``comparison intervals'' around estimated values, which are equal to 1.96$*$QSE in each direction, and 1.96$*$SE in each direction. Note that due to fit constraints, AMS only has an estimated QSE.">>=

# from plotrix package, but made some changes ####
# added cex arg and commented out something with segs[4, and x axis label

centipede.plot <- function (segs, mct = "mean", lower.limit = "std.error", upper.limit = lower.limit, 
    left.labels = NULL, right.labels = NULL, sort.segs = TRUE, 
    main = "", xlab = NA, pch = 21, vgrid = NA, hgrid = NA, gridcol = "lightgray", 
    mar = NA, col = par("fg"), bg = "green", cex = NULL, ...) 
{
    if (missing(segs)) {
        cat("Usage: centipede.plot(segs,...)\n\twhere segs is a dstat object")
        stop("or a matrix of midpoints and limits")
    }
    if (is.list(segs)) {
        if (all(lapply(segs, is.numeric))) 
            segs <- get.segs(segs, mct = mct, lower.limit = lower.limit, 
                upper.limit = upper.limit)
        else stop("If segs is a list, all the components must be numeric")
    }
    if (class(segs) == "dstat") {
        midpoint <- "mean"
        if (lower.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ] * segs[2, ]
        }
        if (upper.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ] * segs[2, ]
        }
        if (lower.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]
        }
        if (upper.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]
        }
        if (lower.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]/sqrt(segs[3, ])
        }
        if (upper.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]/sqrt(segs[3, ])
        }
        segs <- rbind(segs[1, ], ll, ul, segs[3, ])
    }
    segdim <- dim(segs)
    if (sort.segs) {
        seg.order <- order(segs[1, ])
        segs <- segs[, seg.order]
    }
    else seg.order <- 1:segdim[2]
    oldpar <- par("mar")
    #if (is.na(mar[1])) 
    #    mar <- c(4, 6, 1 + 2 * (nchar(main) > 0), 5)
    #par(mar = mar)
    plot(x = c(min(segs[2, ]), max(segs[3, ])), y = c(1, segdim[2]), 
        main = main, xlab = "", ylab = "", type = "n", axes = FALSE, 
        ...)
    box()
    if (!is.na(vgrid[1])) 
        abline(v = vgrid, lty = 1, col = gridcol)
    if (is.null(hgrid)) 
        abline(h = 1:segdim[2], lty = 2, col = gridcol)
    else if (!is.na(hgrid[1])) 
        abline(h = hgrid, lty = 2, col = gridcol)
    axis(1, cex.axis = cex)
    arrows(segs[2, ], 1:segdim[2], segs[3, ], 1:segdim[2], length = 0.05, 
        angle = 90, code = 3, col = col)
    points(segs[1, ], 1:segdim[2], pch = pch, col = col, bg = bg)
    if (is.null(left.labels)) {
        left.labels <- colnames(segs)
        if (is.null(left.labels)) 
            left.labels <- paste("V", seg.order, sep = "")
    }
    else left.labels <- left.labels[seg.order]
    plot.limits <- par("usr")
    mtext(left.labels, 2, line = 0.2, at = 1:segdim[2], adj = 1, 
        las = 1, cex = cex)
    #if (is.null(right.labels)) 
    #    right.labels <- paste(round(segs[1, ], 2), "(", segs[4, 
    #        ], ")", sep = "")
    #else 
    right.labels <- right.labels[seg.order]
    mtext(right.labels, 4, line = 0.2, at = 1:segdim[2], adj = 0, 
        las = 1, cex = cex)
    #if (is.na(xlab)) 
    #    xlab <- paste("| -", rownames(segs)[2], "-", rownames(segs)[1], 
    #        "-", rownames(segs)[3], "- |")
    if (!is.na(xlab)) 
        mtext(xlab, 1, line = 2)
    par(oldpar)
    invisible(segs)
}

# 2-D latent space variance by sample ####
model = latent.srp2.init.r
sd2 = apply(model$sample$receiver - model$sample$sender, 2, sd)
latent2 = model$mkl$receiver - model$mkl$sender
centipede1 = t(cbind(latent2, latent2 - 1.96*sd2, latent2 + 1.96*sd2))
colnames(centipede1) = Cnet%v%"vertex.names"

# QSE using glm output (account for overdispersion) ####
shift = mean(g1$coefficients)
v = c(0, diag(vcov(g1))) #vcov(g1) = summary(g1)$cov.scaled = summary(g1)$cov.unscaled*phi 
centipede.qse = (cbind(fit.table2$quasi,
                        fit.table2$quasi - 1.96*fit.table2$qse,
                        fit.table2$quasi + 1.96*fit.table2$qse)) 

centipede.se = (cbind(g1$coefficients - shift,
                       g1$coefficients - shift - 1.96*sqrt(v), 
                       g1$coefficients - shift + 1.96*sqrt(v)))
rownames(centipede.se) = rownames(fit.table2)

#plot ####
par(mfrow = c(1,2))
centipede.plot(centipede1, vgrid = c(-1,0,1,2), cex = .7, right.labels = round(latent2, 2),
               bg = "red", main = "2-D Latent Space")

centipede.plot(t(centipede.se), vgrid = c(-1,0,1,2), cex = .7, 
               right.labels = round(fit.table2$quasi, 2), 
               left.labels = rownames(fit.table2), 
               bg = c("blue"), col = c("blue"),
               main = "Quasi-Stigler SE and QSE Intervals",
               xlim = c(-2, 2.5))
par(new=TRUE)
centipede.plot(t(centipede.qse), cex = .7,
               bg = c("blue"), col = c("cyan"),
               left.labels = " ",
               right.labels = " ",
               xlim = c(-2, 2.5))
legend("bottomright", legend = c("SE", "QSE"), col = c("blue", "cyan"), pch = 15)

# summary of ses
summary(sqrt(v)[v>0])
summary(sd2)
@

<<r movie_cloud, eval = T, echo = F, fig.height = 6, cache = T, fig.cap = "Uncertainty in estimated film positions illustrated by a sample of positions from the model posterior distribution. Colouring is due to pre-assigned genre labels.">>=
####  cloud/uncertainty plot  ####
n = nrow(acw_mat)
N = 1000
p = latent_acw2.h[["sample"]][["Z"]]
s = sample(5000, N)

#null plot:
plot(latent_acw2.h$mkl$Z, xlab = NA, ylab = NA, type ="n",
           bty="n", yaxt="n", xaxt="n",  main = NA,
           xlim = c(-4,5), ylim = c(-4,4))

legend("topright", pch = 16, col = rainbow(5, alpha = 1)[c(4,3,5,2,1)], legend = levels(as.factor(movie_net_acw_full%v%"genres")), bty = "n", legend.ce)

#add points
for (i in 1:N) {
  points(p[s[i],,], col = (rainbow(5, alpha = 1)[c(4,3,5,2,1)])[as.numeric(as.factor(movie_net_acw_full%v%"genres"))], pch = 15, cex = .2)
}
@

\noindent \section{Appendix: Derivatives for Quasi-Newton Algorithm}\label{Appendix2}


\textbf{1. The posterior log likelihood function of interest, up to a constant (denoted $\mathbbm{llik}$):}
\begin{multline*}
$$ \displaystyle log(P(\theta|Y)) = \sum_{i}\sum_{j \neq i} y_{ij}(\beta + a_i + b_j - \lVert z_i - z_j \rVert) + exp(\beta + a_i + b_j - \lVert z_i - z_j \rVert) - log(y_ij!)~+ \\
  \sum_i log(\frac{1}{\sqrt{2\pi\sigma^2_a}}exp(\frac{-a^2_i}{2\sigma^2_a})) ~+~
  \sum_i log(\frac{1}{\sqrt{2\pi\sigma^2_b}}exp(\frac{-b^2_i}{2\sigma^2_b})) ~+~
  \sum_{i,d} log(\frac{1}{\sqrt{2\pi\sigma^2_z}}exp(\frac{-z^2_{i,d}}{2\sigma^2_z})) ~+~ 
  log(\frac{1}{\sqrt{2\pi\sigma^2_\beta}}exp(\frac{-\beta^2}{2\sigma^2_\beta}))~ + ~\\
  log(\frac{s^2_a\frac{v_a}{2}}{\Gamma(\frac{v_a}{2})})
    ~-~  \frac{v_as^2_a}{2\sigma^2_a} ~-~ log(\sigma^2_a)(1+\frac{v_a}{2}) ~+~
  log(\frac{s^2_b\frac{v_b}{2}}{\Gamma(\frac{v_b}{2})}) 
    ~-~  \frac{v_bs^2_b}{2\sigma^2_b} ~-~ log(\sigma^2_b)(1+\frac{v_b}{2}) ~+~
  log(\frac{s^2_z\frac{v_z}{2}}{\Gamma(\frac{v_z}{2})}) 
    ~-~  \frac{v_zs^2_z}{2\sigma^2_z} ~-~ log(\sigma^2_z)(1+\frac{v_z}{2}) \\
  $$
\end{multline*}

\noindent \textbf{2. First-order Partial Derivatives:} \newline

\noindent Calculations assume no self-edges, $y_{ii} = 0, \forall~1 \leq i \leq n$.

$$\displaystyle  \frac{\partial \mathbbm{llik}}{\partial a_i} = - \frac{a_i}{\sigma^2_a} + \sum_{j \neq i} y_{ij} - exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert)$$

$$\displaystyle  \frac{\partial \mathbbm{llik}}{\partial b_i} = - \frac{b_i}{\sigma^2_b} + \sum_{j \neq i} y_{ji} - exp(\beta +a_j + b_i - \lVert z_i - z_j \rVert)$$

$$\displaystyle \frac{\partial \mathbbm{llik}}{\partial \beta} = - \frac{\beta}{\sigma^2_\beta} + \sum_i \sum_{j \neq i} y_{ij} - exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert))$$

\begin{align*}
\displaystyle \frac{\partial \mathbbm{llik}}{\partial z_{i,d}} &= - \frac{z_{i,d}}{\sigma^2_z} + \sum_{j \neq i} \frac{\partial \mathbbm{llik}}{\partial z_{i,d}}(\lVert z_i - z_j \rVert) \Big[ -y_{ij} - y_{ji} + exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert) + exp(\beta +a_j + b_i - \lVert z_i - z_j \rVert) \Big] \\
  &= - \frac{z_{i,d}}{\sigma^2_z} +
  \sum_{j \neq i} \frac{1}{\lVert z_i - z_j \rVert}(z_{i,d}-z_{j,d}) \Big[ -y_{ij} - y_{ji} + exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert) + exp(\beta +a_j + b_i - \lVert z_i - z_j \rVert) \Big]
\end{align*}

\noindent For variance parameters we can solve the first order partial for 0. We outline the calculation for $\sigma^2_a$. 

$$\displaystyle \frac{\partial \mathbbm{llik}}{
\partial \sigma^2_a} = n(2\pi\sigma^2_a)^{-1}\pi + \sum_i\frac{a_i^2}{2}(\sigma^2_a)^{-2} + \frac{v_as_a^2(\sigma^2_a)^{-2}}{2} - (1+\frac{v_a}{2})(\sigma^2_a)^{-1} = 0$$

$$\displaystyle (\sigma^2_a)^{-1}(\sum_i\frac{a_i^2}{2} + \frac{v_as_a^2}{2}) = \frac{n}{2} + (1+\frac{v_a}{2})$$


$$\displaystyle \sigma^2_a = \frac{v_a s^2_a + \sum_i a_i^2}{n + 2 + v_a}$$
$$\displaystyle \sigma^2_b = \frac{v_b s^2_b + \sum_i b_i^2}{n + 2 + v_b}$$
$$\displaystyle \sigma^2_z = \frac{v_z s^2_z + \sum_{i,d} z_{i,d}^2}{n*D + 2 + v_z}$$

\break

\noindent \textbf{ 3. Second-order Partial Derivatives: } \newline

$$\displaystyle \frac{\partial^2 \mathbbm{llik}}{\partial a^2_i} = \sum_{j \neq i} - exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert) - \frac{1}{\sigma^2_a} < 0$$

$$\displaystyle \frac{\partial^2 \mathbbm{llik}}{\partial b^2_i} = \sum_{j \neq i} - exp(\beta +a_j + b_i - \lVert z_i - z_j \rVert) - \frac{1}{\sigma^2_b} < 0$$

$$\displaystyle \frac{\partial^2 \mathbbm{llik}}{\partial \beta^2} = - \frac{1}{\sigma^2_\beta} + \sum_i \sum_{j \neq i} - exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert)) < 0$$

$$\displaystyle \frac{\partial \mathbbm{llik}}{
\partial^2 \sigma^2_a} = -(\sigma^2_a)^{-2}(\sum_i\frac{a_i^2}{2} + \frac{v_as_a^2}{2}) < 0$$ \\

\noindent let $exp(IJ) = exp(\beta +a_i + b_j - \lVert z_i - z_j \rVert)$ \\
let $exp(JI) = exp(\beta +a_j + b_i - \lVert z_i - z_j \rVert)$

\begin{align*}
\displaystyle \frac{\partial^2 \mathbbm{llik}}{\partial z^2_{i,d}} & = - \frac{1}{\sigma^2_z} + \sum_{j \neq i} \Big[-\frac{1}{\lVert z_i - z_j \rVert^3}(z_{i,d}-z_{j,d})^2 + \frac{1}{\lVert z_i - z_j \rVert} \Big] \Big[ -y_{ij} - y_{ji} + exp(IJ) + exp(JI) \Big] - \\
& \indent \sum_{j \neq i} \Big[\frac{1}{\lVert z_i - z_j \rVert^2}(z_{i,d}-z_{j,d})^2\Big](exp(IJ) + exp(JI)) \\
& = - \frac{1}{\sigma^2_z} + \sum_{j \neq i} \frac{1}{\lVert z_i - z_j \rVert^3}(z_{i,d}-z_{j,d})^2
\Big[ y_{ij} + y_{ji} - (exp(IJ) + exp(JI))(1 + \lVert z_i - z_j \rVert) \Big] + \\
& \indent \sum_{j \neq i}\frac{1}{\lVert z_i - z_j \rVert}\Big[ - y_{ij} - y_{ji} + exp(IJ) + exp(JI) \Big]
\Big]
\end{align*}

\end{appendix}

\newpage


\bibliographystyle{apacite}
\bibliography{latent_ranking}

\end{document}