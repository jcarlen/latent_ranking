\documentclass{article}
\setlength{\oddsidemargin}{0in} 
\setlength{\textwidth}{6.5in} 
\setlength{\evensidemargin}{0in}
\setlength\parindent{24pt}
\usepackage[font = small]{caption}
\captionsetup{width=.8\textwidth}
\captionsetup{labelfont=bf}

\usepackage[natbibapa]{apacite}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{array}
\usepackage{fancyvrb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{bbm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{colortbl, xcolor}

%from jss.cls
\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}
\let\proglang=\textsf
\let\code=\texttt

\title{A Latent Space Network Modelling Approach to Ratings with  \\
Applications to Journal and Film Rating}

\author{Jane Carlen \\ \emph{University of California, Los Angeles}}

%\blind{\date{DRAFT COPY\\DO NOT CIRCULATE\\\today}{}}
%\date{}

\begin{document}
\maketitle
\thispagestyle{empty}
%\pagestyle{empty}
\setcounter{page}{0}

\abstract{We introduce a method for rating items based on network data or incomplete pairwise comparison data, employing the latent space network models developed by \citet{hoff02}, \citet{hoff03} and \citet{krivitskyetal09}. Our model estimates positions of items in latent space, along with individualized sender and receiver coefficients that capture powers of transmission for each item. Ratings are derived from the difference in sender and receiver coefficients. The method is ideal for items with nebulous similarities which can exert a range of influences on the strength of ties. Unlike existing rating methods, the latent space network model provides both a measure of uncertainty in estimates and meaningful visualization.

We show that quasi-Newton estimation produces results on par with commonly used MCMC methods in a fraction of the time. This extends the practical use of the latent space ranking method and of the class of latent space network models more generally. We present two applications. First, we rank statistics journals and show how the method addresses weaknesses of existing ranking methods. Second, we formulate movie ratings for films of several genres and demonstrate the use of latent space positions for genre detection. Our method is implemented in \proglang{R}, using the packages \pkg{latentnet} \citep{latentnet} and \pkg{visNetwork} \citep{visNetwork}}, and supplementary code which is provided. 

\newpage

\section{Introduction} \label{Introduction}

\subsection{Rating methods for network data}\label{Rating Methods}
    
The task of rating items from network or incomplete pairwise comparison data is well studied in some settings, such as ranking web pages. Perhaps the best-known ranking algorithm for network data is Google's PageRank algorithm \citep{pagerank99}. Because it is generalizable, fast, and has a guaranteed solution, it has been applied in many settings, including biology, chemistry, ecology, neuroscience, physics, sports and computer systems \citep{gleich14}. In brief, it ranks pages by the eigenvector of the dominant eigenvalue of a Markov transition matrix which describes traffic flow among web pages. The rating corresponds roughly to the equilibrium amount of time an internet user would spend on a specific web page. It is worth noting that the development of PageRank was influenced by earlier work in citation analysis. \citet{pinski76} proposed a similar eigenvalue-based method for scoring journals, with an application to ranking physics journals. 
% wiki: "The eigenvalue problem was suggested in 1976 by Gabriel Pinski and Francis Narin, who worked on scientometrics ranking scientific journals [7] and in 1977 by Thomas Saaty in his concept of Analytic Hierarchy Process which weighted alternative choices.[8]"

The main advantage of a method like PageRank over raw count-based metrics (including Impact Factor, described below) is that references from highly rated pages (or journals) are more highly valued. As \citet{pagerank99} put it in an early paper, ``we give the following intuitive description of PageRank: a page has high rank if the sum of the ranks of its backlinks is high.'' This is crucial in ranking web pages where most linking pages are not of any interest to a user. However, it is not as important when ranking fairly homogeneous catalogs of items, and in that context can lead to overemphasizing popularity, as we will illustrate in Section \ref{Example1}. 
%Also the web is HUGE but the kinds of things I consider below are smaller enough to navigate, so while losing scalability isn't great, it's not the main focus. Also, new methods being developed to make this scalable? (http://papers.nips.cc/paper/4978-a-scalable-approach-to-probabilistic-latent-space-inference-of-large-scale-networks.pdf)

Eigenfactor \citep{eigenfactor07} is a method similar to PageRank, but tailored specifically to rank journals. The main differences are 1) the data is normalized, i.e., we model the percentage of citations journals send to each other instead of raw counts, and 2) rather than affording every journal a uniform minimum weight
%which ensures a solution and makes it so that if something is pointed to by a lot of sites it will come out decently important, even if those sites aren't important
this amount is scaled to the number of articles published by each journal. (For details of the calculation see \citeauthor{eigenfactor08}.) These changes reflect the greater uniformity and much smaller scale of the journal ranking problem as compared to web-site ranking. The Eigenfactor score can be viewed as the total influence of a journal. In tandem, the Article Influence score also proposed by \citet{eigenfactor07} is a measure of the ``average influence'' of an article in a given journal. It is proportional to the Eigenfactor score divided by the number of articles published by the journal. In Section \ref{Example1} we compare our journal ranking results to rankings by these methods. 
%Eigenfactor may be more appropriate for 'subscription decisions' than rank decisions, where Article Influence may be more appropriate to "measure the average influcence of articles appearing in the same journal" ... "proportional to the Eigenfactor divided by the number of articles." i.e. now penalizing for out citations

%Also:
% HITS uses hub and authority scores. I tried running hits against latent space and page rank results(?)
% not really appropriate for the journal ranking problem

Another area where ratings on incomplete comparison networks have been thoroughly studied is in ranking sport and gaming competitors. A full discussion of available methods is beyond the scope of this paper and we refer the reader to, for example, \citet{barrow} and \citet{stefani}. An example of a network-based ranking system developed for US college football is that of \citet{parknewman05}. %The network density in that application is about 10 percent \citep{parkyook14}, and about 75 percent of matchups are between teams in the same regional conference.
Their method calculates a \textit{win} score for each team as the exponentially decreasing sum of its wins, opponents' wins, opponents' opponents' wins, etc. A \textit{loss} score is analogously calculated and the final rank of the team is its win score minus its loss score. Their measure can be viewed as an extension of Katz centrality. %the authors say it, and see 7.14 of newman. this looks the possible extension of katz centrality given there.

%Further:
%2014 paper by park and Yook? (also sports)
%ecological network?
%elections?

There is a close relationship between rating objects in networks and measuring network centrality. All of the ratings methods we reference are closely connected to an established centrality measure. In particular, the Impact Factor (described in Section \ref{Example1}) is related to degree centrality, PageRank and Eigenfactor are related to eigenvector and Katz centrality; the Park and Newman method is also related to Katz centrality. 
%ImpactFactor - degree centrality
%PageRank - eigenvector centrality/katz centrality (vs katz: http://www.sci.unich.it/~francesc/teaching/network/pagerank)
%Park and Newman - Katz centrality
%HITS - Authority and hub centrality]
In contrast, the latent space method introduced below makes allowance for the fact that influence and centrality are not synonymous. An item may not be central, but may nonetheless have the ability to influence disparate items, and this is reflected in its rating.

\subsection{Applications of latent space network models}\label{Applications}

Latent space network models have been used for various applications, but not specifically for rating, as far as we know. For example, \citet{hoffward04} used a latent space model to visualize the structure of relationships between political actors in Central Asia. %it's bilinear though
\citet{gormley07} developed a latent space model for rank data and used it to co-locate voters and candidates in an Irish parliamentary election. \citet{sewellchen15} employed such a model to dynamic network data to study network stability and the relationship between popularity and stability. They subsequently extended their model to fit dynamic clusters \citep{sewellchen16}. 

Although latent space network models have not been used to rank authors or journals, as we do in Section \ref{Example1}, they have been applied previously to analyze citation networks. For example, \citet{sarkarmoore} developed a dynamic latent space model that can track the relationships between authors and their level of influence over time, which they illustrated on NIPS co-authorship data. Latent class network models have also been used to discover communities in citation networks, for example in \citet{leichtetal}, but the addition of latent positions  adds capability to identify externally mislabeled nodes, as we demonstrate in Section \ref{Example2}.

\subsection{Overview}\label{Overview}

The latent space network rating method we introduce incorporates the following features uniquely: 1) accounting for similarity between nodes, whether implicit or through covariates; 2) providing measures of uncertainty in estimates; 3) meaningfully and easily visualizing results; 4) distinction between influence and centrality; and 5) simple implementation in \proglang{R} \citep{r}. The method is applicable to directed networks, including those derived from pairwise comparison data. We focus on the case where the edges are valued and can be reasonably modeled as Poisson-distributed.

Section \ref{Model} of the paper details the latent space network model we employ and its estimation methods. Sections \ref{Example1} and \ref{Example2} describe two applications of the model, first to ranking statistics journals using citation data and second to rating films from several genres. In both applications we discuss the value added through network visualization. Section \ref{Discussion} concludes the paper with a discussion of benefits, limitations, and possible future improvements to the model.

\section{Latent space network model for rating} \label{Model}

We denote a network of $n$ nodes by its adjacency matrix $Y = \{y_{ij}\}, 1 \leq i,j \leq n$. A dyad in the network consists of two directed edges, $Y_{ij}$ and $Y_{ji}$. The latent space models introduced by \citet{hoff02} assume that nodes in a network have implicit positions in ``social space''. Given these $d$-dimensional positions, $Z$, as well as possible covariates, $X$, and corresponding parameters $\beta$, the probability of an edge is independent of all other edges. Thus, the probability of a graph $Y$ is the product over its edges

$$P(Y|X, \beta, Z) = \prod_{i\neq j} P(y_{ij}|x_{ij}, \beta, z_i, z_j).$$

\citet{hoff03} recast the parameters with unobserved random effects,

\begin{equation}\label{eq1}
P(Y|X, \beta, \gamma) = \prod_{i\neq j} P(y_{ij}| x_{ij}, \beta, \gamma_{ij}),
\end{equation}


where here we model $\gamma$ as in the ``distance model'' of \citet{hoff03}:

$$\gamma_{ij} = a_i + b_j + \epsilon_{ij},$$

\begin{equation}\label{eq2}
\epsilon_{ij} = f(z_i,z_j) = - \lVert z_i - z_j \rVert.
\end{equation}

We consider $a_i$ and $b_j$ to be node-specific sender and receiver effects. $\lVert z_i - z_j \rVert$ is the Euclidean distance between nodal positions $z_i$ and $z_j$. (\citet{hoff02} also considered an asymmetric projection model which we do not employ.) Although the positions can be in high-dimensional space, we usually consider one to three dimensions for reasons of interpretability, visualization, or parsimony. %Didn't see max on dimension in the package. other distances are theoretically possible but the ergmm package uses Euclidean distance, see "fitting position" article, or projection distance.


Adapting machinery of the generalized linear model (GLM), let

\begin{align}\label{eq3} 
E(y_{ij}) &= g^{-1}(\eta_{ij}) \nonumber \\
\eta_{ij} &= \beta' x_{ij} + a_i + b_j - \lVert z_i - z_j \rVert.
\end{align}

\noindent In the context of our applications, we assume $y_{ij}$ is Poisson distributed and let $g$ be the standard log link function. In some cases other distributions may be more appropriate, but here we use the Poisson because we are dealing with count data. This is in contrast to the binomial distributions of edge weights in the quasi-Stigler model described in Section~\ref{QS}. Unlike the quasi-Stigler model the estimates are not conditioned on the total weight of each dyad ($y_{ij} + y_{ji}$). 

% latentnet doesn't allow for a unique number of trials in a binomial distribution on each edge, but you can use optim to get a quick estimate with binomial likelihood. Fixing dyad totals basically nullifies the positions though.

We note some features of the model:

\begin{itemize}

\item Increasing distance between $z_i$ and $z_j$ implies decreasing expectation of $y_{ij}$. One way to view this is as controlling for similarity between nodes. Nodes with salient similarities are likely to have fitted positions relatively close together. Some of the magnitude of their connection is attributable to their similarly, and the rest to their individual sender (``push'') and receiver (``pull'') effects.

\item The effect of positions on expected edge weights is symmetric, affecting both edges in a dyad equally. To condition on total weight, as in the quasi-Stigler method discussed below, greatly diminishes the value of estimated positions.

\item The \textit{rating} or \textit{score} of node $i$ is its receiver minus sender coefficient,

\begin{equation}
rating_i = b_i - a_i. 
\end{equation}

Its \textit{rank} is derived from its order among the ratings.

\end{itemize}

\subsection{Parameter estimation}\label{Parameter Estimation}

With the introduction of latent space network models, \citet{hoff02} developed a Markov chain Monte Carlo (MCMC) estimation algorithm. \citet{hoff03} added the capacity to fit random effects, and \citet{krivitskyetal09} extended the model further and refined the underlying algorithm. In this section we present an overview of the MCMC estimation described by those authors, adapted for the latent space rating model and embellished with details from the \pkg{latentnet} implementation \citep{latentnet_jss, latentnet}. We conclude the section by discussing previous use of quasi-Newton estimation for latent space models and tradeoffs between this strategy and MCMC. 

First, we describe the Bayesian framework and initialization method. This applies to both MCMC estimation and direct optimization by a quasi-Newton method. Our aim is to return a sample from the posterior parameter distribution and desired point estimates, such as a maximum likelihood estimate, posterior mean and mode. We adapt Equation \ref{eq1} for the case when the random effects are as described in (\ref{eq2}), and our only nodal covariate is the intercept term, $B_0$.

\begin{equation}\label{eq4}
P(\theta|Y) \propto P(Y|\theta)P(\theta) = \prod_{i\neq j} P(y_{ij}|\beta_0, z_i, z_j, a_i, b_j)P(\theta).
\end{equation}

%"A closed form expression for the desired conditional distribution is generally unavailable" \citep{hoff03} (When? Here it is available)

We posit independent normal distributions for the components of $\gamma_i$, $i = 1,....,n$ \citep{hoff03}.

$$a_i \sim N(0, \sigma_a^2))$$
$$b_i \sim N(0, \sigma_b^2))$$
$$z_i \sim MVN_d(0, I_d*\sigma_z^2))$$ %I switched mean from \mu to 0

We expand Equation \ref{eq4}:

\begin{equation}\label{eq5}
P(Y|\theta)P(\theta) = \prod_{i\neq j} P(y_{ij}|\beta_0, z_i, z_j, a_i, b_j)P(a|\sigma_a^2)P(b|\sigma_b^2)P(z|\sigma_z^2)P(\beta_0)P(\sigma_a^2)P(\sigma_b^2)P(\sigma_z^2)
\end{equation}

Equation \ref{eq5} reflects the dependence structure of the parameters as displayed in Figure \ref{fig:dep}. As stated above we assume $y_{ij}|\beta_0, z, a, b$ is Poisson distributed with mean parameter given by Equation \ref{eq3}, where $B = B_0$ and $g^{-1} = exp$.

\begin{figure}
  \centering
    \includegraphics[width=0.3\textwidth]{dependency_struct.pdf}
    \caption{Dependence structure of the latent space sender-receiver model.}
  \label{fig:dep}  
\end{figure}

We place priors on the elements of $\beta$, $\sigma_z^2$, $\sigma_a^2$ and $\sigma_b^2$.

$$\beta \sim N(\textbf{0}, I\sigma_\beta^2)$$ %ergmm default 0,9
$$\sigma_a^2 \sim \text{Scale-inv-}\chi^2 (v_a, s^2_a))$$ %ergmm default 3,1
$$\sigma_b^2 \sim \text{Scale-inv-}\chi^2 (v_b, s^2_b))$$ %ergmm default 3,1
%documentation for rsender/rreceiver says prior for variance of those effects has scale-inverse-chi-squared distribution which is equivalent to the inverse gamma described in the papers, just reparameterized.-->
$$\sigma_z^2 \sim \text{Scale-inv-}\chi^2 (v_z, s^2_z))$$ %ergmm determined by default function below -  


The estimation algorithm of \pkg{latentnet} sets default values for the hyperparameters to generate diffuse distributions on the prior parameters. The default value for $\sigma_\beta^2$ is $9$ to allow a wide range of $\beta$ values.   %hoff03 uses 100. Is wider better? 
Low degrees of freedom ($v_a$ = $v_b$ = $3$) reflect uncertainty in the values of $\sigma_a^2$ and $\sigma_b^2$, while the default scale parameters ($s^2_a$ = $s^2_b$ = $1$) curtail them to a wide but reasonable range. The default values for $v_z$ and $s^2_z$ are $\sqrt{n}$ and $\frac{1}{8}\sqrt[\leftroot{-3}\uproot{3}d/2]{n}$. These values reflect that larger networks tend to take up more space, but as observed network size increases the influence of prior variance should decline. For discussion of the choice of hyperparameters see \citet{krivitskyetal09} and \citet{latentnet_jss}. These values are fixed throughout the estimation process. %these defaults assume no clusters in the model

<<r more_prior_notes, eval = F, echo = F, warning = F, message= F>>=
#InitErgmm.euclidean<-function(model, d, G=0, var.mul=1/8, var=NULL, var.df.mul=1, var.df=NULL, mean.var.mul=2, mean.var=NULL, pK.mul=1, pK=NULL)

# What is the motivation for these functions? Documented somewhere?

#1. Z.var - scale parameter of the inverse chi square prior on var of z's
#  if(!("Z.var" %in% names(model[["prior"]]))) model[["prior"]][["Z.var"]]<-model[["prior"]][["Z.var.mul"]]*(network.size(model[["Yg"]])/max(1,model[["G"]]))^(2/model[["d"]])
#DEFAULT: 1/8 * #vertices / max(1, #clusters)^(2/dimension)

#2. Z.var.df - dof parameter of the inverse chi square prior on var of z's
#  if(!("Z.var.df" %in% names(model[["prior"]]))) model[["prior"]][["Z.var.df"]]<-model[["prior"]][["Z.var.df.mul"]]*sqrt(network.size(model[["Yg"]])/max(1,model[["G"]]))
#DEFAULT: 1 * sqrt(#vertices) / max(1, #clusters)
# why sqrt? based on presumed density/dependence?

#3. Z.mean.var - variance of the gaussian prior on cluster means, if necessary
# if(!("Z.mean.var" %in% names(model[["prior"]]))) model[["prior"]][["Z.mean.var"]]<-model[["prior"]][["Z.mean.var.mul"]]*model[["prior"]][["Z.var"]]*max(1,model[["G"]])^(2/model[["d"]])
#DEFAULT:  2 * Z.var * max(1, #clusters)^(2/dimension)

#4. Z.pK - parameter (#clusters) of the dirichlet prior on cluster assignment, if nec.
#if(!("Z.pK" %in% names(model[["prior"]]))) model[["prior"]][["Z.pK"]]<-model[["prior"]][["Z.pK.mul"]]*sqrt(network.size(model[["Yg"]])/max(1,model[["G"]]))
#DEFAULT:  1 * sqrt(#vertices)/max(1,#clusters)
@

We must supply initial parameter values. Below we list the default initializations implemented in \pkg{latentnet}. They are functions of the observed network. While they may speed convergence in some cases, in our applications in low dimension we found that random initialization on a reasonable scale performs as well or better. \newline

\begin{itemize}
\item[] $z^{(0)}$: The positions are initialized through either multidimensional scaling (MDS) or normal draws. In the former, the geodesic distances for all dyads are computed from the binary adjacency matrix $Y_b$. Disconnected pairs are given distances of $n$. The initial value $z^{(0)}$ is then computed by multidimensional scaling, returning optimal $d$-dimensional coordinates whose Euclidean distances best approximate the geodesic distances between nodes. In the latter, $z^{(0)}$ is generated via independent draws from a normal distribution. \citet{hoff02} noted that the choice of initialization does not impact their results. We found that the scale of the initial positions was more important than the values, with preference for smaller starting values.

\item[] $\displaystyle a_i^{(0)} =  logit \left( \frac{Y_{b_{i \cdot}}  + 1}{n - 1 + 2} \right) - \frac{1}{n}\sum_{j=1}^{n} logit \left( \frac{Y_{b_{i \cdot}} + 1}{n - 1 + 2} \right)$

The initialization of sender parameters shown above is derived by considering initial nodal degrees as binomially distributed with $n-1$ trials, observed success probability $\frac{Y_{b_{i \cdot}}}{n-1}$, and a uniform prior on success probability. As a reminder, $Y_b$ denotes the binary adjacency matrix and $Y_{b_{i \cdot}}$ denotes the $ith$ row sum of $Y_b$. The initialization of receiver coefficients is analogous. %n-1 is the number of possible edges on each node. the +1(num), +2(denom) is as though a^(0) is a prediction/new draw of the number of edges when edges ~ binomially with total trials n-1, observed success prob rowsums/(n-1), and with a uniform prior on the success prob (see gelman p. 36)
% See ergmm.initvals, bayes.prop

\item[] $\displaystyle b_i^{(0)} = logit \left( \frac{Y_{b_{\cdot i}}  + 1}{n - 1 + 2} \right) - \frac{1}{n}\sum_{i=1}^{n} logit \left( \frac{Y_{b_{\cdot i}} + 1}{n - 1 + 2} \right)$

\item[] $\displaystyle \beta_0^{(0)} = logit\left( \frac{1}{n(n-1)}\sum_{i,i\neq j}\mathbbm{1}(y_{ij} > \overline{y_{ij}})\right) + \frac{1}{{n \choose 2}}\sum_{i,i<j}{\lVert z^{(0)}_i - z^{(0)}_j \rVert}$ 

The initial intercept is composed of an ``edge intercept,'' a valued-network analog of graph density, plus a ``distance intercept,'' the average initial pairwise distance.

% See ergmm.initvals 
% logit(mean(indicator(Y_ij > mean(Y_ij))))
% Yg<- model[["Yg"]]
% Ym<-getYm(Yg,model[["response"]]) #getYm returns as.matrix.network(Yg, "citations", matrix.type="adjacency") with NA's on diag.
% Ym01<-Ym>mean(Ym,na.rm=TRUE)
% pm[["beta"]]<-logit(mean(Ym01,na.rm=TRUE))+if(!is.null(pm[["Z"]]))mean(dist2(pm[["Z"]])) else 0 #edge intercept + distance intercept

\item[] The initial values of $\displaystyle \sigma_z^{2(0)}, \sigma_a^{2(0)},\text{and } \sigma_b^{2(0)}$ are the variances of $z^{(0)}, a^{(0)},\text{and }b^{(0)}$, respectively.
\end{itemize}

\noindent \emph{Parameter Updates}:  \newline

Before starting an MCMC chain, \pkg{latentnet} employs an intermediate optimization step. It uses the bounded quasi-Newton optimization routine of \citet{lbfgsb}, as implemented in the \code{optim} function of the \proglang{R} base package \pkg{stats}. This returns starting values for MCMC with higher posterior likelihood than the initial values described above. In our applications the results are competitive with the final MCMC output, and we compare them in Section \ref{estimeval}.

After intermediate optimization, the MCMC chain runs through a suitably long burn-in period. If the automated burn-in length is insufficient, proper length can be determined by carrying out MCMC diagnostics on the results, such as those in the \code{mcmc.diagnostics} function of \pkg{latentnet}. Once starting values for MCMC sampling are determined, parameters are updated as follows:  

%what is the ordering/grouping of parameter updates? latentnet allows for group.deltas for proposing changes to multiple parameters simultaneously. When are they updated singly vs as a group?

%how does latentnet's adaptive sampling work, e.g. how are proposal dist. parameters updated?

\begin{itemize}

\item $\sigma^2_a, \sigma^2_b, \sigma^2_z$: 

The variances of the sender, receiver, and position parameters may be sampled directly from their posterior distributions because they were assigned conjugate priors. 

$$\displaystyle \sigma^2_a|a \sim \text{Scale-inv-}\chi^2 (v_a + n, \frac{v_a s^2_a + \sum_{i=1}^n{a^2_i}}{v_a + n}))$$

$$\displaystyle \sigma^2_b|b \sim \text{Scale-inv-}\chi^2 (v_b + n, \frac{v_b s^2_b + \sum_{i=1}^n{b^2_i}}{v_b + n}))$$

$$\displaystyle \sigma^2_z|z \sim \text{Scale-inv-}\chi^2 (v_z + n*d, \frac{v_z s^2_z + \sum_{i=1}^{n*d}{z_{i,d}^2}}{v_z + n*d}))$$

\item Actor-specific parameters, $a, b, z$:

The sender, receiver and position parameters cannot be sampled directly and may be strongly correlated. They are updated for each actor in random order by Metropolis-Hastings block updates.

  \begin{enumerate}
  %could be metrpolis since symmetric proposal distribution
  
  \item Propose $z_i^*, a_i^*, b_i^*$ from symmetric proposal distributions. %s, $J(z|z^{(t)})$.
  $$z_i^* \sim MVN_d(z_i, \tau_z^2I_d)$$
  $$a_i^* \sim N(a_i, \tau_a^2)$$
  $$b_i^* \sim N(b_i, \tau_b^2)$$

  \item Accept as a block with probability $min(1, \frac {P(Y|z^*, a^*, b^*, \beta) P(z^*)P(a^*)P(b^*)} {P(Y|z, a, b, \beta) P(z)P(a)P(b)}$).

  %\item $\theta\setminus z$: Metropolis-Hastings \citep{hoff03}.

  \end{enumerate}
  
\item $\beta$, shift of random effects, position scale:

  To speed convergence, a simultaneous shift in $\beta$ and the random effects and a rescaling of the positions is proposed. The magnitude of the shifts and multiplier is proposed by: 
  $$(h_\beta, h_a, h_b, h_z) \sim MVN_4(0, \tau_{\beta,a,b,z})$$
  $$ \beta^* = \beta +h_\beta $$
  $$ a^* = a +h_a $$
  $$ b^* = b + h_b $$
  $$ z^* = exp(h_z)z $$
  $$ \sigma_z^{2*} = exp(2h_z)\sigma_z^2 $$
  
  The move is block-accepted or rejected. For discussion of the acceptance probability see Section 3.2 of \citet{krivitskyetal09}.

  \item \textit{Proposal Variances} $\tau_z^2, \tau_a^2, \tau_b^2, \tau_{\beta,a,b,z}$:
  
  The variances of the proposal distributions are set adaptively during the burn-in period to stay near a fixed acceptance rate, with a default target rate of 0.234 (\cite{latentnet_jss}, following \cite{nealandroberts}).
  
\end{itemize}


%This is just basic MH so I don't need to describe until I get more detail on the prosal distribution. [In \citep{shortreed06} proposal distributions are Gaussian centered at current values, $\theta^{(t)}$ with some standard deviation, $\delta_{\beta}$ or $\delta_z$. Is this still true in latentnet? If so, and since these are symmetric, is all done with Metropolis?]

%1. Propose $\theta^*$ from a proposal distribution, $J(\theta|\theta^{(t)})$ %[individual or group?]

%2. Compute acceptance probability $r = min(1, \frac{P(Y|\theta^{*})P(\theta^{*})J(\theta^{(t)}|\theta^{*})}  {P(Y|\theta^{(t)})P(\theta^{(t)})J(\theta^{*}|\theta^{(t)})})$ %lots of cancellation here if only updating one parameter at a time

%3. Set $\theta^* = \theta^{(t+1)}$ with probability $r$, otherwise $\theta^{(t+1)} = \theta^{(t)}$


\emph{MCMC Post-processing}:  \newline
  
  The likelihood depends on positions only through their pairwise distances, and is invariant to rotations, reflections and translations of the positions. We are interested in the posterior variance in positions that comes from changing distances between points rather than distance-preserving transformations. One way to address this and stabilize our estimates is to store not the sampled positions, but a transformed set that has minimal squared distance to a set of reference positions. This is the Procrustean transformation used by \citet{hoff02}. They let $z^*_{store} = argmin_{Tz^*}tr(z_{ref} - Tz^*)^\top(z_{ref} - Tz^*)$, where $T$ is the set of distance-preserving transformations.
  
There may be strong correlation or near non-identifiability between a node's actor-specific parameters, especially if it is poorly connected. To reduce instability in the estimate \citet{shortreed06} considered the parameter estimate that is optimal in the sense of minimizing Bayes risk with a Kullback-Leibler (KL) loss function. Their ``MKL'' estimate minimizes the posterior expectation of the KL divergence from its predictive distribution of networks to the posterior predictive distribution of networks. As such, the MKL estimate only pertains to the parameters on which networks are immediately dependent.
\begin{equation}
\theta_{MKL} = argmin_{\widetilde{Z} \widetilde{a}, \widetilde{b}, \widetilde{\beta}} \left[ E_{Z, a, b, \beta|Y_{obs}}\left[\sum_Y log(\frac{P(Y|Z, a, b, \beta)}{P(Y|\widetilde{Z} \widetilde{a}, \widetilde{b}, \widetilde{\beta})})P(Y|Z, a, b, \beta)\right]\right]
\end{equation}
The MKL positions are more stable than standard point estimates because they average over all networks. They require the posterior sample to calculate, so, unlike \citet{hoff02}, the sampled positions are transformed with MKL positions as reference \textit{after} the sampling is complete.

<<r invchisq, eval = T, results = 'hide', echo = F, cache = T, warning = F, message = F>>=
#took these from geoR package which was crashing

rinvchisq <- function (n, df, scale = 1/df) 
{
  if ((length(scale) != 1) & (length(scale) != n)) 
    stop("scale should be a scalar or a vector of the same length as x")
  if (df <= 0) 
    stop("df must be greater than zero")
  if (any(scale <= 0)) 
    stop("scale must be greater than zero")
  return((df * scale)/rchisq(n, df = df))
}

dinvchisq <-function (x, df, scale = 1/df, log = FALSE) 
{
  if (df <= 0) 
    stop("df must be greater than zero")
  if (scale <= 0) 
    stop("scale must be greater than zero")
  nu <- df/2
  if (log) 
    return(ifelse(x > 0, nu * log(nu) - log(gamma(nu)) + 
                    nu * log(scale) - (nu + 1) * log(x) - (nu * scale/x), 
                  NA))
  else return(ifelse(x > 0, (((nu)^(nu))/gamma(nu)) * (scale^nu) * 
                       (x^(-(nu + 1))) * exp(-nu * scale/x), NA))
}
@

%Estimation is implemented in the \code{ergmm} function of \pkg{latentnet} \citep{latentnet_jss}. 

\subsubsection{Quasi-Newton estimation}\label{QN}

Typically, quasi-Newton estimation has been used as an initialization step of MCMC, as in \citet{hoff02} and \citep{latentnet}. \citet{handcock07} employed it for the first stage of a two-stage maximum likelihood method to estimate a latent space cluster model. They found that the two-stage MLE gives a good match to the MCMC fit in terms of cluster membership and relative positions, but the clusters are more spread out. However, in that case, it was only applied to the simplified model without clusters, so it could not capture dependence between positions and cluster assignments. In addition, the networks being modelled were binary, so edges provided less granular information than in valued networks.

Methods proposed to speed up latent space mode fitting, such as the variational Bayesian method of \citet{vblpcm10, vblpcm} or the case-control approximate likelihood of \citet{raftery12}, treat Bayesian MCMC as the benchmark for estimation speed and complexity. These methods compromise the true likelihood function and introduce bias on behalf of speed, justified by impracticality of MCMC estimation. In examples of \citet{vblpcm10}, positions fit by the variational Bayesian algorithm also show greater within-cluster variance than MCMC estimates. In our trials on networks of up to several hundred nodes, results from quasi-Newton estimation closely approximate those from MCMC, as we discuss in Sections \ref{estimeval} and \ref{Model2}.

The main argument against a quasi-Newton estimation method is that it is not guaranteed to converge to a global optimum since the likelihood function is not convex. However, there are several reasons why it is often still successful in practice. First, in applications well suited to latent space network rating the search space for the positions is relatively small. This is especially true when positions are in low dimension. Second, although MCMC estimation is theoretically guaranteed to converge to the true distribution, it may face prohibitively slow mixing time and fail to converge to the global optimum. We see a minor example of this in Section \ref{estimeval}. Third, the speed of the quasi-Newton method means we can consider many initial values to increase our chance of finding the global optimum. If necessary, we can still use MCMC estimation to validate the quasi-Netwon results, but with a much smaller burn-in than would otherwise be required.

Although quasi-Newton estimation does not return a sample from the posterior parameter distribution like MCMC estimation does, we can still approximate the uncertainty in ratings using a Poisson GLM. This is described in Section \ref{estimeval}. While there may be strong dependence between positions and sender or receiver parameters, the dependence between the positions and the ratings (receiver minus sender) is much less. This is confirmed by the similarity we find between MCMC estimates of ratings uncertainty and those by the Poisson GLM.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SETUP FOR APPLICATION SECTION %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<r setup, echo = F, include=FALSE, warning = F, message= F, cache = T, results = 'hide'>>=
knitr::opts_chunk$set(echo = FALSE, results = 'hide')
library(ergm)
library(ernm)
@

<<r setup2, cache = T, echo = F, results='hide', warning=F, echo = F, message=F>>=
library(latentnet)
library(ergm.count)
#load matrix

Cmatrix <- as.matrix(read.csv("~/Documents/citation/Citation_supplement/Data/cross-citation-matrix.csv", row.names = 1)) #47 x 47 #

#Self-citations are removed: The highest raw counts are from self-citations of CSDA (486) and StMed (628), which would seem to skew any analysis of importance. The authors exclude self-citations for the Stigler method: "Both the eigenfactor and the article influence score are computed over a 5-year time period, with journal self-citations removed to eliminate possible sources of manipulation."

Cmatrix.diag = Cmatrix #store a copy before removing diag
diag(Cmatrix) = rep(0,47) #shouldn't actually matter, as.network zeros out diag

#Note the use of the tranposed Cmatrix in the next line to correspond to standard i,j entry = citation FROM i to j. Original Cmatrix has i,j indicates citation from j to i.

Cnet = as.network(t(Cmatrix), directed=T, matrix.type="a", ignore.eval=F,  
       names.eval="citations") #as valued net, see \cite{krivitsky2015}

#as a binary network
Cbinet <- as.network(t(Cmatrix))

#cited/citing
cited = rowSums(Cmatrix) #citations in
citing = colSums(Cmatrix) #citations out
cite.ratio = cited/citing

#normalized
Cmatrix.norm = t(Cmatrix)/citing #entries are % of i's citations to j (column)
Cnet.norm = as.network(Cmatrix.norm, directed=T, matrix.type="a", ignore.eval=F,
            names.eval="citations")
@

<<r setup_jrss, cache = T, echo = F, results='hide', warning=F, echo = F, message=F, dependson = "setup">>=
#code from JRSS-PR-SA-Dec-13-0008_supplement.R to the paper
journal.abbr <- rownames(Cmatrix)

Tmatrix <- Cmatrix + t(Cmatrix)
diag(Tmatrix) <- diag(Cmatrix.diag)

journals.cluster <- hclust(d = as.dist(1 - cor(Tmatrix)))
#plot(journals.cluster, sub = "", xlab = "")
#cutree(journals.cluster, h = 0.6)  
library(BradleyTerry2)

Cdata <- countsToBinomial(Cmatrix)
fit <- BTm(outcome = cbind(win1, win2), player1 = player1, player2 = player2, data = Cdata)

npairs <- NROW(Cdata)
njournals <- nlevels(Cdata$player1)
phi <- sum(residuals(fit, "pearson")^2) / (npairs - (njournals - 1))

## 3.1 Journal residuals
journal.res <- rep(NA, njournals)
res <- residuals(fit, type = "pearson")
coefs <- c(0, coef(fit)) # 0 is the coefficient of the first journal
for(i in 1:njournals){
    A <- which(Cdata$player1 == journal.abbr[i])
    B <- which(Cdata$player2 == journal.abbr[i])
    y <- c(res[A], -res[B])
    x <- c(-coefs[Cdata$player2[A]], -coefs[Cdata$player1[B]])
    journal.res[i] <- sum(y * x) / sqrt(phi * sum(x ^ 2))
}
names(journal.res) <- journal.abbr

library(qvcalc)
cov.matrix <- matrix(0, nrow = njournals, ncol = njournals)
cov.matrix[-1, -1] <- vcov(fit)
qse <- qvcalc(phi * cov.matrix, estimates = c(0, coef(fit)),
              labels = journal.abbr)
#qse[["covmat"]] = cov.matrix*phi

export.scores <- qse$qvframe$estimate
export.scores <- export.scores - mean(export.scores)
names(export.scores) <- journal.abbr

sort.id <- sort(export.scores, decreasing = TRUE, index.return = TRUE)$ix
fit.table <- data.frame(quasi = export.scores[sort.id], qse = qse$qvframe$quasiSE[sort.id])
rownames(fit.table)
rownames(fit.table)[c(1,6,20)] = c("JRSS-B", "JRSS-A", "JRSS-C")
match(rownames(fit.table),Cnet%v%"vertex.names")
fit.table2 = fit.table[order(match(rownames(fit.table),Cnet%v%"vertex.names")),]
@

<< llik_and_latent_qn, cache = T, echo = F, results='hide', warning = F, message = F, dependson = c("setup_jrss", "latent_sr2_plot")>>=

# source necessary functions:

source("./latent_ranking_repo/ls_quasi_newton.R")

# contains, llik, log-likelihood function, off by a constant: 
# contains lsqn, the latent space quasi-netwon function

# helper function
dist2 <- function(Z) {
  as.matrix(dist(Z, upper = T))
}

@

%could potentially embed users in the same network...for later

\section{Ranking statistics journals using citation data}\label{Example1}

In this section we examine rankings of 47 statistics and probability journals. The data set we consider was gathered and analyzed by \citet{varinetal} from Journal Citation Reports \citep{JCR}. It consists of a $47 \times 47$ matrix of directed citation counts, encompassing within-network citations from 2001 to 2010. We compare latent space model rankings to several competing methods in Section \ref{Comparison}, visualize our results in Section \ref{Visualization}, and evaluate competing models and estimation methods in Section \ref{modeleval}. 

Impact Factor is the most commonly referenced journal rating measure, despite widespread criticism \citep{if_research, if_abuse, if_poor}. %It is the only such measure listed on the sidebar of a journal's Wikipedia page.
Impact Factor measures how frequently articles from a specific journal are cited. An Impact Factor of $1.0$ means that articles published by that journal in the last two years have been cited once on average \citep{JCR}. Journal Citation Reports also publish modified versions of Impact Factor that exclude journal self-citations or alter the size of the time window to one or five years. These modifications address two problems with Impact Factor, but it has additional pitfalls as a proxy measure of journal quality. %It is fair to argue that authors should not be judged at all on the journal ranking (importance by association) but we're just not addressing that here. that's for another paper. %Aston further commented that this is ``inherently dangerous if weight is given to such metrics in crucial decisions such as grant awards or promotion and tenure cases'' \citep{varincomment}. 

% The Immediacy Index only averages over citations receved in the last year, while the IF5 expands the window to five years. The IFno excludes journal self-citations \citep{varinetal}. This addresses a major criticism of the impact factor, which is that journals can artificially inflate their score, whether intentionally or not, by encouraging cross-citations in published articles. Changing the size of the window for citations balances the criticism that articles may not achieve peak influence for several or more years, a lag that depends on the publication field, with the desire to include newer journals in the rankings and measure immediate impact.
%A 5-year Impact Factor is also published --  The 5-year journal Impact Factor is the average number of times articles from the journal published in the past five years have been cited in the JCR year. It is caclulated by dividing the number of citations in the JCR year by the total number of articles published in the five previous years.
%JCR Year (Journal Citation Report Year): The year of the JCR edition displayed in the top right-hand corner of the page. Each JCR year contains one year of citation data. You select the JCR year on the Welcome page.-->

<<r cite_cor, echo = F, results = 'hide', eval = T, cache = T, dependson = c("setup", "setup_jrss")>>=
ij = t(Cmatrix)[lower.tri(t(Cmatrix), diag = F)]
ji = Cmatrix[lower.tri(Cmatrix, diag = F)]
cor(ij, ji)
@

First, Impact Factor does not normalize for article length or out-citations. Whether planned or not, there is documented reciprocity in citations between journals. (In our data the correlation in one-way citations is $0.57$.) Second, it does not account for differences in citation patterns between fields, such as mathematics papers tending to have relatively few citations while bioscience papers have many \citep{if_fraction}. Third, the distribution of citations counts by article is very long-tailed, with a few articles receiving many citations and most receiving only a few. As Colquhoun wrote in the Discussion on the paper by \citet{varinetal}, ``It has been obvious for a long time that it is statistically illiterate to characterize very skew distributions by their mean. And it is statistically illiterate to present point estimates with no indication of their uncertainty'' \citep{varincomment}. %latent space ranking uses totals, not means, and has a measure of uncertainty 
%https://www.timeshighereducation.com/news/citation-averages-2000-2010-by-fields-and-years/415643.article

\subsection{The quasi-Stigler model} \label{QS}
 
\citet{varinetal} introduced the \textit{quasi-Stigler} model to address the criticisms above. The second criticism is not accounted for by the model, but by restricting the data to only 47 out of 110 journals of statistics and probability. The quasi-Stigler model requires that journals are fairly homogeneous and have a relatively high level of citation exchange. %"A key requirement for the methods that are described here, as well as in our view for any sensible analysis of citation data, is that the journals jointly analysed should be as homogeneous as possible. Accordingly, analyses are conducted on a subset of the journals from the statistics and probability category, among which there is a relatively high level of citation exchange."" [@varinetal]

%The percent of citations to/from other statistics journals considered is roughly 15 to 60 percent. The median fraction of sent citations lost by subsetting from statistics journals is 4%; received is 7%. <!--of the statistics and probability category of JCR-->

The model measures each journal's ```propensity to export intellectual influence''' (\citeauthor{varinetal}, quoting Stephen Stigler). The rank of journal $i$ is determined by its \textit{export score}, $u_i$, under the assumption that citations counts, $C_{ij}$, are quasi-binomially distributed as follows:

\begin{equation}\label{eq4a}
\begin{aligned}
E(C_{ij}) &= t_{ij}\pi_{ij} \\
\pi_{ij} &= logit^{-1}(u_j - u_i) \\
& = \frac{exp(u_j - u_i)}{1 + exp(u_j - u_i)} \\
var(C_{ij}) &= \phi t_{ij} \pi_{ij} (1 - \pi_{ij})
\end{aligned}
\end{equation}

% \mu_i = b_i - a_i, \text{ where } C_{ij} \text{ is quasi-binomially distributed and } E(C_{ij}) = t_{ij}\exp{(a_i+b_j)}. 

\noindent where $t_{ij}$ is the observed total number of citations between journals $i$ and $j$, i.e.,~$t_{ij} = c_{ij} + c_{ji}$. The notation here is slightly different than in \cite{varinetal} due to transposition of the citation matrix $\{c_{ij}\}$. We note as \citeauthor{varinetal} that the export scores could be obtained as estimates from a quasibinomial GLM with logit link.
% see note in code chunk about how response and weights are set in R for glm, 

<<r glm, eval = T, cache = T, echo = F, results = 'hide', dependson= c("setup", "setup_jrss", "latent_sr1"), fig.keep='none', warning=F>>=

library("extraDistr") #for rbern

Tmatrix <- Cmatrix + t(Cmatrix)

# 1. Binomial GLM ####

# build design matrix - adjusted to not repeat observations, since they should be independnet

n = 47
k = 1
x1 = matrix(0, choose(n, 2), n)
for (i in 1:(n-1)) { #sender
  for (j in (i+1):n) { 
    x1[k,i]=1;
    x1[k,j]=-1;
    k = k + 1
    }
}
x1[,1] = 0 #forces first coef to NA, later set -> 0

#response 
y = as.vector(t(Cmatrix))/as.vector(Tmatrix)
y = y[which(lower.tri(Cmatrix))]
y[is.nan(y)]=0

# model
g1 = glm(y~x1-1,family = quasibinomial(link="logit"), weights = Tmatrix[lower.tri(Cmatrix)])
g1$coefficients[1] = 0
# Note that this is one way to set up the quasibinomial glm in R: 
# "As a numerical vector with values between 0 and 1, interpreted as the proportion of successful cases (with the total number of cases given by the weights)."
# AIC NA because fit by quasi-likelihood

# Results Identical to Sigler

cor(fit.table2$quasi, g1$coefficients, use="pairwise.complete.obs")
    
plot(fit.table2$quasi, g1$coefficients, ylab = "glm", xlab = "Stigler") 
    #shifted by -.7 = fit.table2$quasi[47]

summary(y-predict(g1)*as.vector(Tmatrix))

# Overdispersion - no need to estimate seperately - now agres with paper (after removing repeats from design matrix), 1.759
summary(g1)$dispersion

## 1a. Binomial GLM sender + receiver ####

#Setting it up with "sender"" and "receiver" effects in the design matrix gives basically the same results but it's superfluous, as the "sender" and "receiver" coefficents are perfectly negatively correlated (we're really fitting the differences).

#build design matrix:
x1a = matrix(0,n^2,2*n)
for (i in 1:n) { #sender
  for (j in 1:n) { 
    if (i != j) {
      x1a[n*(i-1)+j,i]=1;
      x1a[n*(i-1)+j,j+n]=1;
    }
  }
}

#g1a = glm(y~x1a-1,family = quasibinomial(link="logit"), weights = as.vector(Tmatrix))

#identical to Stigler up to scaling and loss of 1 DOF (last value NA) :
#cor(fit.table2$quasi, g1a$coefficients[1:47]-g1a$coefficients[48:94], use="pairwise.complete.obs") #=1

#"receiver" and "sender" coefficents perfectly negatively correlated.
#cor(g1a$coefficients[1:47], g1a$coefficients[48:94], use = "complete.obs")

# 2. Poisson GLM (compare to latent.srp1) ####

#response
y2 = as.vector(t(Cmatrix))

#model
g2a = glm.fit(x1a-1, y2, family = poisson(link="log"))
# matches latent.srp1. (where no option for weights or quasi)
# latent.srp1 was the sender receiver only model with fixed effects
# what if weighted and quasi? more appropriate? should these be options of ergmm?
# remember with binomial we have the added constraint that y_ij + y_ji = 1

#  g2a
# MKL S-R fit estimates almost match (shifted) latent.srp1 estiamtes
# Are the underlying models the same, just by different fits?
# '"glm.fit" uses iteratively reweighted least squares (IWLS)' (always same output)
# ergmm uses mcmc (output differs slightly each time)

#plot(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47],
 #    g2a$coefficients[48:94] - g2a$coefficients[1:47])
#cor(latent.srp1$mkl$beta[48:94] - latent.srp1$mkl$beta[1:47],
#    g2a$coefficients[48:94] - g2a$coefficients[1:47], use="complete.obs") #.999

#Poisson S-R GLM also close to Stigler:
cor(fit.table2$quasi, g2a$coefficients[1:47] - g2a$coefficients[48:94], 
    use="complete.obs")
plot(fit.table2$quasi, g2a$coefficients[1:47] - g2a$coefficients[48:94])

# 3. Simulate function for quasibinomial GLM | Quasi-Stigler using beta-binomial (used in later chunks) ####

# based on a quasi-binomial glm, return a list of matrices of predicted
simulate.qs <- function(g1, nsim = 10) {
  
  P = g1$fitted.values #or P = inv.logit(x1 %*% c(0, fit$coefficients)) 
  PW = g1$prior.weights
  phi = summary(g1)$dispersion
  
  # set alpha and beta to agree with above mean and var
  mean1 = PW*P
  var1 = phi*PW*P*(1-P)
  B = (1-P)/P
  D = phi*P*(1-P)
  alpha = (PW*B - D*(1+B)^2)/(D*(1 + B)^3 - (B+B^2))
  beta = alpha*(1-P)/P
  mean2 = PW*alpha/(alpha + beta) 
  net.dyads = length(g1$fitted.values)
  var2 = PW*alpha*beta*(alpha + beta +PW)/((alpha + beta)^2 * (alpha + beta +1))
  n = ncol(g1$model$x1)
  k = 1
  x1 = matrix(0, choose(n, 2), n)
  for (i in 1:(n-1)) { #sender
    for (j in (i+1):n) { 
     x1[k,i]=1;
      x1[k,j]=-1;
     k = k + 1
     }
  }
  x1[,1] = 0 #forces first coef to NA, later set -> 0
  
  # sample
  g = matrix(0, net.dyads, nsim)
  mats = list(nsim)
  for (i in 1:nsim) {
    sample1 = rep(0, net.dyads)
    for (j in 1:net.dyads) {
      #if (PW[i]==0) {return(0)}
      if (PW[j]==1) {sample1[j] = rbern(1, P[j])}
      if (PW[j] > 1) {sample1[j] = rbbinom(1, size = PW[j], 
                                           alpha = alpha[j], beta = beta[j])}
    }
    y1 = sample1/g1$model$`(weights)`
    y1[is.nan(y1)]=0
    g = glm(y1~x1-1, family = quasibinomial(link="logit"),
                weights = g1$model$`(weights)`)$fitted.values
    mat = matrix(0, n, n)
    mat[lower.tri(mat)] = (g1$model$`(weights)`)*g
    mat = t(mat)
    mat[lower.tri(mat)] = (g1$model$`(weights)`)*(1-g)
    mats[[i]] = t(mat)
  }
  return(mats)
  
}

@

Uncertainty in the export scores is conveyed through the \textit{quasi-variance}, $qvar_i$, of each $\mu_i$. The quasi-variances are estimated to minimize the difference between the true pairwise variances, $var(\hat{\mu}_i - \hat{\mu}_j)$, and a quasi-variance approximation, $qvar_i+qvar_j$. Although they could convey exact variances, the authors preferred quasi-variances and quasi-standard errors (QSE) because they can be succinctly displayed alongside export scores.

To connect the quasi-Stigler model to the latent space model, consider the quasi-symmetry formulation of the model (see \citeauthor{varinetal} (4), with some notation changes), where the export score is expanded $\mu_i = b_i - a_i$. As in the latent space model, $a_i$ and $b_i$ are sender and receiver coefficients, respectively. If we constrain $E(C_{ij}) + E(C_{ji}) = t_{ij}$, then the expectation term reduces to $ E(C_{ij}) = t_{ij} exp(a_i + b_j).$

% need to check the last statement again?
% Should the quasi-symmetry formulation have the additional constraint explicitly that E(c_ij) + E(c_ji) = T_ij,
% i.e. exp(a_i + b_j) + exp(a_j + b_i) = 1 ?

\subsection{Comparison of journal rankings} \label{Comparison}

We compare the rankings from our latent space model to others discussed. Unless otherwise stated, our results are based on two-dimensional MKL parameter estimates from MCMC estimation. These had the highest posterior probability and graph probability of any two-dimensional estimates considered. In Section \ref{modeleval} we provide a foundation for the choice of model dimension and estimation method. The \proglang {R} code used to implement our model and PageRank is included in the supplementary material. 

%%%%%%%%%%%%%%%%%%%
% CITATION MODELS %
%%%%%%%%%%%%%%%%%%%

%random initialization of sender and receiver didn't make a difference to likelihood
<<r latent_sr0, eval = T, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE, dependson = c("setup", "setup2"), highlight = FALSE>>=

latent.srp0 = ergmm(Cnet ~ rsender + rreceiver,
                    response = "citations", family = "Poisson.log", seed = 30,
                    tofit = c("mcmc", "mkl", "procrustes", "mle"),
                    control = ergmm.control(burnin = 100000, interval = 500,
                                            sample.size = 5000, mle.maxit = 50))
@

<<r latent_sr2, eval = T, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE, dependson = c("setup", "setup2"), highlight = FALSE>>=

t1.srp2 = Sys.time()
latent.srp2 = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
                    response = "citations", family = "Poisson.log", seed = 30,
                    tofit = c("mcmc", "mkl", "procrustes", "mle"),
                    control = ergmm.control(burnin = 500000, interval = 500,
                                            sample.size = 5000, mle.maxit = 100,
                                            pilot.runs = 10))
t2.srp2 = Sys.time()
# The \code{euclidean(d=2)} argument indicates that the latent positions are two-dimensional.
# reponse argument conveys a valued network 
# checked mcmc.diagnostics and they look fine
# mcmc.diagnostics(latent.srp2)
# Raftery Lewis diagnsotic suggests 10000 burnin, but I increased it because acceptance was low and there seemed to be a transition after the burnin period
# Far fewer than the default mle.maxit = 100 is needed

@

<< latent_sr2_init_r, eval = T, results = 'hide', echo = F, cache = T, cache.comments = FALSE, warning= FALSE>>=

#random initation of sender and receiver, and scaling Z helps the model fit
D = 2; N = nrow(Cmatrix)
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

t1.init.r = Sys.time()
latent.srp2.init.r = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 500000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        pilot.runs = 10),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
t2.init.r = Sys.time()
@

% PageRank does do something that other methods i talk about don't, which is weight citations by importance of the citing journal.

<< pagerank, eval = T, cache = T, echo = F, results = 'hide',dependson = c("setup", "setup_jrss", "latent_sr2"), fig.keep = 'none', message = F, warning = F>>=

# "Page rank is the best-known technique for link-based importance ranking"
# https://www.cs.umd.edu/class/spring2008/cmsc828g/Slides/node-ranking.pdf

# PageRank has been applied to citation data:
# http://arxiv.org/pdf/0901.2640.pdf <- prob most useful, good summary
# http://arxiv.org/pdf/1012.4872.pdf (damping factor)
# http://onlinelibrary.wiley.com/doi/10.1002/asi.21452/epdf (weighted page rank)

# apply page rank ####
Cgraph = igraph::graph.adjacency(t(Cmatrix), weighted=T) #transpose so that i,j indicates citation from i to j.
#get.edge.attribute(Cgraph, "weight")
Cgraph.page.rank = igraph::page.rank(Cgraph, damping = .95)$vector #"If weights arg is NULL and the graph has a weight edge attribute then that is used."
# note, EXTREMELY fast
#       page rank normalizes so doing that wouldn't change results

# compare results to other ranks ####

## to cited/citing ratio
plot(Cgraph.page.rank, cite.ratio)
cor(Cgraph.page.rank, cite.ratio) #.68

## to quasi-stigler

### scores
plot(Cgraph.page.rank, fit.table2$quasi, type = "n")
text(labels = names(Cgraph.page.rank), x = Cgraph.page.rank, y = fit.table2$quasi, cex = .5, xlab = "PageRank", ylab = "quasi-Stigler", srt = -45)
cor(Cgraph.page.rank, fit.table2$quasi) #.659 with damping at .95, and .65 with damping at .85

### ranks
temp = data.frame(row.names = names(Cgraph.page.rank))
temp[,1] = rank(Cgraph.page.rank)
temp[,2] = rank(fit.table2$quasi)
plot(temp, type = "n", xlab = "PageRank", ylab = "quasi-Stigler")
text(labels = rownames(temp), x = temp[,1], y = temp[,2], cex = .5)

### raw counts
cor(Cgraph.page.rank, colSums(t(Cmatrix))) #.985 - correlates very highly with number of  citations received
plot(Cgraph.page.rank, colSums(t(Cmatrix)))
#plot(vc2, colSums(t(Cmatrix))) how would latent space compare?
@

<<r Eigenfactor, eval = T, echo = F, cache = T, results = 'hide', fig.keep = 'none', dependson = c("latent_srp2")>>=

# http://www.eigenfactor.org/projects/journalRank/rankings.php?search=XY&year=2010&searchby=isicat&orderby=Eigenfactor
# 2010, going back five years

EF = c(0.004,0.003,0.035,0.002,0.008,0.005,0.02,0.018,0.012,0.004,0.003,0.006,0.002,0.022,0.002,0.003,0.002,0.002,0.04,0.002,0.004,0.007,0.011,0.002,0.007,0.021,0.004,0.003,0.017,0.007,0.004,0.002,0.002,0.006,0.006,0.006,0.002,0.038,0.005,0.001,0.001,0.002,0.011,0.008,0.007,0.006,0.003)

AI = c(.9, .7, 3.3, .6, 1.6, .8, 1.6, 2.4, 2.3, 1.2, .3, .3, .5, .8, .9, .6, .6, .7, 3.3, .3, .6, 1.6, .9, .5, 1.8, 4.8, 1.0, .4, .6, 1.7, .9, .9, .5, 1.4, 2.0, 1.8, .5, 1.3, 1.5, .8, .5, .4, .4, 3.4, 1.0, 1.4, 1.2)

names(EF) =  names(AI) = c('AmS','AISM','AoS','ANZS','Bern','BioJ','Bcs','Bka','Biost','CJS','CSSC','CSTM','CmpSt','CSDA','EES','Envr','ISR','JABES','JASA','JAS','JBS','JCGS','JMA','JNS','JRSS.A','JRSS.B','JRSS.C','JSCS','JSPI','JSS','JTSA','LDA','Mtka','SJS','StataJ','StCmp','Stats','StMed','SMMR','StMod','StNee','StPap','SPL','StSci','StSin','Tech','Test')

par(mfrow = c(2,2))
#vs latent 
plot(fit.table2$quasi, EF, main = as.character(round( cor(fit.table2$quasi, EF), 3)), type ="n")
text(names(EF), x = fit.table2$quasi, y = EF, cex = .6) 
#the fact that StMed scores very highly on eigenfactor is a knock against it. StMed is middle of the road in cited/citing ratio and latent score and pagerank
plot(fit.table2$quasi, AI, main = as.character(round( cor(fit.table2$quasi, AI), 3)), type ="n")
text(names(AI), x = fit.table2$quasi, y = AI, cex = .6) 

#vs page rank
#PageRank does not directly penalize out-links, even though journals which cite much more often than they are cited are less likely to be highly regarded. 

plot(Cgraph.page.rank, EF, main = as.character(round( cor(Cgraph.page.rank, EF), 3)), type ="n")
text(names(EF), x = Cgraph.page.rank, y = EF, cex = .6) 

plot(Cgraph.page.rank, AI, main = as.character(round( cor(Cgraph.page.rank, AI), 3)), type ="n")
text(names(AI), x = Cgraph.page.rank, y = AI, cex = .6) 

# compared to latent and colored by cluster
#par(mfrow = c(1,1))
#plot(vc2, AI, main = as.character(round( cor(vc2, AI), 3)), type ="n")
#text(names(AI), x = vc2, y = AI, cex = .9, col = cutree(journals.cluster, h = 0.6)+3) #shows AI seems to place "general" and "computational" publications lower as opposed to latent method (because light blue color at the bottom)

# Eigenfactor correlattion to ratio of in to out citations
cor(EF, colSums(t(Cmatrix))/rowSums(t(Cmatrix)))
@

<<r IF, eval = T, echo = F, cache = T, results = 'hide', fig.keep = 'none', warning = F, message = F >>=

#This is taken from Table 4 of varinetal, via their journal-scores.csv file in their supplement folder. BUT not all the names match up so I made the journal-scores_edited file. 

IF1 = c(0.981,
0.966,
2.94,
0.618,
1,
1.438,
1.764,
1.833,
2.769,
0.689,
NA,
0.351,
0.5,
1.089,
NA,
0.75,
0.86,
0.722,
2.063,
0.306,
1.073,
1.206,
1.01,
NA,
2.57,
3.5,
0.645,
0.469,
0.691,
NA,
0.678,
0.873,
0.584,
0.835,
NA,
1.851,
0.519,
2.328,
1.768,
0.714,
0.322,
NA,
0.443,
2.48,
0.956,
1.56,
NA)

#Same as above, but with 2010 missing values supplied from https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
IF2 = c(0.981,
0.966,
2.94,
0.618,
1,
1.438,
1.764,
1.833,
2.769,
0.689,
0.343, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.351,
0.5,
1.089,
1.645, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.75,
0.86,
0.722,
2.063,
0.306,
1.073,
1.206,
1.01,
0.455, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
2.57,
3.5,
0.645,
0.469,
0.691,
2.647, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.678,
0.873,
0.584,
0.835,
2.0, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
1.851,
0.519,
2.328,
1.768,
0.714,
0.322,
0.595, #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?
0.443,
2.48,
0.956,
1.56,
1.036) #https://jcr.incites.thomsonreuters.com/JCRJournalHomeAction.action?

#From varinetal Table4. Filling in missing values would require JCR login
IFno = c(0.752,
0.966,
2.573,
0.582,
0.964,
1.278,
1.601,
1.686,
2.615,
0.676,
NA,
0.311,
0.5,
0.815,
NA,
0.707,
0.8,
0.667,
1.929,
0.281,
0.76,
1.137,
0.816,
NA,
2.354,
3.427,
0.566,
0.429,
0.594,
NA,
0.632,
0.836,
0.573,
0.813,
NA,
1.743,
0.519,
2.072,
1.725,
0.686,
0.305,
NA,
0.356,
2.08,
0.889,
1.387,
NA)

IF2.rank = rank(-IF2, na.last = "keep")
IFno.rank = rank(-IFno, na.last = "keep")
@

<<r HITS, eval = F, echo = F, cache = F>>=
library(igraph)

# http://stackoverflow.com/questions/29911300/how-to-get-the-hits-function-in-r-tool
HITS<-function(g,k)  { 
    adj <- g
    nodes <- dim(adj)[1] 
    auth <- c(rep(1,nodes)) 
    hub <- c(rep(1,nodes)) 
    for(i in 1:k){ 
        t_adj <- t(adj) 
        auth <- t_adj%*%hub 
        hub <- adj%*%auth 
        sum_sq_auth <- sum(auth*auth) 
        sum_sq_hub <- sum(hub*hub) 
        auth <- auth/sqrt(sum_sq_auth) 
        hub <- hub/sqrt(sum_sq_hub) 
    } 
    result <- data.frame(auth = auth,hub = hub)   
    return(result) 
}

hits <- HITS(Cmatrix, 100)
par(mfrow = c(2,2))
cor(vc2, hits$auth) #not similar
cor(vc2, hits$hub) #a bit similar
plot(vc2, hits$hub)
cor(Cgraph.page.rank, hits$auth) #a bit simlar
plot(Cgraph.page.rank, hits$auth) 
cor(Cgraph.page.rank, hits$hub) #very similar
plot(Cgraph.page.rank, hits$hub)
cor(Cgraph.page.rank, hits$auth+hits$hub) 
plot(Cgraph.page.rank, hits$auth+hits$hub) 
@

<<r summarytable, eval = T, cache = T, echo = F, results = 'asis', dependson = c("setup", "setup_jrss", "latent_sr2", "pagerank", "Eigenfactor", "IF"), warning = F, message = F, fig.width = 6 >>=

# varintable <- read.csv("/Users/jac/Documents/citation/Varin_Cattelan_Firth_supplement/Data/journal-scores.csv")
# can't use this table directly it includes too many journals

summarytable_rate = round(data.frame("Latent Space" = latent.srp2$mkl$receiver -
                                       latent.srp2$mkl$sender,
                          "quasi-Stigler"= fit.table2$quasi,
                          "PageRank" = Cgraph.page.rank,
                          "Eigenfactor" = EF,
                          "Article Inf." = AI,
                          "Impact Fac." = IF2),3)

summarytable_rank = data.frame(
                          "Latent Space" = order(order(latent.srp2.init.r$mkl$receiver -
                                        latent.srp2.init.r$mkl$sender, decreasing = T)),
                          "quasi-Stigler"= order(order(fit.table2$quasi, decreasing = T)),
                          "PageRank" = order(order(Cgraph.page.rank, decreasing = T)),
                          "Eigenfactor" = order(order(EF, decreasing = T)),
                          "Article Inf." = order(order(AI, decreasing = T)),
                          "Impact Fac." = IF2.rank)

rownames(summarytable_rank) = rownames(summarytable_rate) 

library(xtable)
summary_xtable_rank <- xtable(summarytable_rank, caption = "Comparison of journal rankings. The ``big four'' have gray background.", digits = 0, label = "summarytable_rank")
   
summary_xtable_rate <-xtable(round(cor(summarytable_rate),2), caption = "Correlation of journal rating methods", label = "correlation_table")

col1 <- rep("\\rowcolor[gray]{0.9}", 4)
print.xtable(summary_xtable_rank, booktabs = TRUE,
             table.placement = getOption("xtable.table.placement", "!"),
             add.to.row = list(pos = as.list(c(2, 7, 25, 18)), command = col1),
             sanitize.colnames.function=function(x)gsub("\\."," ",x),
             size="\\fontsize{9pt}{10pt}\\selectfont",
             caption.placement = "top")


print.xtable(summary_xtable_rate, sanitize.colnames.function=function(x)gsub("\\."," ",x), sanitize.rownames.function=function(x)gsub("\\."," ",x), caption.placement = "top", size="\\fontsize{9pt}{10pt}\\selectfont",
             align=c('p{.5in}',rep('|c', 7)))   
@

Table \ref{summarytable_rank} compares journal rankings from the latent space model, quasi-Stigler model, PageRank, Eigenfactor, Article Influence, and Impact Factor.
%Explain how I fit all these/whether the data's from. Include code for latent space model?
Comparisons to other versions of the Impact Factor can be found in Table 4 of \citeauthor{varinetal} for a slightly different data set. Table \ref{summarytable_rank} presents ranks rather than ratings to facilitate comparisons across the methods. According to \citet{varinetal}, there is ``diffuse opinion'' among statisticians that the most prestigious statistics journals are, in alphabetical order, \textit{Annals of Statistics} (AoS), \textit{Biometrika} (Bka), the \textit{Journal of the American Statistical Association} (JASA) and the \textit{Journal of the Royal Statistical Society}, Series B (JRSS-B). (These journals have gray background in Table \ref{summarytable_rank}.) Accordingly, they argue that a good rating method will put them near the top.

Although PageRank ranks the ``big four'' journals highest, it places \textit{Statistics in Medicine} (StMed), \textit{Journal of Statistical Planning and Inference} (JSPI), and \textit{Computational Statistics and Data Analysis} (CSDA) in positions six through eight, much higher than most other methods. These journals have the three highest out-citation counts in our data, and are among the most prolific citers of the top four journals. However, their ratios of in- to out-citations rank 20th, 30th, and 35th. Eigenfactor behaves similarly to PageRank, with some differences reflecting its use of normalized data. (Eigenfactor is strongly correlated with PageRank, $0.91$, as shown in Table \ref{correlation_table}.) We conclude that PageRank and Eigenfactor are better measures of centrality or activity level than importance, influence or prestige. 

The Impact Factor rankings show the detriment of averaging per article and not controlling for out-citations or a citation's field of origin. For example, \textit{Environmental and Ecological Statistics} (EES) ranks 14th and the \textit{Journal of Statistical Software} (JSS) ranks 4th, though they have the second- and ninth-lowest in-citation counts in our data. On the other hand, the ``big four'' journals Bka and JASA are ranked 11th and 8th. The Impact Factor ratings are most strongly correlated with Article Influence ($0.87$, see Table \ref{correlation_table}), which is also normalized by articles per journal and calculated using all citations, not just ones from the $47$ journals in our data set.  %I checked and pagerank calculated on normalized data and just the 47 jounals is pretty similar to Eigenfactor.
The two methods share some anomalous rankings, such as \textit{Statistical Science} placing 2nd and 6th respectively. We argue that the high rank for this journal is not reflective of its importance within the field. As a review journal it is more likely to disseminate than publish cutting-edge research. It owes its high rank to connections to top journals (confirmed by PageRank and Eigenfactor ranking it 13th), citations from outside statistics, and a relatively low number of articles published. Its ratio of in- to out-citations ranks 24th in the network. The visualizations provided by the latent space model help to further explain the position of \textit{Statistical Science}, which will be revisited below.
%The journal's founding editor stated that a central goal of the journal is ``presenting the full range of contemporary statistical thought at a modest technical level accessible to the wide community of practitioners, teachers, researchers and students of statistics and probability'' \citep{degroot86}.

The latent space and quasi-Stigler rankings are very similar (correlation $0.99$, see Table \ref{correlation_table}) and seem to provide the best measures of influence or importance. They rank the ``big four'' journals in the top four positions, the \textit{Journal of Statistical Software} 42nd, and \textit{Statistical Science} 19th and 18th respectively. On the other hand, the methodological journal \textit{Scandinavian Journal of Statistics} (SJS) is highly ranked by both, at 6th and 8th respectively. SJS states its mission as ``reporting significant and innovative original contributions to statistical methodology, both theory and applications'' \citep{SJS}. \citet{varinetal} use data from the UK Research Assessment Exercise (RAE), a periodic evaluation of UK university departments, as an external check on the quasi-Stigler rankings. They find some evidence that the quasi-Stigler model provides stronger correlation to RAE assessment of research quality than other methods. Details of that comparison, and its many caveats, are found in Section 6 of \citet{varinetal}.

\subsubsection{Comparison of latent space and quasi-Stigler model output}\label{Comparison1}

As the quasi-Stigler and latent space models emerge as the best suited to our ranking priorities, we compare their results most closely. Figure \ref{fig:r latent2_stigler} (left) plots the compared ranks, with lighter labels for larger differences in rank. Three is the highest observed difference. However, the underlying differences in scores are very small, as shown in the right panel of Figure \ref{fig:r latent2_stigler}. Figure \ref{fig:r latent2_joy} plots the posterior distributions of latent space scores. It confirms that the small differences in rank shown in Figure \ref{fig:r latent2_stigler} are not significant given the uncertainty in the estimated scores, highlighting the importance of capturing model uncertainty.

<<r latent2_stigler, eval = T, echo = F, cache = T, warning = F, message = F, fig.height = 3, fig.width = 6, fig.align = "center", results = 'hide', dependson= c("setup", "latent_sr2"), fig.cap = 'Left: Latent space vs. quasi-Stigler rankings. Better-ranked journals are at the top right, corresponding to higher numbers. Right: Comparison of scores rather than rankings. Lighter labels means larger differences. Maximum observed rank difference is 3.'>>=

par(mfrow = c(1,2))
par(mai = c(.8,.8,.8,.4))
# compare ranks ####

srs2 = data.frame(quasi.Stigler = fit.table2$quasi,
             latent_space = latent.srp2.init.r$mkl$receiver -latent.srp2.init.r$mkl$sender)
rownames(srs2) = Cnet%v%"vertex.names"
srs2b = srs2
srs2b[,1] = rank(srs2b[,1])
srs2b[,2] = rank(srs2b[,2])
srdiff = srs2b[,1]-srs2b[,2]; #latent space - stigler.

## plot rank comparison ####
cr = grDevices::colorRampPalette(c("black", "gray"))( 4 )
col1 = rep(cr[1], 47)
col1[abs(srs2b[,1]-srs2b[,2]) >.5 ] = cr[2]
col1[abs(srs2b[,1]-srs2b[,2]) >1.5 ] = cr[3]
col1[abs(srs2b[,1]-srs2b[,2]) >2.5 ] = cr[4]

col1 = abs(srs2b[,1]-srs2b[,2])
rc = ggplot(data = srs2b, aes(x = quasi.Stigler, y = latent_space,
                         label = rownames(srs2b), col = col1)) +
    geom_text(angle = -45, size = 2.5) + 
    scale_color_gradient(low = "black", high = "gray", guide = F) + 
    theme_bw() +
    ylab("2D Latent Space") +
    ggtitle(label = "Rank comparison") +
    theme(plot.title = element_text(size = 12))

sc = ggplot(data = srs2, aes(x = quasi.Stigler, y = latent_space,
                         label = rownames(srs2b), col = col1)) +
    scale_color_gradient(low = "black", high = "gray", guide = F) + 
    geom_point() +
    theme_bw() +
    ylab("2D Latent Space") +
    ggtitle(label = "Score comparison") +
    theme(plot.title = element_text(size = 12))

cowplot::plot_grid(rc, sc)
@

<<r latent2_joy, eval = T, echo = F, cache = T, warning = F, message = F, fig.pos='t!', fig.height = 6, fig.width = 6, fig.align = "center", fig.cap = 'Posterior distributions of latent space score. Most small differences in ratings are not signficiant.'>>=

library(ggplot2)
library(ggjoy)

# scores:
latent2 = latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender

# Posterior distribution of scores:
quantiles2 = apply(latent.srp2.init.r$sample$receiver - latent.srp2.init.r$sample$sender,
              2, quantile, seq(0,1,length.out = 101))
o = order(quantiles2[51,])
quantiles2 = quantiles2[,o]
tmp = data.frame(score = as.vector(quantiles2), node = as.factor(rep(1:47, each = 101)))

# Plot
ggplot(tmp, aes(x = score, y = node)) + geom_joy() + 
  ggtitle("Posterior distributions of latent space scores") +
  theme_bw() + 
  theme(plot.title = element_text(size = 12),
        axis.text.y = element_text(size = 7)) +
  scale_y_discrete(name ="journal & point estimate",
                   labels= apply(cbind(rownames(Cmatrix)[o], sort(round(latent2, 2))),
                                 1, paste, collapse = "  "))

@

Uncertainty in ratings is very similar between the two models, with quasi-Stigler standard deviations being 0.04 smaller on average, and at most 0.069 smaller. Under both models, \textit{Stata Journal} (StataJ) has the largest standard error, which is due to the fact that it has by far the fewest in- and out-citations. As expected, standard error of scores and citation counts are inversely correlated. Figure \ref{fig:r latent2_centipede} shows point estimates from each model inside intervals of $1.96$ times the standard error in either direction. On the left, the variances of the latent space scores are calculated from a sample of 5000 draws from the posterior distribution of parameters stored during MCMC estimation. On the right, the longer intervals are calculated from standard errors extracted from the scaled covariance matrix of the model. The interval for \textit{American Statistician} (AmS) is missing because its coefficients were fixed at zero for identifiability. The interior intervals are 1.96 quasi-standard errors (QSE) in each direction. These ``comparison intervals" are analogous to those in Figure 4 of \citet{varinetal}. We see that they are smaller and more variable than the true standard error intervals. The justification by \citeauthor{varinetal} to present uncertainty through quasi-standard errors is that they can be listed alongside estimates in a table, and allow a familiar Pythagorean estimate of the standard error of a difference of two export scores.  However, in a centipede plot as shown, the true standard errors are just as compact and easy to compare, and reveal that the uncertainty in scores is roughly equivalent between the two models. 

<<latent_binomial, eval = F, echo = F, cache = T, warning = F, results = 'hide'>>=

# the pi's fit by quasi-stigler are as good as when latent component added
# the fit z's for the binomial are very close to 0 
# one way we can interpret this as: total citations exchanged
# between two journals depends on positions, but given that total, 
# the specific taken by each can be modeled with just sender/receiver

llik2 <- function(theta, Y = t(Cmatrix), d = 2, n=47, family = "poisson", est = "MAP") {
    a = theta[1:n]
    b = theta[(n+1):(2*n)]
    B = theta[(2*n+1)]
    Z = matrix(theta[(2*n+2):(2*n+1+d*n)], ncol = d, nrow = n)
    sigma2_a = theta[2*n+2+d*n]
    sigma2_b = theta[(2*n+3+d*n)]
    sigma2_z = theta[(2*n+4+d*n)]
    return(llik(Y=Y, sender = a, receiver = b, beta = B, Z = Z,
                sender.var = sigma2_a, receiver.var =sigma2_b,
                Z.var = sigma2_z, family = family, est = est))
  }

theta = optim(c(a, b, B, Z),
                   Y = t(Cmatrix), d = 2, family = "binomial", llik2,
                   method = "BFGS", est = "Y", lower = -Inf,
                   control=list(maxit = 1000, fnscale = -1))
n = nrow(Y)
a = theta$par[1:n]
b = theta$par[(n+1):(2*n)]
B = theta$par[(2*n+1)]
Z = matrix(theta$par[(2*n+2):(2*n+1+d*n)], ncol = d, nrow = n)
Z_dist = dist2(Z)
l_lambda = t(b + t(a - Z_dist)) + B
#binomial
lambda = inv.logit(l_lambda); diag(lambda) = 0
res = Tmatrix * lambda - Y; diag(res) = NA 
#plot
plot(sort(as.vector(res)), type = "l", lwd = 2)
points(sort(as.vector(g1.res)), col = "blue", type = "l", lty = 2, lwd = 2)

# [removed] poisson.c
#lambda = exp(l_lambda); diag(lambda) = 0
#Tmatrix = Y + t(Y); diag(Tmatrix) = 0
#lambda[lambda > Tmatrix] = Tmatrix[lambda > Tmatrix]
#lambda[lower.tri(lambda)] = (Tmatrix - t(lambda))[lower.tri(lambda)]
#res = (lambda - Y); diag(res) = NA 

@

The near identical rankings from these two models belie the fact that the quasi-Stigler model is conditioned on dyad totals. In contrast, the positions in the latent space model help to explain those totals. The distances resulting from the positions exert the only dyad-specific symmetric effect in the model. (We fit a binomial two-dimensional latent space model conditioned on dyad totals and found the estimated positions to be very close to zero.) %using optim, see latent_binomial chunk
A major advantage of the latent space model is that the positions can be visualized and help us to better understand the ratings.

<<r centipede_function, eval = T, cache = T, echo = F>>=
centipede.plot <- function (segs, mct = "mean", lower.limit = "std.error", upper.limit = lower.limit, 
    left.labels = NULL, right.labels = NULL, sort.segs = TRUE, 
    main = "", xlab = NA, pch = 21, vgrid = NA, hgrid = NA, gridcol = "lightgray", 
    left.labels.col = 1, right.labels.col = 1,
    mar = NA, col = par("fg"), bg = "green", cex = NULL, ...) 
{
    if (missing(segs)) {
        cat("Usage: centipede.plot(segs,...)\n\twhere segs is a dstat object")
        stop("or a matrix of midpoints and limits")
    }
    if (is.list(segs)) {
        if (all(lapply(segs, is.numeric))) 
            segs <- get.segs(segs, mct = mct, lower.limit = lower.limit, 
                upper.limit = upper.limit)
        else stop("If segs is a list, all the components must be numeric")
    }
    if (class(segs) == "dstat") {
        midpoint <- "mean"
        if (lower.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ] * segs[2, ]
        }
        if (upper.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ] * segs[2, ]
        }
        if (lower.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]
        }
        if (upper.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]
        }
        if (lower.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]/sqrt(segs[3, ])
        }
        if (upper.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]/sqrt(segs[3, ])
        }
        segs <- rbind(segs[1, ], ll, ul, segs[3, ])
    }
    segdim <- dim(segs)
    if (sort.segs) {
        seg.order <- order(segs[1, ])
        segs <- segs[, seg.order]
    }
    else seg.order <- 1:segdim[2]
    oldpar <- par("mar")
    #if (is.na(mar[1])) 
    #    mar <- c(4, 6, 1 + 2 * (nchar(main) > 0), 5)
    #par(mar = mar)
    plot(x = c(min(segs[2, ]), max(segs[3, ])), y = c(1, segdim[2]), 
        main = main, xlab = "", ylab = "", type = "n", axes = FALSE, 
        ...)
    box()
    if (!is.na(vgrid[1])) 
        abline(v = vgrid, lty = 1, col = gridcol)
    if (is.null(hgrid)) 
        abline(h = 1:segdim[2], lty = 2, col = gridcol)
    else if (!is.na(hgrid[1])) 
        abline(h = hgrid, lty = 2, col = gridcol)
    axis(1, cex.axis = cex)
    arrows(segs[2, ], 1:segdim[2], segs[3, ], 1:segdim[2], length = 0.05, 
        angle = 90, code = 3, col = col)
    points(segs[1, ], 1:segdim[2], pch = pch, col = col, bg = bg)
    if (is.null(left.labels)) {
        left.labels <- colnames(segs)
        if (is.null(left.labels)) 
            left.labels <- paste("V", seg.order, sep = "")
    }
    else left.labels <- left.labels[seg.order]
    plot.limits <- par("usr")
    mtext(left.labels, 2, line = 0.2, at = 1:segdim[2], adj = 1, 
        las = 1, cex = cex, col = left.labels.col)
    #if (is.null(right.labels)) 
    #    right.labels <- paste(round(segs[1, ], 2), "(", segs[4, 
    #        ], ")", sep = "")
    #else 
    right.labels <- right.labels[seg.order]
    mtext(right.labels, 4, line = 0.2, at = 1:segdim[2], adj = 0, 
        las = 1, cex = cex, col = right.labels.col)
    #if (is.na(xlab)) 
    #    xlab <- paste("| -", rownames(segs)[2], "-", rownames(segs)[1], 
    #        "-", rownames(segs)[3], "- |")
    if (!is.na(xlab)) 
        mtext(xlab, 1, line = 2)
    par(oldpar)
    invisible(segs)
}
@

<<r latent2_centipede, eval = T, echo = F, cache = T, warning = F, message = F, dependson= c("setup", "latent_sr2", "setup_jrss", "latent2_stigler"), results = "hide", fig.height = 5.5, fig.width = 6, fig.align = "center", fig.pos = "H", fig.cap = "Visualizing uncertainty in latent space scores (left) and quasi-Stigler scores (right). The error bars are +/- 1.96*SE in each direction, and inner intervals on the right are ``comparison intervals'' equal to 1.96$*$QSE in each direction. Due to quasi-Stigler model constraints, AMS only has an estimated QSE.">>=

# from plotrix package, but made some changes ####
# added cex arg and commented out something with segs[4, and x axis label

centipede.plot <- function (segs, mct = "mean", lower.limit = "std.error", upper.limit = lower.limit, 
    left.labels = NULL, right.labels = NULL, sort.segs = TRUE, 
    main = "", xlab = NA, pch = 21, vgrid = NA, hgrid = NA, gridcol = "lightgray", 
    mar = NA, col = par("fg"), bg = "green", cex = NULL, ...) 
{
    if (missing(segs)) {
        cat("Usage: centipede.plot(segs,...)\n\twhere segs is a dstat object")
        stop("or a matrix of midpoints and limits")
    }
    if (is.list(segs)) {
        if (all(lapply(segs, is.numeric))) 
            segs <- get.segs(segs, mct = mct, lower.limit = lower.limit, 
                upper.limit = upper.limit)
        else stop("If segs is a list, all the components must be numeric")
    }
    if (class(segs) == "dstat") {
        midpoint <- "mean"
        if (lower.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ] * segs[2, ]
        }
        if (upper.limit == "var") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + segs[2, ]
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ] * segs[2, ]
        }
        if (lower.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]
        }
        if (upper.limit == "sd") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]
        }
        if (lower.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ll <- segs[1, ] - sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ll <- segs[1, ] - segs[2, ]/sqrt(segs[3, ])
        }
        if (upper.limit == "std.error") {
            if (rownames(segs)[2] == "var") 
                ul <- segs[1, ] + sqrt(segs[2, ])/sqrt(segs[3, 
                  ])
            if (rownames(segs)[2] == "sd") 
                ul <- segs[1, ] + segs[2, ]/sqrt(segs[3, ])
        }
        segs <- rbind(segs[1, ], ll, ul, segs[3, ])
    }
    segdim <- dim(segs)
    if (sort.segs) {
        seg.order <- order(segs[1, ])
        segs <- segs[, seg.order]
    }
    else seg.order <- 1:segdim[2]
    oldpar <- par("mar")
    #if (is.na(mar[1])) 
    #    mar <- c(4, 6, 1 + 2 * (nchar(main) > 0), 5)
    #par(mar = mar)
    plot(x = c(min(segs[2, ]), max(segs[3, ])), y = c(1, segdim[2]), 
        main = main, xlab = "", ylab = "", type = "n", axes = FALSE, 
        ...)
    box()
    if (!is.na(vgrid[1])) 
        abline(v = vgrid, lty = 1, col = gridcol)
    if (is.null(hgrid)) 
        abline(h = 1:segdim[2], lty = 2, col = gridcol)
    else if (!is.na(hgrid[1])) 
        abline(h = hgrid, lty = 2, col = gridcol)
    axis(1, cex.axis = cex)
    arrows(segs[2, ], 1:segdim[2], segs[3, ], 1:segdim[2], length = 0.05, 
        angle = 90, code = 3, col = col)
    points(segs[1, ], 1:segdim[2], pch = pch, col = col, bg = bg)
    if (is.null(left.labels)) {
        left.labels <- colnames(segs)
        if (is.null(left.labels)) 
            left.labels <- paste("V", seg.order, sep = "")
    }
    else left.labels <- left.labels[seg.order]
    plot.limits <- par("usr")
    mtext(left.labels, 2, line = 0.2, at = 1:segdim[2], adj = 1, 
        las = 1, cex = cex)
    #if (is.null(right.labels)) 
    #    right.labels <- paste(round(segs[1, ], 2), "(", segs[4, 
    #        ], ")", sep = "")
    #else 
    right.labels <- right.labels[seg.order]
    mtext(right.labels, 4, line = 0.2, at = 1:segdim[2], adj = 0, 
        las = 1, cex = cex)
    #if (is.na(xlab)) 
    #    xlab <- paste("| -", rownames(segs)[2], "-", rownames(segs)[1], 
    #        "-", rownames(segs)[3], "- |")
    if (!is.na(xlab)) 
        mtext(xlab, 1, line = 2)
    par(oldpar)
    invisible(segs)
}

# 2-D latent space variance by sample ####
model = latent.srp2.init.r
sd2 = apply(model$sample$receiver - model$sample$sender, 2, sd)
latent2 = model$mkl$receiver - model$mkl$sender
centipede1 = t(cbind(latent2, latent2 - 1.96*sd2, latent2 + 1.96*sd2))
colnames(centipede1) = Cnet%v%"vertex.names"

# QSE using glm output (account for overdispersion) ####
shift = mean(g1$coefficients)
v = c(0, diag(vcov(g1))) #vcov(g1) = summary(g1)$cov.scaled = summary(g1)$cov.unscaled*phi 
centipede.qse = (cbind(fit.table2$quasi,
                        fit.table2$quasi - 1.96*fit.table2$qse,
                        fit.table2$quasi + 1.96*fit.table2$qse)) 

centipede.se = (cbind(g1$coefficients - shift,
                       g1$coefficients - shift - 1.96*sqrt(v), 
                       g1$coefficients - shift + 1.96*sqrt(v)))
rownames(centipede.se) = rownames(fit.table2)

#plot ####
par(mfrow = c(1,2))
par(mai = c(0.8, 0.6, .1, 0.6))

centipede.plot(centipede1, vgrid = c(-1,0,1,2), cex = .6, right.labels = round(latent2, 2),
               main = NULL,
               bg = "red", xlab = "2-D Latent Space")

centipede.plot(t(centipede.se), vgrid = c(-1,0,1,2), cex = .6, 
               right.labels = round(fit.table2$quasi, 2), 
               left.labels = rownames(fit.table2), 
               bg = c("blue"), col = c("blue"),
               main = NULL,
               xlab = "Quasi-Stigler SE and QSE Intervals",
               xlim = c(-2, 2.5))
par(new=TRUE)
centipede.plot(t(centipede.qse), cex = .6,
               bg = c("blue"), col = c("cyan"),
               left.labels = " ",
               right.labels = " ",
               xlim = c(-2, 2.5))
legend("bottomright", legend = c("SE", "QSE"), col = c("blue", "cyan"), pch = 15)

# summary of ses
mean(sd2); range(sd2); range(sd2[-35])
summary(sqrt(v)[v>0])
summary(sd2)
@

<<r QS_sim_CI, eval = F, cache = T, echo = F, results = 'hide', warning = F>>=

# "True SE vs. Simulated 95% Intervals"
# No real discrepancy between simulated and exact, but the sampling method may be useful later, e.g. for joy plot

# Simulated CI Method ####

   # to get a predicution under the model we 
   # we need to draw from 47^2 - 47 (for the diag)
   # independent quasi-binomials with P, N as below
   # variance is d * N * P * (1-P)
   #
   # To simulate from a closely related distribution with desired mean and variance, we use the beta-binomial with appropriately set shape parameters. Results are very similar to using true SE, as shown below.

   # We generate data with the prescribed mean and overdispered variance, and repeatedly fit the export scores (for which we may use the `glm` function in the `stats` package or the `BTm` function in te `BradleyTerry2` package used by @varinetal, since the original formulaton of the Quasi-Stigler models citation counts as overdispersed binomial distributions.
   # $$E(C_{ij}) = t_{ij}\pi_{ij}),$$
   # $$var(C_{ij}) =  \phi t_{ij}\pi_{ij}(1 - \pi_{ij}),$$
   # where, given the transposed citation matrix notation in this paper
   # $\pi_{ij} = \frac{exp(u_j - u_i)}{1 + exp(u_j - u_i)} = logit^{-1}(u_j - u_i)$. 
 
# 1 Use beta-binomial - Setup #### 
      # roughly: introduces dependence in pi's which increases variance
      # we can adjust alpha and beta to achieve the QB mean and var

#using the BradleyTerry output instead of glm to ensure comparability
library(boot) #for inv.logit
library("extraDistr") #for rbern
d = phi
P = inv.logit(x1 %*% c(0, fit$coefficients)) 
#or inv.logit(x1 %*% c(0, (g1$coefficients)[2:47]))
#or P = g1$fitted.values
N = g1$prior.weights

# 2 set alpha and beta to agree with above mean and var: ####
mean1 = N*P
var1 = d*N*P*(1-P)
B = (1-P)/P
D = d*P*(1-P)
alpha = (N*B - D*(1+B)^2)/(D*(1 + B)^3 - (B+B^2))
beta = alpha*(1-P)/P
mean2 = N*alpha/(alpha + beta) 
var2 = N*alpha*beta*(alpha + beta +N)/((alpha + beta)^2 * (alpha + beta +1)) #should agree with mean1 for N >= 2
#special cases for N = 0 or 1 !!

#test comparison
#plot(dbinom(0:11, size = 11, prob = P[2]), type = "b")
#points(dbbinom(0:11, size = 11, alpha = alpha[2], beta = beta[2]), col = 2, type = "b")

# 3 simulate ####

nsim = 5000
g = matrix(0, 47, nsim)

for (i in 1:nsim) {
  sample1 = rep(0, length(N))
  for (j in 1:length(N)) {
    #if (N[i]==0) {return(0)}
    if (N[j]==1) {sample1[j] = rbern(1, P[j])}
    if (N[j] > 1) {sample1[j] = rbbinom(1, size = N[j], 
                alpha = alpha[j], beta = beta[j])}
  }
  y1 = sample1/Tmatrix[which(lower.tri(Tmatrix))]
  y1[is.nan(y1)]=0

  #fit model (can use glm or Bradley terry)
  #Ymatrix = matrix(y1, 47, 47, byrow = T)
  #rownames(Ymatrix) = colnames(Ymatrix) = rownames(Cmatrix)
  #Ydata <- countsToBinomial(Ymatrix)
  #g[,i] <- c(0,BTm(outcome = cbind(win1, win2),
  #                  player1 = player1,
  #              player2 = player2,
  #              data = Ydata)$coefficients)
  g[,i] = glm(y1~x1-1,family = quasibinomial(link="logit"), weights =   (Tmatrix)[lower.tri(Tmatrix)])$coefficients
}

g[1,] = 0

# 4 centipede plot ####

#original 
#centipede.plot(t(centipede.qse), vgrid = c(-1,0,1,2), cex = .7, 
#               right.labels = round(fit.table2$quasi, 2),
#               bg = "blue", main = "Quasi-Stigler")

#Beta-Binomial vs. SE
shift = mean(apply(g, 1, mean))
centipede.sim = cbind(apply(g, 1, mean), apply(g, 1, quantile, .025), apply(g, 1, quantile, .975)) - shift

centipede.plot(t(centipede.se), vgrid = c(-1,0,1,2), cex = .7, 
               right.labels = round(centipede.se[,1], 2), 
               left.labels = rownames(fit.table2), 
               bg = c("red"), col = c("red"),
               main = "True SE vs. Simulated 95% Intervals",
               xlim = c(-2, 2.5))
par(new=TRUE)
centipede.plot(t(centipede.sim), cex = .7,
               bg = c("green"), col = c("green"),
               left.labels = " ",
               right.labels = " ",
               xlim = c(-2, 2.5))
legend("bottomright", legend = c("SE", "SIM"), col = c("red", "green"), pch = 16)

@

\subsection{Visualization of latent space journal rankings} \label{Visualization}

<<r latent_sr2_plot, eval = T, cache = T, dependson= c("setup", "setup2", "latent_sr2", "setup_jrss"), echo = F, fig.width = 6, cache = T, warning=F, message=F, fig.cap= 'Estimated journal positions from the two-dimensional latent space model. Top: Point estimates with node size scaled to receiver minus sender coefficient. Bottom: Sample of positions from the model. Colouring is due to the hierarchical clustering of Varin et al. '>>=


# to correspond to visNetwork
library(scales)
alpha2 <- 0.6
color1 <- c("deepskyblue3","gold", "red", "green", "deeppink", "darkorchid4", "orange", "black")
Cnet%v%"color" = color1[cutree(journals.cluster, h = 0.6)]
write.csv(latent.srp2.init.r$mkl$Z, file = "~/Documents/citation/latent_ranking/latent_srp2_init_r_Z.csv")

# plot window
par(mai = c(.1,.1,.1,.1))
#layout(matrix(c(1,1,1,2,2,1,1,1,2,2,1,1,1,2,2), 3, 5, byrow = TRUE), respect = T)
par(mfrow = c(2,1))

#### position plot ####
vc2 <- (latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender)/2+1
col2 <- alpha(color1[cutree(journals.cluster, h = 0.6)], alpha2)
plot(latent.srp2.init.r$mkl$Z, col = col2, cex = vc2, pch = 16,
     bty="n", yaxt="n", xaxt="n")
text(latent.srp2.init.r$mkl$Z, labels = rownames(Cmatrix), cex = .5, pos = 3, offset = .3)
legend("topleft", col = alpha(color1, alpha2), pch = 16, legend=c("review", "general", "theory/method", "appl./health", "computational","eco./envir.", "JSS", "StataJ"), cex=0.7, box.col=0)

####  cloud/uncertainty plot  ####
n = 47
N = 1000
p = latent.srp2.init.r[["sample"]][["Z"]]
s = sample(5000, N)

#null plot:
plot(latent.srp2.init.r$mkl$Z, xlab = NA, ylab = NA, vertex.col=0, edge.col=0, 
           plot.vars=F, suppress.axes=T, bty="n", yaxt="n", xaxt="n", type ="n",
           xlim = c(-4,4), ylim = c(-3,5), main = NA)

#add points
for (i in 1:N) {
  points(p[s[i],,], col = col2, pch = 15, cex = .2)
}

@

Figure \ref{fig:r latent_sr2_plot} (top) shows estimated MKL positions. %shortreed06: heuristically, posterior mean graph is closer to the true than oberved graph due to averaging and use of prior info.
The size of the nodes is scaled to the estimated scores. The coloring and cluster labels for the nodes in both panels are based on the hierarchical clustering of \citet{varinetal}. Although there is no clustering term in our latent space model, the estimated positions are consistent with these clusters. (Some labels are difficult to read, but this is addressed by the dynamic plot referenced below.) The bottom panel of Figure \ref{fig:r latent_sr2_plot} shows a sample of 1000 posterior draws of positions, visualizing the uncertainty. Although individual journal positions are variable, the clusters occupy discernible areas. However, their irregular shapes caution against applying the latent space clustering model of \citet{handcock07}, in which positions given clusters have spherical Gaussian distributions. 
%I did experiment with it and it didn't fit.
The plot shows how they fit together and provides more information than discrete labels. For example the \textit{Journal of Biopharmaceutical Statistics} is most deeply embedded in the applied/health cluster, while \textit{Statistical Science} is on the border. In fact, it should be classified with the review journals it lies near. (We discuss the use of latent space visualization to identify mislabeled classes in Section \ref{Example2}.)  %Broadly speaking, as we move from left to right we go from general journals to specific application areas. The computational journals are towards the top, and the theoretical cluster is the most centrally located. 

The citation network is too dense to display network edges in a static plot. However, using the \pkg{visNetwork} package we render a dynamic plot of the citation network to explore its connections. A version of the plot is included as an html file in the supplementary material, and is available on the author's website (\url{http://www.stat.ucla.edu/~jane.carlen/pages/citation_net.html}). Because of the density of the network, edges in the dynamic visualization are only shown if they account for at least three percent of a journal's out-citations, and their width when highlighted is scaled to that percentage. Rankings based on the latent space model are reported next to the journal titles in the drop-down menu on the left and in the hover text. Coloring of highlighted edges is determined by the cluster of the originating node, the same clusters as in Figure \ref{fig:r latent_sr2_plot}.

<<r latent_sr2_interactive, eval = F, results = 'hide', dependson= c("setup", "latent_sr2", "setup_jrss", "latent2_stigler"), fig.keep='last', echo = F, fig.height=5, fig.width=6, cache = T, warning=F, message=F>>=

# setup ####

library(visNetwork)
library(igraph)
#insertSource("~/Documents/citation/latent_ranking/visIgraph.R",
#             package = "visNetwork", functions = "visIgraph")
#insertSource("~/Documents/citation/latent_ranking/visIgraphLayout.R",
#             package = "visNetwork", functions = "visIgraphLayout")


# data ####
nodes <- data.frame(id = 1:47, 
                    label = Cnet%v%"vertex.names",
                    #title = Cnet%v%"vertex.names", #for selection dropdown
                    title = paste(Cnet%v%"vertex.names", 48 - srs2b[,2]), #for tooltip
                    value = vc2, #conveys node size, on a scale from 0-1
                    #color.highlight.background = "red",
                    group = rep((cutree(journals.cluster, h = 0.6))))


edges <- data.frame(from=data.frame(as.edgelist(Cnet))$X1, 
                    to=data.frame(as.edgelist(Cnet))$X2)
                    #value = Cnet%e%'citations'

cite_igraph <- graph.data.frame(edges, directed=TRUE, vertices=nodes)
Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/latent_srp2_init_r_Z.csv")[,2:3])
Z<- -Z[ ,c(2,1)] #to preserve orientaiton in paper

#fewer edges
# Cmatrix.norm = t(Cmatrix)/citing #entries are % of i's citations to j (column); rows sum to 1; 'citing' is total out for each journal
strength = .03 #only include "strong receivers", e.g. receiver accounts for >.05 = 5% of sender's citations sent [different form the plot in `compare_clustered`]
#interesting plot with strength = .1 (100 edges remain)  
Cstrong = t(Cmatrix)
Cstrong[Cmatrix.norm<strength] = 0
#rowSums(Cstrong>0)
Cnet_strong = as.network(Cstrong, directed=T, matrix.type="a", ignore.eval=F,  names.eval="citations")

edges2 <- data.frame(from=data.frame(as.edgelist(Cnet_strong))$X1, 
                    to=data.frame(as.edgelist(Cnet_strong))$X2,
                    value = as.edgelist(Cnet_strong, as.sna.edgelist = T, attrname = "citations")[,3])

cite_igraph <- igraph::graph.data.frame(edges2, directed=TRUE, vertices=nodes)

# dynamic plot ####

visNetwork(nodes, edges2, main = "Statistics Journals", submain = "latent space positions") %>%
  visIgraphLayout(cite_igraph, layout = "layout.norm",
                  layoutMatrix = cbind(-Z[,2],Z[,1]), type = "full") %>%
  #visOptions(highlightNearest = list(enabled =TRUE, degree = 1)) %>%
  visNodes(#color = list(highlight = list(background = "green")), #not working as expected
           shape = "dot", #shape = "text"
           scaling = list(min = 3, max = 15),
           labelHighlightBold = TRUE,
           font= list(size = 20, color = "black", strokeWidth = 1)) %>%
  visOptions(highlightNearest = list(enabled = TRUE, degree = .5), #hover = TRUE doesn't work?
             nodesIdSelection = FALSE, 
             selectedBy = "title") %>% 
  visEdges(shadow = FALSE,
           #width = 1, to use width turn edge value off
           scaling = list(min = 1, max = 15),
           #selectionWidth = "function(x) {Cnet_strong%e%'citations';}",
           #scaling = list(min = 1, max = 10),
           arrows = list(to = list(enabled = FALSE, scaleFactor = 3),
                    middle = list(enabled = TRUE, scaleFactor = 1)),
           color = list(inherit = "from", highlight = "red", opacity = .03),
           hidden = F,
           smooth = list(type = "curvedCW")) #%>% visLegend()

  #turned these off bc interfering with edge inherit coloring
   # visGroups(groupname = "1", color = list(background = "blue")) %>%
   # visGroups(groupname = "2", color = list(background = "cyan")) %>%
   # visGroups(groupname = "3", color = list(background = "hotpink")) %>%
   # visGroups(groupname = "4", color = list(background = "yellow")) %>%
   # visGroups(groupname = "5", color = list(background = "grey")) %>%
   # visGroups(groupname = "6", color = list(background = "lavendar")) %>%
   # visGroups(groupname = "7", color = list(background = "red")) %>%
   # visGroups(groupname = "8", color = list(background = "green")) %>%

#saved as citation_net_revised.html        
@

Now we revisit the rankings of journals discussed in Section \ref{Comparison}. The ``big four'' journals all draw citations widely from the network but tend to cite journals fairly nearby. In contrast, StMed, JSPI and CSDA, which are ranked highly by PageRank and Eigenfactor but not the latent space model, give and receive citations from a wide range of journals. JSS, which is ranked highly by Impact Factor and Article Influence but 42nd by the latent space model, only accounts for significant out-citations from StataJ. \textit{Statistical Science} does draw citations from two top-four journals, but not at the rate of the highest-ranked journals. Its in-citations have roughly the same span as its out-citations, but generally come from theoretical or computational journals and go to theoretical or applied ones. The visualization helps us to see how research flows through the network.

\subsection{Model evaluation} \label{modeleval}

Having illustrated the value of the latent space rating model, in this section we compare competing latent space models and estimation methods. We discuss the choice of model dimension, model fit to the observed network, tradeoffs between estimation methods, and sensitivity to hyperpriors.

\subsubsection{Latent dimension} \label{dim}

To choose the latent space dimension of the model we consider adaptations of the Bayesian information criterion (BIC), an estimate of the integrated likelihood of the data. %P(Y) = \int P(\theta|Y)P(\theta)d\theta. 
The classic Bayesian information criterion (BIC) of a model is an approximation to $-2log(P(Y))$, assuming that the data under the model follows an exponential family distribution.

\begin{equation}
BIC = -2log(P(Y \vert \hat{\theta})) + d \cdot log(s),
\end{equation}

\noindent where $\hat{\theta}$ is the maximum-likelihood parameter estimate, $d$ is the dimension of the parameter space, and $s$ is the effective sample size. In the case of the latent space rating model, this is complicated by several factors. The MCMC-generated maximum-likelihood estimate may not be globally optimal, the parameter dimension is constrained by prior distributions, and different effective sample sizes are used in estimating various parameters. As an alternative to BIC we consider the BIC for model selection (BICM) of \citet{BICM}.

\begin{equation}
BICM = -2\hat{\ell}_{max} + \sum_{k=1}^{K}log(n_k) + (\hat{d} - K)log(n_{K}),
\end{equation}

\noindent where $K$ is the number of fixed effects in the model. We let $\hat{\ell}_{max} = log(P(Y \vert \hat{\theta}))$, where $\hat{\theta}$ is our highest-likelihood MCMC estimate. We consider the estimate of $\hat{d}$ derived by \citet{BICM}, which draws on the asymptotic distribution

\begin{equation}
\ell_{max} - \ell_t \sim Gamma(\alpha, 1),
\end{equation}

\noindent where $\ell_t$ is the log likelihood of a draw from the posterior distribution of $\theta$, and $\alpha$ = $d/2$. The variance of this gamma distribution is $\alpha$, which we estimate by $var(\ell_t)$, and thereby estimate

\begin{equation}
\hat{d} = 2var(\ell_t).
\end{equation}

\noindent We use the MCMC sample for this estimate, so it is important that it has been thinned enough that the posterior log likelihoods are independent. (We verify this using the \code{mcmc.diagnostics} function of \pkg{latentnet}.) In addition, we note that the assumed scale parameter implies another estimate of $\alpha$ if we have a pre-existing estimate of $l_{max}$, namely $\alpha = E[l_{max} - l_t]$. This recovers the estimate of parameter dimension introduced by \citet{paramD},    %Note that this is similar to a one half variance formula in Gelman, but that formula has questioned. http://andrewgelman.com/2006/07/06/number_of_param/

\begin{equation}
p_D = -2*\hat E[l_t - l_{max}] = \hat{D}_{avg}(Y, \theta) - D(Y, \hat{\theta}), %also in gelman
\end{equation}

\noindent where deviance $D(Y,\theta) = -2log(P(Y \vert \theta))$ and  $\hat{D}_{avg}(Y)$ is calculated by averaging over the MCMC sample. \citet{paramD} use the posterior mean as the point estimate $\hat\theta$, but note that others can be justified. Discrepancy between $\hat{d}$ and $p_D$ reveals the extent to which the asymptotic gamma assumption does not fit the data. This may imply that the scale parameter is not exactly one, though it should be close. Alternately, it may indicate error in our estimate of $l_{max}$ or $var(l_t)$, but if the magnitude of the discrepancy error is small, as we find in this example, then we can proceed with model selection.

For the effective sample size, we are primarily concerned with the number of data points used to estimate positions, since here our models only differ in the dimension of positions. Although \citet{BICM} based the effective sample size for positions in a binary network on the number of realized edges, in a valued network zero-weight edges are informative, so we use $2(n-1)$. This poses a slight problem because the sender and receiver random effects would logically halve that effective sample size to $n-1$, since they only depend on out- and in-edges respectively. However, the over-penalization that results is consistent across dimensional models, so we can ignore it when comparing models.

% We can use the simulated graph stuff from sim_point_point to establish that the marginal distribution of data under the model is similar to the distribution of data given theta, which is poisson/exponential family?

<< latent_sr1_init_r, eval = T, results = 'hide', echo = F, cache = T, cache.comments = FALSE, warning= FALSE>>=

#random initation of sender and receiver, and scaling Z helps the model fit
D = 1; N = nrow(Cmatrix)
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

t1.init.r1 = Sys.time()
latent.srp1.init.r = ergmm(Cnet ~ euclidean(d = 1) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 500000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        pilot.runs = 10),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
t2.init.r1 = Sys.time()
@

<< latent_sr3_init_r, eval = T, results = 'hide', echo = F, cache = T, cache.comments = FALSE, warning= FALSE>>=

#random initation of sender and receiver, and scaling Z helps the model fit
D = 3; N = nrow(Cmatrix)
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

t1.init.r3 = Sys.time()
latent.srp3.init.r = ergmm(Cnet ~ euclidean(d = 3) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mcmc", "mkl", "procrustes", "mle"),
                                control = ergmm.control(burnin = 500000, interval = 500,
                                                        sample.size = 5000, mle.maxit = 100,
                                                        pilot.runs = 10),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
t2.init.r3 = Sys.time()
@

<< plot_3d, eval = F, echo = F, cache = T, fig.keep = "none", results = "asis">>=

# Three-dimensional model/plot?
tmp = lsqn(t(Cmatrix), runs = 50, tol = .01, Z.init = "rnorm", D = 3)
plot_ly(data = data.frame(tmp$map$Z), x = ~X1, y = ~X2, z = ~X3, color = col2)

hist((26501.9 - lgamma.constant)- latent.srp0$sample$lpY, freq = F, breaks = 50)
curve(dgamma(x, (47*2+1)/2, 1), add = T, col = "red") #pretty good
@

<<BIC, eval = T, echo = F, cache = T, fig.keep = "none", results = "asis">>=

# BIC and BICM type estimates. Vary depending on how we estimate:
# 1. l_max: (from sample or shifted gamma assumption) - we'll stick with our MLE (pmean or mkl)
# 2. d: d_hat based on deviance vs variance of posterior log likelihood (2)
# 3. n: Constant n vs. n adjusted (for number of data points involved in estimating a parameter) (2)

n = 47
# values for table ####

lhat_0 = mean(latent.srp0$sample$lpY) 
lhat_1 = mean(latent.srp1.init.r$sample$lpY) 
lhat_2 = mean(latent.srp2.init.r$sample$lpY) 
lhat_3 = mean(latent.srp3.init.r$sample$lpY) 

#dimensions based on posterior variance
dhat_0 = 2*var(latent.srp0$sample$lpY)
dhat_1 = 2*var(latent.srp1.init.r$sample$lpY)
dhat_2 = 2*var(latent.srp2.init.r$sample$lpY)
dhat_3 = 2*var(latent.srp3.init.r$sample$lpY)

#posterior means
model = latent.srp0
pmean_0 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = matrix(0, 47, 1),
                beta = mean(model$sample$beta)) 

model = latent.srp1.init.r
pmean_1 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = apply(model$sample$Z, c(2,3), mean),
                beta = mean(model$sample$beta)) 

model = latent.srp2.init.r
pmean_2 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = apply(model$sample$Z, c(2,3), mean),
                beta = mean(model$sample$beta)) 

model = latent.srp3.init.r
pmean_3 = list(sender = colMeans(model$sample$sender),
                receiver = colMeans(model$sample$receiver),
                Z = apply(model$sample$Z, c(2,3), mean),
                beta = mean(model$sample$beta)) 

lmean_0 = llik(pmean_0, Z = matrix(0, 47, 1), Y = t(Cmatrix), est = "Y")
lmean_1 = llik(pmean_1, Y = t(Cmatrix), est = "Y")
lmean_2 = llik(pmean_2, Y = t(Cmatrix), est = "Y")
lmean_3 = llik(pmean_3, Y = t(Cmatrix), est = "Y")

lmax_0 = llik(latent.srp0$mkl, Z = matrix(0, 47, 1), Y = t(Cmatrix), est = "Y")
lmax_1 = llik(latent.srp1.init.r$mkl, Y = t(Cmatrix), est = "Y")
lmax_2 = llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y")
lmax_3 = llik(latent.srp3.init.r$mkl, Y = t(Cmatrix), est = "Y")

#dimensions based on deviance
pD_0 = -2*lhat_0 + 2*lmean_0
pD_1 = -2*lhat_1 + 2*lmean_1
pD_2 = -2*lhat_2 + 2*lmean_2
pD_3 = -2*lhat_3 + 2*lmean_3

# bootstrap SE for BICM_dhat. for pD should be even smaller/can be controlled by sample size
SE_0 = (2*log(2*(n-1))) * sqrt(var(sapply(1:5000, function(x)
       {var((latent.srp0$sample$lpY)[sample(5000, 5000, replace = T)])})))
SE_1 = (2*log(2*(n-1))) * sqrt(var(sapply(1:5000, function(x)
       {var((latent.srp1.init.r$sample$lpY)[sample(5000, 5000, replace = T)])})))
SE_2 = (2*log(2*(n-1))) * sqrt( var(sapply(1:5000, function(x)
       {var((latent.srp2.init.r$sample$lpY)[sample(5000, 5000, replace = T)])})))
SE_3 = (2*log(2*(n-1))) * sqrt( var(sapply(1:5000, function(x)
       {var((latent.srp3.init.r$sample$lpY)[sample(5000, 5000, replace = T)])})))
# BIC table ####

BIC.table = data.frame(row.names = c("0", "1", "2", "3"))

           BIC.table$"BIC" = c(-2*(lmax_0) + (n*2 + 3)*log(n*(n-1)),
                     -2*(lmax_1) + (n*3 + 4)*log(n*(n-1)),
                     -2*(lmax_2) + (n*4 + 4)*log(n*(n-1)),
                     -2*(lmax_3) + (n*5 + 4)*log(n*(n-1)))
                     #note: bic.ergmm function of latentnet uses the handock et al BIC which is designed for selecting number of clusters

           BIC.table$"$BICM_{\\hat{d}}$" =
                        c(-2*(lmax_0) + 3*(log(n*(n-1))) + (dhat_0 - 3)*log(2*(n-1)),
                        -2*(lmax_1) + 4*(log(n*(n-1))) + (dhat_1 - 4)*log(2*(n-1)), 
                        -2*(lmax_2)+ 4*(log(n*(n-1))) + (dhat_2 - 4)*log(2*(n-1)), 
                        -2*(lmax_3)+ 4*(log(n*(n-1))) + (dhat_3 - 4)*log(2*(n-1)))
           
            BIC.table$"$BICM_{p_D}$" = 
                      c(-2*(lmax_0) + 3*(log(n*(n-1))) + ((pD_0) - 3)*log(2*(n-1)),
                      -2*(lmax_1) + 4*(log(n*(n-1))) + ((pD_1) - 4)*log(2*(n-1)),
                      -2*(lmax_2) + 4*(log(n*(n-1))) + ((pD_2) - 4)*log(2*(n-1)),
                      -2*(lmax_3) + 4*(log(n*(n-1))) + ((pD_3) - 4)*log(2*(n-1)))
                      
           #BIC.table$"$\\hat{SE}_{mcmc}$" = c(SE_0, SE_1, SE_2, SE_3)
           
           BIC.table$"$\\hat{d}$" = c(dhat_0, dhat_1, dhat_2, dhat_3)
        
           BIC.table$"$p_D$" = c(pD_0, pD_1, pD_2, pD_3)
           
           BIC.table$"Rating Cor." =
                           as.character(round(c(cor(latent.srp0$mkl$receiver - latent.srp0$mkl$sender, 
                                latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender),
                           cor(latent.srp1.init.r$mkl$receiver - latent.srp1.init.r$mkl$sender, 
                                latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender),
                           1,
                           cor(latent.srp3.init.r$mkl$receiver - latent.srp3.init.r$mkl$sender, 
                                latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender)), 4))
           

print(xtable(BIC.table, caption = "Comparison of BIC(M), estimated parameter dimension and ratings correlation for models in zero to three dimensions. Correlations listed are to the two-dimensional ratings.", digits = 0, label = "BIC.table"), caption.placement = "top", sanitize.text.function=function(x){x})


@

<<BIC_2, eval = F, echo = F, cache = T, fig.keep = "none", results = "asis">>=

# Comparing estimates ####

#1. lmax. raftery et al assume you don't have lmax, estimate it by mean(lpY) + var(lpY). How does that compare to what we found? Pretty close.
model = latent.srp2.init.r
mean(model$sample$lpY) + var(model$sample$lpY) #-4556.06
llik(model$mkl, Y = t(Cmatrix), est = "Y")  #-4560.834

#2. How do estimates of parameter dimension compare? Does gamma assumption hold up?
# how does the resulting gamma look? with pmean estimate of lmax:
hist(lmax_2 - latent.srp2.init.r$sample$lpY, freq = F, breaks = 30)
curve(dgamma(x, d_hat2/2, 1), xlim = c(0,150), add = T, col = "red") #pretty good (1)
#with adjustment to scale parameter based on discrepancy in alpha estimates:
curve(dgamma(x, d_hat2*((lmax_2-lmean_2)/var(model$sample$lpY))/2, 1), xlim = c(0,150), add = T, col = "green") #better -> (1)
curve(dgamma(x, pD_2/2, 1), xlim = c(0,150), add = T, col = "blue") #best

#with mkl estimate of lmax
hist(llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y") - latent.srp2.init.r$sample$lpY, freq = F, breaks = 50)
curve(dgamma(x, d_hat2/2, 1), xlim = c(0,150), add = T, col = "red") #pretty good

#So either var(l_t) isn't quite right (1), scale = 1 isn't quite right (2), and/or l_max estimate isn't quite right (though this is least likely since we're pretty sure our estimate is close) (3)

#p(y|M) estimation? not enough information #### 
samp1 = rep(0,10000)
N = 47
for (i in 1:length(samp1)) {
    sender.var = rinvchisq(1, 12, 2)
    receiver.var = rinvchisq(1, 12, 2)
    Z.var = rinvchisq(1, sqrt(N), N/8)
  object = list(
    beta = rnorm(1, 0, sd = sqrt(beta.var)),
    sender = rnorm(N, 0, sqrt(sender.var)),
    receiver = rnorm(N, 0, sqrt(receiver.var)),
    Z = matrix(rnorm(N*d, 0, sqrt(Z.var)), nrow = N, ncol = d)
  )
  samp1[i] = llik(object, Y, est = "Y")
}
@

Table \ref{BIC.table} shows, from left to right, a traditional BIC estimate with parameter dimension equal to the number of parameters and sample size equal to $n*(n-1)$; BICM using $\hat{d}$; BICM using $p_D$; the $\hat{d}$ and $p_D$ estimates of parameter dimension; and the correlation in ratings from each model to the two-dimensional model. The two-dimensional model is a major improvement over the zero- and one-dimensional models by all measures. The three-dimensional model is a small improvement over the two-dimensional model. However, the correlation in ratings between the two and three-dimensional models is extremely high (0.9991). These factors combined with the increased difficulty of visualizing three-dimensional estimates leads us to select the two-dimensional model. Although the error of BICM estimates is not of primary interest here, bootstrap estimates of the standard error in $BICM$ due to MCMC sampling variation show that it is small relative to differences between models.

\subsubsection{Model fit} \label{dim}

The latent space and quasi-Stigler models produce roughly equivalent journal rankings, but how well do they model the observed network? We consider elements of the fit of our two-dimensional latent space model in comparison to the quasi-Stigler model.

<<residuals, eval = T, cache = T, message =F, warning = F, echo = F, dependson = c("glm"), fig.width = 6, fig.height = 2.5, fig.align="center", results = "hide", fig.cap = 'Comparison of 2162 residuals for latent space models in zero and two dimensions and the quasi-Stigler model. Left: Residuals ordered by value of the corresponding edge. A linear model is fit to each set. Right: Pearson residuals for Poisson models plotted against fitted values. The highest fitted value is left out to enhance detail.', fig.pos = 'H'>>=

# g1 (quasi stigler, or quassibinomial glm) ####

# g1.res = residuals for expected graph | theta point estimate

QSmat = matrix(0, 47, 47)
QSmat[lower.tri(QSmat)] = (Tmatrix[lower.tri(Tmatrix)])*g1$fitted.values # fitted.values are for upper.tri: 2,1 3,1 ... 3,2, 4,2, ... 47,46 because of how the responses were listed in g1
QSmat = t(QSmat)
QSmat[lower.tri(QSmat)] = (Tmatrix[lower.tri(Tmatrix)])*(1-g1$fitted.values)
QSmat = t(QSmat)

g1.res = QSmat - t(Cmatrix); diag(g1.res) = NA

# latent model without positions ####
model = latent.srp0
R0 = predict(model, type = "mkl") - model$model$Ym; diag(R0) = rep(NA, nrow(model$model$Ym))

# latent model with 2-D positions ####
model = latent.srp2.init.r
R2 = predict(model, type = "mkl") - model$model$Ym; diag(R2) = rep(NA, nrow(model$model$Ym))

# other models with positions ####
#model = latent.srp1
#R1 = predict(model, type = "mkl") - model$model$Ym; diag(R1) = rep(NA, nrow(model$model$Ym))

model = latent.srp3.init.r
R3 = predict(model, type = "mkl") - model$model$Ym; diag(R3) = rep(NA, nrow(model$model$Ym))

# Summary ####

sd(g1.res, na.rm = T); sd(R0, na.rm = T);  sd(R2, na.rm = T)
quantile(g1.res, na.rm = T, c(.05, .95)); quantile(R0, na.rm = T, c(.05, .95)); quantile(R2, na.rm = T, c(.05, .95))

R = data.frame(residual = c(as.vector(g1.res), as.vector(R0), as.vector(R2)),
               model = as.factor(c(rep("QS", 2209), rep("D = 0", 2209), rep("D = 2", 2209))),
               edgeweight = rep(as.vector(t(Cmatrix)), 3),
               order = c(rank(g1.res, ties.method = "random"),
                         rank(R0, ties.method = "random"),
                         rank(R2, ties.method = "random")))

# Sorted Residual Plot ####
res_vs_index = ggplot(R, aes(x = order, y = residual, color = model)) + 
  geom_hline(yintercept = 0, col = "gray") +
  geom_point(size = .4) + ylim(-25,25) +
  theme_bw() +
  theme(legend.position="none") +
  ggtitle("Ordered Residuals")

# Residuals vs. Edge Weight Plot  ####
# not surprisingly, quasi fits best bc binomial model assumes totals (Tmatrix) known
res_vs_edge = ggplot(R, aes(x = edgeweight, y = residual, color = model)) + 
  geom_hline(yintercept = 0, col = "gray") +
  geom_point(size = .5, alpha = .5) +
  geom_smooth(method = "lm", se = F, size = .5) +
  theme_bw() + 
  theme(legend.position = c(.65,.9), legend.text = element_text(size = 7),
        legend.key = element_rect(fill = "transparent"),
        legend.background=element_rect(fill="transparent"),
        legend.direction="horizontal",
        legend.title = element_blank(),
        plot.title = element_text(size = 12)) +
  xlab("edge value") +
  ggtitle("Residuals vs. Edge Value") #+ facet_wrap(~model, scales = "free_x")

# Overdispersion? ####
#Is there overdispersion in the poisson models? We plot the Pearson residuals:

data1 = data.frame(fitted2 = predict(latent.srp2.init.r, type = "mkl")[!is.na(R2)], #removes diag
                   pearson2 = as.vector(R2[!is.na(R2)]/
                                  sqrt(predict(latent.srp2.init.r, type = "mkl")[!is.na(R2)])),
                   fitted3 = predict(latent.srp2.init.r, type = "mkl")[!is.na(R3)],
                   pearson3 = as.vector(R3[!is.na(R3)]/
                                  sqrt(predict(latent.srp2.init.r, type = "mkl")[!is.na(R3)])))
                   #fitted0 = predict(latent.srp0)[!is.na(R0)],
                   #pearson0 = as.vector(R0[!is.na(R0)]/sqrt(predict(latent.srp0)[!is.na(R0)])))

res_vs_fitted = ggplot(data1) +
  #geom_point(aes(color = hue_pal()(3)[2], x = fitted3, y = pearson3),
   #          size = .5, alpha = .5, inherit.aes = F) +
   geom_point(aes(color = hue_pal()(3)[1], x = fitted2, y = pearson2),
           size = .5, alpha = .5, inherit.aes = F) +
  theme_bw() + ggtitle("Residuals vs. Fitted") +
  theme(legend.position = "none",
        plot.title = element_text(size = 12)) +
  ylab("Pearson residual") + xlab("fitted value") + xlim(0,150) +
  scale_color_manual(values = c(hue_pal()(3)[2]))


# PLOT ####

cowplot::plot_grid(res_vs_edge, res_vs_fitted, rel_widths = c(1, .8), nrow = 1)
# left out res_vs_index: Residuals ordered by size and only the range from -25 to 25 is shown to enhance detail.

@

<<residuals2, eval = F, echo = F>>=

#to view outliers of sender and receiver distributions
library(reshape)
Y = data.frame(t(Cmatrix))
Y = melt(Y); colnames(Y) = c("journal", "citations")
receiver_dist = ggplot(data = Y, aes(x = journal, y = citations, color = journal)) + geom_boxplot() + theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("receiver distibutions")

Y = data.frame(Cmatrix)
Y = melt(Y); colnames(Y) = c("journal", "citations")
sender_dist = ggplot(data = Y, aes(x = journal, y = citations, color = journal)) + geom_boxplot() + theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("sender distributions")

ggplot(Y, aes(x = citations, y = journal)) + geom_joy() + xlim(0, 50)
cowplot::plot_grid(sender_dist, receiver_dist)
@

Figure \ref{fig:residuals} (left) displays model residuals ordered by the value of the underlying edge, i.e. number of citations, for a Poisson sender-receiver model with no latent positions (D = 0), the two-dimensional latent space model (D = 2), and the quasi-Stigler model. We include the zero-dimensional model to help distinguish the impact of modelling edge weights as Poisson from the effect of adding positions. The quasi-Stigler model has the smallest residuals overall ($SD = 2.2$), which we expect since the model is constrained by dyad totals. The two-dimensional latent space model is a bit worse ($SD = 3.7$), but of course much better than the no-position model ($SD = 8.8$). The no-position model is systematically biased, in that edges with lower citation counts are more likely to be overestimated, and heavier edges more likely to be underestimated. This pattern is evident in the other two models, but much less severe. Examining high-value edges underestimated by the two-dimensional model reveals that they are outliers relative to the citation patterns of the underlying journals. This may be a result of the long-tailed distribution of citation counts per article, but we do not have access to the itemized counts to investigate. There may be additional covariates that we could add to the model to account for these seeming outliers. We also note that by far the highest-valued edge in the network is from \textit{Statistics in Medicine} to \textit{Biometrika}. These journals are related to biological sciences, a field with much higher average citations counts than statistics \citep{if_fraction}. This hints at the importance of a model extension to account for differing activity patterns within journal fields. Although the data for related fields is not currently available for publication, we revisit relevant model extensions in the discussion.

The right-hand plot of Figure \ref{fig:residuals} displays Pearson residuals against fitted values for the two-dimensional model. There is heightened variation where fitted values are close to zero and residuals must be negative. Although these mostly correspond to small absolute differences, the standardized scale reveals some lack of fit. To account for this in future implementations we may add a small constant to the citation data matrix to stabilize the model for low counts. This is reasonable, since we presume that all statistics journals in the network have at least a small chance of exchanging citations. Outside the near-zero area, the model spread appears roughly constant given the amount of data across fitted values, and the model does not appear overdispersed.

%3). Structure, reciprocity 

Although dyad-total constraints in the quasi-Stigler model lower the residuals, they also restrict the space of simulated networks. To further compare the model fits we conduct a posterior predictive check on a feature of interest, network reciprocity. We employ a version of the reciprocity statistic defined by \citet{newman10}, which we adapt for weighted networks,

\begin{equation}
reciprocity(Y) = \frac{\sum_{i \neq j}y_{ij}*y_{ji}}{\sum_{i \neq j}(\frac{y_{ij}+y_{ji}}{2})^2}.
\end{equation}

\noindent The value of the statistic ranges from zero to one, achieving the maximum if the network is symmetric. The denominator controls for the influence of variation in the dyad totals. We also consider an unnormalized version with the denominator removed. Figure \ref{fig:structure_reciprocity_plot} shows distributions of reciprocity statistics, each based on 5000 simulated networks. They are simulated from an MKL point estimate for the two-dimensional latent space model; the posterior parameter distribution of the two-dimensional latent space model; and the point estimate for the quasi-Stigler model. To simulate networks from the quasi-Stigler model we use a beta-binomial approximation with mean and variance matching the overdispersed binomial. The left panel shows reciprocity while the right panel shows unnormalized values. 

<<structure_reciprocity, eval = T, echo = F, cache = T, message =F, warning = F, results = 'hide', fig.height = 4>>=

reciprocity = function(A, normalize = T) {
 if(normalize) {sum(A*t(A), na.rm = T)/sum(((A+t(A))/2)^2, na.rm = T)}
  else {sum(A*t(A), na.rm = T)}
}

nsim = 5000
nsim.iter = 500 #can't do all at once
rounds = nsim/nsim.iter

# 1. unnormalized ####
Q = reciprocity(t(Cmatrix), normalize = F)

# Poisson, QS
tmp.qs = c()
for (i in 1:rounds) {
  g = simulate.qs(g1, nsim = nsim.iter)
  #g = lapply(g, round)
  tmp = unlist( lapply(g, reciprocity, normalize = F))
  tmp.qs = c(tmp.qs, tmp)
}

# Poisson, D = 2
# the simulate.ergmm function randomly picks an mcmc sample to use for simulatution, i.e. simulating from the posterior distribution.
tmp2 = c()
for (i in 1:rounds) {
  sim2 = simulate(latent.srp2.init.r, nsim.iter)
  tmp = unlist(lapply(sim2[[2]], function(x) {reciprocity(as.sociomatrix(x, attrname = "citations"), normalize = F)}) )
  tmp2 = c(tmp, tmp2)
}

# Poisson, D = 2, MKL point estimate
model = latent.srp2.init.r
lambda.mkl = exp(model$mkl$receiver + t(model$mkl$sender -
                dist2(model$mkl$Z)) + model$mkl$beta); diag(lambda.mkl) = 0
tmp2mkl = rep(0, nsim)
for (i in 1:(nsim)) {
  mat = matrix(rpois(2209, lambda.mkl), ncol = 47, nrow = 47); diag(mat) = 0 #0 instea of NA. NA causes all mod values to go to 0
  tmp2mkl[i] = reciprocity(mat, normalize = F)
}


# 2. normalized ####
Q.norm = reciprocity(t(Cmatrix), normalize = T)

# QS
tmp.qs.norm = c()
for (i in 1:rounds) {
  g = simulate.qs(g1, nsim = nsim.iter)
  #g = lapply(g, round)
  tmp = unlist( lapply(g, reciprocity, normalize = T))
  tmp.qs.norm = c(tmp.qs.norm, tmp)
}

# Poisson, D = 2
# the simulate.ergmm function randomly picks an mcmc sample to use for simulatution, i.e. simulating from the posterior distribution.
tmp2.norm = c()
for (i in 1:rounds) {
  sim2 = simulate(latent.srp2.init.r, nsim.iter)
  tmp = unlist(lapply(sim2[[2]], function(x) {reciprocity(as.sociomatrix(x, attrname = "citations"), normalize = T)}) )
  tmp2.norm = c(tmp, tmp2.norm)
}

# Poisson, D = 2, MKL point estimate
tmp2mkl.norm = rep(0, nsim)
for (i in 1:(nsim)) {
  mat = matrix(rpois(2209, lambda.mkl), ncol = 47, nrow = 47); diag(mat) = 0 #0 instea of NA. NA causes all mod values to go to 0
  tmp2mkl.norm[i] = reciprocity(mat, normalize = T)
}

rm(sim2); rm(g)
@

<<structure_reciprocity_plot, eval = T, echo = F, cache = T, message =F, warning = F, fig.height = 2.5, fig.width = 6, fig.align = "center", fig.pos = 'H', fig.cap = "Comparison of simulated reciprocity distributions based on 5000 simulations" , dependson = c("structure_reciprocity")>>=

rec.data = data.frame(model = c(rep("QS", nsim), rep("D = 2", nsim),
                                rep("D = 2, posterior", nsim)), 
                      normalized_reciprocity = c(tmp.qs.norm, tmp2mkl.norm, tmp2.norm)) 

recplot1.norm = ggplot(rec.data, aes(x = normalized_reciprocity, color = model)) +
  geom_freqpoly(size = .75, aes(linetype = model)) + 
  scale_linetype_manual(values=c("solid", "dashed", "solid"))+
  geom_vline(xintercept = Q.norm, color = "gray", show.legend = T) + 
  geom_text(x=Q.norm, label="observed", y = 500, colour="gray", angle=90, vjust = -.2, size=3) +
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  theme_bw() + 
  theme(legend.position = "none",
        plot.title = element_text(size = 12)) +
  scale_color_manual(values = c(hue_pal()(3)[2], hue_pal()(8)[8], hue_pal()(3)[3])) +
  ggtitle("Reciprocity") + labs(x = NULL) 

rec.data = data.frame(model = c(rep("QS", nsim), rep("D = 2", nsim), rep("D = 2,\nposterior", nsim)), 
                      reciprocity = c(tmp.qs, tmp2mkl, tmp2))

recplot1 = ggplot(rec.data, aes(x = reciprocity, color = model)) +
  geom_freqpoly(size = .75, aes(linetype = model)) + 
  scale_linetype_manual(values=c("solid", "dashed", "solid"))+
  geom_vline(xintercept = Q, color = "gray", show.legend = T) + 
  geom_text(x=Q, label="observed", y = 1400, colour="gray", angle=90, vjust = -.2, size=3) +
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  theme_bw() +
  theme(axis.text.x = element_text(size = 8),
        legend.position = c(.8,.8), legend.text = element_text(size = 7),
        legend.key = element_rect(fill = "transparent"),
        legend.background=element_rect(fill="transparent"),
        legend.title = element_blank(),
        plot.title = element_text(size = 12)) +
  scale_color_manual(values = c(hue_pal()(3)[2], hue_pal()(8)[8], hue_pal()(3)[3])) +
  ggtitle("Unnormalized Reciprocity") +  labs(x = NULL)


cowplot::plot_grid(recplot1.norm, recplot1, rel_widths = c(.9, 1), nrow = 1)

@

The average reciprocity estimates are similar across models and biased to the right. This is likely due to the underestimation of high-valued outliers. However, the variance is larger for the two-dimensional models, especially for unnormalized reciprocity. As a result, only the two-dimensional models include the observed values in their simulated ranges. For reciprocity, the percents of simulated values to the left of the observed value are 0.012 and 0.001 for the posterior distribution and point estimate respectively; for unnormalized reciprocity they are 0.19 and 0.15. The quasi-Stigler model is overly restrictive in that simulations from the model do not include the observed network with respect to this statistic.

<<structure_reciprocity_plot2, eval = F, echo = F, cache = T, message =F, warning = F, fig.height = 2, fig.width = 6, fig.pos = 'H', fig.cap = "Comparison of simulated reciprocity distributions for two-dimensional Poisson posterior and MKL point estimate based on 1000 simulations" >>=

# Compare modularity distribution from simulating from MKL point estimate vs. posterior distribution

# Normalized ####
rec.data = data.frame(model = c(rep("D = 2", nsim), rep("D = 2, mkl", nsim)), 
                      reciprocity = c(tmp2.norm, tmp2mkl.norm))

recplot2.norm = ggplot(rec.data, aes(x = reciprocity, fill = model)) +
  geom_histogram(position = "identity", bins = 40) +
  geom_vline(xintercept = Q.norm, color = "gray", show.legend = T) + 
  theme_bw() + facet_wrap(~model, scales = "fixed") + 
  theme(legend.position = "none") + 
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  scale_fill_manual(values = c(hue_pal()(3)[2], hue_pal()(4)[4])) +
  labs(x = "Normalized Reciprocity")

# Unnormalized ####

rec.data = data.frame(model = c(rep("D = 2", nsim), rep("D = 2, mkl", nsim)), 
                      reciprocity = c(tmp2, tmp2mkl))
recplot2 = ggplot(rec.data, aes(x = reciprocity, fill = model)) +
  geom_histogram(position = "identity", bins = 40) +
  geom_vline(xintercept = Q, color = "gray", show.legend = T) + 
  theme_bw() + facet_wrap(~model, scales = "fixed") + 
  theme(legend.position = "none") +
  scale_x_discrete(limits=c(360000, 400000) )+
  geom_hline(yintercept = 0, col = "gray", size = .3) +
  scale_fill_manual(values = c(hue_pal()(3)[2], hue_pal()(4)[4])) +
  labs(x = "Reciprocity")

cowplot::plot_grid(recplot2.norm, recplot2, rel_widths = c(1, 1), nrow = 1)

@

<<structure_reciprocity_stats, eval = F, echo = F>>=
nsim
sum(tmp2<Q)/nsim
sum(tmp2mkl<Q)/nsim
sum(tmp.qs<Q)/nsim
sum(tmp2.norm<Q.norm)/nsim
sum(tmp2mkl.norm<Q.norm)/nsim
sum(tmp.qs.norm<Q.norm)/nsim

c(mean(tmp2), mean(tmp2mkl), mean(tmp.qs), mean(tmp2.norm), mean(tmp2mkl.norm), mean(tmp.qs.norm))

c(sd(tmp2), sd(tmp2mkl), sd(tmp.qs), sd(tmp2.norm), sd(tmp2mkl.norm), sd(tmp.qs.norm))

@

\subsubsection{Comparison of estimation methods} \label{estimeval}

We now turn to the comparison of estimation methods for the latent space model. We illustrate that quasi-Netwon optimization returns ratings nearly identical to MCMC results in much less time. 

<<compare_latent_models, eval = T, echo = F, cache=T, results='asis', cache.comments = FALSE, warning= FALSE, dependson = c("latent_sr2_init_r", "latent_sr2", "latent_qn")>>=

results.table = data.frame(row.names = c("QN (L-BFGS-B)", "MCMC (MKL)", "MCMC.init.r (MKL)" ))

results.table$"Time (min)" = c("$<1$", round(t2.srp2 - t1.srp2), round(t2.init.r - t1.init.r))

results.table$"$l(Y|\\hat{\\theta})$" = as.character(round(
                                c(llik(latent.srp2.init.r$burnin.start, Y = t(Cmatrix), est = "Y"),
                                #latentnet finds a conditional posterior mode using optim before burnin
                                llik(latent.srp2$mkl, Y = t(Cmatrix), est = "Y"),
                                llik(latent.srp2.init.r$mkl, Y = t(Cmatrix), est = "Y"))))
results.table$"$l(\\hat{\\theta})$" = as.character(round(
                                c(llik(latent.srp2.init.r$burnin.start, Y = t(Cmatrix), est = "theta"),
                                with(latent.srp2$mcmc.pmode, llik(latent.srp2$mkl,
                                     Y = t(Cmatrix), sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")),
                                with(latent.srp2.init.r$pmode, llik(latent.srp2.init.r$mkl,
                                     Y = t(Cmatrix), sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")))))
results.table$"$l(\\hat{\\theta}|Y)$" = as.character(as.numeric(results.table$"$l(Y|\\hat{\\theta})$") + as.numeric(results.table$"$l(\\hat{\\theta})$"))

latent2 = latent.srp2.init.r$mkl$receiver - latent.srp2.init.r$mkl$sender
results.table$"Rating Cor." = c(cor(latent2, latent.srp2.init.r$burnin.start$receiver - 
                                            latent.srp2.init.r$burnin.start$sender),
                               cor(latent2, latent.srp2$mkl$receiver - latent.srp2$mkl$sender),
                               cor(latent2, latent.srp2.init.r$mkl$receiver - 
                                            latent.srp2.init.r$mkl$sender))
                               #vs.mle cor(latent2, latent.srp2.init.r$mle$receiver - latent.srp2.init.r$mle$sender)

print(xtable(results.table, caption = "Comparison of estimation methods. The $\\ell$ function is the log of the probability.", digits = 4, label = "results.table"), caption.placement = "top", sanitize.text.function=function(x){x})

# MCMC diagnostics ####

  # these all look fine
  # mcmc.diagnostics(latent.srp2)
  # mcmc.diagnostics(latent.srp2.init.r)
  # mcmc.diagnostics(latent.srp2.init.r.sann)

# Other estimates ####
# est = "Y"
# llik(latent.srp2.init.r$mcmc.mle, Y = t(Cmatrix), est = est)
# llik(latent.srp2.init.r$mcmc.pmode, Y = t(Cmatrix), est = est)
# with(latent.srp2.init.r.bfgs$mcmc.mle, llik(latent.srp2.init.r.bfgs$mkl, Y = t(Cmatrix), sender.var = sender.var, receiver.var = receiver.var,  Z.var = Z.var, est = est))
#llik(latent.srp2.init.r.bfgs$mcmc.mle, Y = t(Cmatrix), est = est)
#llik(latent.srp2.init.r.bfgs$mcmc.pmode, Y = t(Cmatrix), est = est)#same as sann

# Compare positions (with or without procrustes) ####
#plot(latent.srp2.init.r$mkl$Z, col = col2, pch = 16)
#plot(vegan::procrustes(latent.srp2.init.r$mkl$Z, latent.srp2.init.r$burnin.start$Z)$Yrot, col = col2, pch = 16)


@

Table \ref{results.table} compares results for the two-dimensional latent space rating model by three different estimations techniques: 1) The limited-memory bounded quasi-Newton method as implemented by \code{optim} (QN L-BFGS-B). 2) MCMC estimation as implemented by \pkg{latentnet}; and 3) MCMC estimation as in method two, but with random initialization of sender and receiver coefficients and MDS initial positions scaled so that the maximum coordinate size is one. (We henceforth refer to this as the ``random'' initialization method.) Note that this initialization is also employed for the quasi-Netwon estimate. For MCMC chains we allowed a burn-in period of $500000$, and collected a sample of size $5000$ by storing every $500th$ draw. We checked for convergence and appropriate MCMC interval and burn-in values using the \code{mcmc.diagnostics} function of \pkg{latentnet}. The MCMC method of row two seems to converge based on diagnostic plots, but given the higher-probably estimate resulting from a different initialization method, we see that it has in fact converged to a local maximum.

Timing of computations was in R 3.3.2 with a MacBookPro, 2.6 GHz Intel Core i5 processor with 8 GB 1600 MHz DDR3 memory. Although not employed here, \pkg{latentnet} does allow for parallel processing of multiple MCMC chains. The time estimates should be considered ballpark for each method, as the number of iterations for the quasi-Netwon methods and control parameters for the MCMC could be augmented to achieve some speed-up. However, the process of optimizing these would mostly likely negate the time benefits.

We present MKL estimates from MCMC methods because they have higher posterior and graph likelihood than other point estimates, such as the posterior mode. To calculate $\ell(\theta)$ for these estimates, where $\ell$ is the log of the probability function, we use the random effect variances estimated by the MCMC posterior mode. Results in previous sections have all drawn on the MKL estimates from MCMC estimation with random initialization (row three), because they have even higher probably than other MKL estimates. The MCMC methods are clearly much slower than the quasi-Newton methods due to the number of iterations required, although they have the advantage of returning a posterior sample. The much faster estimates from the quasi-Newton methods are very highly correlated with these results (column five), and the log likelihood is only a tiny bit worse.

<<journal_simulation_study_data_optim, eval = T, echo = F, warning = F, message = F, cache = T, results = "hide">>=

# 3. fits.o with optim (via latentnet tofit = mle) ####

trials = 100; tol = .01; #tried adding precision but that didn't really change anything
D = 2; N = nrow(Cmatrix)
fits.o = list()
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))

t1.fits.o = Sys.time()
for(i in 1:trials) {
  s = simulate(model$model, par = model$mkl)
  a = rnorm(N) 
  b = rnorm(N) 
  q  = ergmm(s ~ euclidean(d = 2) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mle"),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
  fits.o[[i]] = q
}
t2.fits.o = Sys.time()
@

<<journal_simulation_study_plots, eval = T, echo = F, cache = T, warning = F, fig.width = 6, fig.height = 5, fig.cap = 'Distribution of differences in ratings (receiver - sender) and total actor parameters (sender + receiver + intercept) from the data-generating model based on 100 simulations' , dependson = c("journal_simulation_study_data")>>=

# Plot distributions of differences in ratings and sum of actor effects + intercept
library(ggplot2)

# MAP ####
# data ####
data.o = data.frame(journal = rep(rownames(Cmatrix), length(fits.o)),
                   sender_diff_raw = unlist(lapply(fits.o, function(x) {x$start$sender - model$mkl$sender})),
                   receiver_diff_raw = unlist(lapply(fits.o, function(x) {x$start$receiver - model$mkl$receiver})),
                   sender_diff = unlist(lapply(fits.o, function(x) {x$start$sender - mean(x$start$sender) - 
                                                (model$mkl$sender - mean(model$mkl$sender))})),
                   receiver_diff = unlist(lapply(fits.o, function(x) {x$start$receiver - mean(x$start$receiver) - 
                                                (model$mkl$receiver - mean(model$mkl$receiver))})),
                   beta_diff = rep(unlist(lapply(fits.o, function(x) {x$start$beta - model$mkl$beta})), each = 47)
                   )

# 1. Receiver - sender to show ratings changes ####
fits.o.rs = ggplot(data.o, aes(x = journal, y = receiver_diff - sender_diff, color = journal)) +
  geom_boxplot() +
  theme_bw() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1, size = 7),
        plot.title = element_text(size = 12)) +
  geom_abline(slope = 0, intercept = 0, col = "gray") + 
  ggtitle("Difference in rating") +
  xlab(NULL) + ylab(NULL)

# 2. Sender + receiver + beta to show variation with positions ####
fits.o.rsb = ggplot(data.o, aes(x = journal, y = sender_diff_raw + receiver_diff_raw + beta_diff, color = journal)) +
  geom_boxplot() +
  theme_bw() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1, size = 7),
        plot.title = element_text(size = 12)) +
  geom_abline(slope = 0, intercept = 0, col = "gray") +
  ggtitle("Difference in total") +
  xlab(NULL) + ylab(NULL)

### MLE ####
# data, mle ####
data.o.mle = data.frame(journal = rep(rownames(Cmatrix), length(fits.o)),
                  sender_diff_raw = unlist(lapply(fits.o, function(x) {x$start$sender - model$mle$sender})),
                  receiver_diff_raw = unlist(lapply(fits.o, function(x) {x$start$receiver - model$mle$receiver})),
                  sender_diff = unlist(lapply(fits.o, function(x) {x$mle$sender - mean(x$mle$sender) - 
                                                (model$mkl$sender - mean(model$mkl$sender))})),
                   receiver_diff = unlist(lapply(fits.o, function(x) {x$mle$receiver - mean(x$mle$receiver) - 
                                                (model$mkl$receiver - mean(model$mkl$receiver))})),
                   beta_diff = rep(unlist(lapply(fits.o, function(x) {x$mle$beta - model$mkl$beta})), each = 47)
                   )

# 1. Receiver - sender to show ratings changes ####
fits.o.rs.mle = ggplot(data.o.mle, aes(x = journal, y = receiver_diff - sender_diff, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_abline(slope = 0, intercept = 0, col = "gray") + 
  ggtitle("Difference in rating") +
  xlab(NULL) + ylab(NULL)

# 2. Sender + receiver + beta to show variation with positions ####
fits.o.rsb.mle = ggplot(data.o.mle, aes(x = journal, y = sender_diff + receiver_diff + beta_diff, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_abline(slope = 0, intercept = 0, col = "gray") +
  ggtitle("Difference in total") +
  xlab(NULL) + ylab(NULL) + 
  ylim(-2,3)

# PLOTS ####
cowplot::plot_grid(fits.o.rs, fits.o.rsb, nrow = 2)
@

To more formally examine the accuracy of quasi-Newton estimation we conduct a simulation study. We generate 100 simulated citation networks from the best MKL point estimate and fit each by QN L-BFGS-B. We then calculate the differences in ratings (sender minus receiver coefficient) and total actor parameters (sender plus receiver coefficient plus intercept) between the data generating model and the estimate from each simulation. Although the zero-mean prior reduces the non-identifiability of the random effects, with diffuse priors it is not entirely eliminated in practice. To account for this when calculating differences in ratings between fits, we center the sender and receiver estimates to mean zero. The results are presented in Figure \ref{fig:journal_simulation_study_plots}. The top panel shows that the differences in ratings are all concentrated near zero, indicating that QN L-BFGS-B does reliably recreate the ``true'' ratings in this case. Slightly larger variances occur for journals such as StataJ which have fewer non-zero edges. The bottom panel shows the differences in total actor parameters. Deviations from zero in this panel indicate changes in positions relative to the data-generating model. Journals estimated to be closer on aggregate to other journals compensate with smaller random effects, and appear below the x-axis. Three journals lie significantly below the x-axis. We can see in the network visualization that these three are located in the periphery of the network space. Thus, there is a linear range in which their position is nearly identified with the total of their sender and receiver coefficients. This is not a problem with the estimation method but stems from underlying uncertainty in the model. However, it does not affect our estimated rankings, as we see in the top panel, nor the overall configuration of the network. That said, we must recognize that, in general, estimated positions of peripheral nodes may be much more variable than central ones.

<< poisson_glm_offset, eval = T, echo = F, warning = F, message = F, fig.keep = "none", cache = T, results = "hide">>=

# To estimate uncertainty in latent position model based on quasi-Newton estimate 

# build design matrix:
n = 47
x1c = matrix(0, n^2, 2*n)
for (i in 1:n) {
  for (j in 1:n) { 
    if (i != j) {
      x1c[n*(i-1)+j,i]=1;
      x1c[n*(i-1)+j,j+n]=1;
    }
  }
}

x1c[,35] = 0 #choose the low data journal as the constrained one
x1c[,35+47] = 0 #choose the low data journal as the constrained one

# response
y2 = as.vector(t(Cmatrix))

# offset
Z_dist = dist2(model$mkl$Z)

#model to compare to 
model = latent.srp2.init.r

# glm:
g2c = glm(y2 ~ x1c + offset(-as.vector(Z_dist)), family = poisson(link="log"))

# compare rating (basically identical):
plot(g2c$coefficients[2:48] - g2c$coefficients[49:95], model$mkl$receiver - model$mkl$sender)
cor(g2c$coefficients[2:48] - g2c$coefficients[49:95], model$mkl$receiver - model$mkl$sender, use = "complete.obs")

# compare sd
v_g2c = vcov(g2c) #vcov(g1) = summary(g1)$cov.scaled = summary(g1)$cov.unscaled*phi 
sd2 = apply(model$sample$receiver - model$sample$sender, 2, sd)[-35]
sd2g = sqrt(sapply(2:47, function(x) {v_g2c[x,x] + v_g2c[x+46,x+46] + 2*v_g2c[x,x+46]}))
plot(sd2, sd2g)
range(sd2)
range(sd2g)
range(sd2 - sd2g)
mean(sd2 - sd2g)
@

A drawback to quasi-Newton estimation is that it does not generate a sample of the posterior distribution which which to estimate MKL positions or the variance of ratings. However, we can still approximate the ratings uncertainty. One way to do this is to fit a Poisson GLM to the data matrix with estimated positions as an offset, i.e the distance term in Equation \ref{eq3} is treated as a constant. We implement such a model and find similar estimates of ratings uncertainty to those derived from the MCMC sample, although we do lose one journal comparison due to a model-fitting constraint. The estimated standard deviations range from 0.22 to 0.28, compared to the MCMC range of 0.22 to 0.29, with differences having mean 0.002 and maximum 0.013. Not only does this provide an estimate of rating uncertainty, it also confirms that there is minimal dependence between positional parameters and ratings estimates, as was alluded to by Figure \ref{fig:journal_simulation_study_plots}.
%The dependence between ratings and positions in a valued network is likely to be low. Roughly, the positions model symmetric effects, while the ratings model directed effects, and the edge values help us to distinguish them. 

<<hyperpriors, eval = T, cache = T, echo = F, results = 'hide', warning= F, message = F, fig.keep='none', dependson= c("llik_and_latent_qn")>>=

# Which parameters could we adjust for more realistic priors?
# We consider putting narrower priors on the variance and intercept parameters

# Adjust the models ####

N = 47; d = 2
sender.var.df = 2*3 #default 3 ** shrink sender range
receiver.var.df  = 2*3 #default 3 ** shrink receiver range
Z.var.df = 2*sqrt(N)  #default sqrt(N) ** shrink z range

prior.sender.var = .5 #default 1
prior.receiver.var = .5
prior.Z.var = N/32 #default N/8 ** shrink sigma_z range
beta.var = 1 #default 9

quantile(rinvchisq(100000, sender.var.df, prior.sender.var), probs = c(0.05, .5, .95))
quantile(rinvchisq(100000, Z.var.df, prior.Z.var), probs = c(0.05, .5, .95))

samp1 = rep(0, 500)
median1 = rep(0, length(samp1))
mean1 = rep(0, length(samp1))
N = 47
for (i in 1:length(samp1)) {
  # higher df condenses it
  # higher var moves mode from 0
  sender.var = rinvchisq(1, sender.var.df, prior.sender.var)
  receiver.var = rinvchisq(1, receiver.var.df, prior.receiver.var)
  Z.var = rinvchisq(1, Z.var.df, prior.Z.var)
  object = list(map = list(
    beta = rnorm(1, 0, sd = sqrt(beta.var)),
    sender = rnorm(N, 0, sqrt(sender.var)),
    receiver = rnorm(N, 0, sqrt(receiver.var)),
    Z = matrix(rnorm(N*d, 0, sqrt(Z.var)), nrow = N, ncol = d)
  ))
  median1[i] = median(predict.lsqn(object, type = "rpois"), na.rm = T)
  mean1[i] = mean(predict.lsqn(object, type = "rpois"), na.rm = T)
}
Z_dist = dist2(object$map$Z)
summary(median1)
summary(mean1)
@

<< latent_hyperpriors, eval = T, cache = T, echo = F, results = 'hide', warning= F, message = F, fig.keep='none', dependson= c("llik_and_latent_qn")>>=

latent.srp2.h = ergmm(Cnet ~ euclidean(d = 2) + rsender + rreceiver, 
                    response = "citations", family = "Poisson.log", seed = 30,
                    prior = ergmm.prior(sender.var.df = sender.var.df,
                                        receiver.var.df = receiver.var.df,
                                        Z.var.df = Z.var.df,
                                        sender.var = prior.sender.var,
                                        receiver.var = prior.receiver.var, 
                                        Z.var = prior.Z.var,
                                        beta.var = beta.var),
                    control = ergmm.control(burnin = 50000, interval = 500,
                                            sample.size = 5000, mle.maxit = 100,
                                            refine.user.start = F),
                    user.start = with(latent_qn2.h$map, list(Z=Z, sender=sender, receiver=receiver,
                                            beta = beta, sender.var = sender.var,
                                            receiver.var = receiver.var, Z.var = Z.var))
                    )

# Analyze ####

model = latent.srp2.init.r
sd2 = apply(model$sample$receiver - model$sample$sender, 2, sd)
latent2 = model$mkl$receiver - model$mkl$sender
centipede1 = t(cbind(latent2, latent2 - 1.96*sd2, latent2 + 1.96*sd2))
colnames(centipede1) = Cnet%v%"vertex.names"

model = latent.srp2.h
sd.h = apply(model$sample$receiver - model$sample$sender, 2, sd)
latent.h = model$mkl$receiver - model$mkl$sender
centipede.h = t(cbind(latent.h, latent.h - 1.96*sd.h, latent.h + 1.96*sd.h))
colnames(centipede.h) = Cnet%v%"vertex.names"

par(mfrow = c(1,2))
centipede.plot(centipede1, vgrid = c(-1,0,1,2), cex = .7, right.labels = round(latent2, 2),
               bg = "red", main = "Original Hyperpriors")
centipede.plot(centipede.h, vgrid = c(-1,0,1,2), cex = .7, right.labels = round(latent.h, 2),
                bg = "red", main = "Adjusted Hyperpriors")
cor(latent2, latent.h)
cor(sd.h^2, sd2^2)

@

<<journal_simulation_study_data_optim_h, eval = T, echo = F, warning = F, message = F, cache = T, results = "hide">>=


trials = 100; tol = .01; #tried adding precision but that didn't really change anything
D = 2; N = nrow(Cmatrix)
fits.oh = list()
Z_dist = dist2(t(Cmatrix))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
N = 47; d = 2

sender.var.df = 2*3 #default 3 ** shrink sender range.  higher df condenses
receiver.var.df  = 2*3 #default 3 ** shrink receiver range
Z.var.df = 2*sqrt(N)  #default sqrt(N) ** shrink z range

prior.sender.var = .5 #default 1. higher var moves mode from 0
prior.receiver.var = .5
prior.Z.var = N/32 #default N/8 ** shrink sigma_z range
beta.var = 1 #default 9

t1.fits.oh = Sys.time()
for(i in 1:trials) {
  s = simulate(model$model, par = model$mkl)
  a = rnorm(N) 
  b = rnorm(N) 
  q  = ergmm(s ~ euclidean(d = 2) + rsender + rreceiver,
                                response = "citations", family = "Poisson.log", seed = 30,
                                tofit = c("mle"),
                                prior = ergmm.prior(sender.var.df = sender.var.df,
                                        receiver.var.df = receiver.var.df,
                                        Z.var.df = Z.var.df,
                                        beta.var = beta.var),
                                user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                  sender.var = var(a), receiver.var = var(b),
                                                  Z.var = var(as.vector(Z))))
  fits.oh[[i]] = q
}
t2.fits.oh = Sys.time()

@

<<journal_simulation_study_plots_h, eval = F, echo = F, warning = F, dependson = c("journal_simulation_study_data", "journal_simulation_study_data_h", cache = T)>>=

library(ggplot2)
# Not a big difference with these hyperprior adjustments. MAP estimates have slight shrinkage, maybe .1 total,
# 
# MAP - adjusted hyperpriors ####
# data ####
data.oh = data.frame(journal = rep(rownames(Cmatrix), length(fits.oh)),
                   sender = unlist(lapply(fits.oh, function(x) {x$start$sender - mean(x$start$sender)})),
                   receiver = unlist(lapply(fits.oh, function(x) {x$start$receiver - mean(x$start$receiver)})),
                   sender_diff = unlist(lapply(fits.oh, function(x) {x$start$sender - mean(x$start$sender) - 
                                                (model$mkl$sender - mean(model$mkl$sender))})),
                   receiver_diff = unlist(lapply(fits.oh, function(x) {x$start$receiver - mean(x$start$receiver) - 
                                                (model$mkl$receiver - mean(model$mkl$receiver))})),
                   beta_diff = rep(unlist(lapply(fits.oh, function(x) {x$start$beta - model$mkl$beta})), each = 47)
                   )

# 1. Receiver - sender to show ratings changes ####
fits.oh.rs = ggplot(data.oh, aes(x = journal, y = receiver_diff - sender_diff, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_abline(slope = 0, intercept = 0, col = "gray") + 
  ggtitle("Difference in rating") + ylim(-1,1) +
  xlab(NULL) + ylab(NULL)

# 2. Sender + receiver + beta to show variation with positions ####
fits.oh.rsb = ggplot(data.oh, aes(x = journal, y = sender_diff + receiver_diff + beta_diff, color = journal)) +
  geom_boxplot() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_abline(slope = 0, intercept = 0, col = "gray") +
  ggtitle("Difference in total") +
  xlab(NULL) + ylab(NULL)

cowplot::plot_grid(fits.oh.rs, fits.oh.rsb, nrow = 2)

@

As with any Bayesian model we must consider the sensitivity of estimates to the assumptions contained in the prior structure and hyperprior values. The near-identical ratings between MLE and MKL estimates suggest that the diffuse priors do not impact the ratings. Still, we fit a model with adjustments to the hyperprior degrees of freedom and variance which narrow the prior variance distributions to more realistic ranges based on previous results. 
%The middle 90 percent intervals drop to roughly $[0.2, 1.8]$ instead of $[0.4, 8.5]$ for sender and receiver variances, and $[0.9, 3.2]$ instead of $[2.9, 20]$ for position variance.
The effect on the results is trivial, both in the ratings and their standard errors. In some cases it may be valuable to limit the spread of position estimates with stronger priors, but unless they restrict beyond likely observable ranges the impact on ratings should be minimal.  %cor (.9999, .9960)

%%The average computational journal as categorized by \citet{varinetal} has 419 out-citation in the network while the average ecological journal has only 115. A covariate could be added to the model to augment the expected counts within or between subfields. The value of journals that cite heavily outside of statistics, such as the \textit{Stata Journal} which cites many economic journals, could also be downweighted.


\section{Movie rating and genre identification}\label{Example2}

In this section we apply latent space rating to a set of films with viewer-supplied star ratings. Film rating systems based on viewer ratings are biased by the fact that viewers do not choose movies to rate randomly, and they may skew high or low in their ratings. In addition, average ratings obscure the information provided by the volume of ratings for different films. The latent space rating method efficiently addresses those challenges. The concomitant visualizations aid in genre detection and enhance our ability to explore and compare related films.  Through a dynamic network plot we increase the amount of data we can present. Movie ratings are extremely subjective, so without knowing detailed individual preferences the ability to explore a network of films may be more useful than finding ``correct'' ratings. 

\subsection{Data}\label{Data2}

Our data came from the MovieLens data set \citep{movielens}, collected by the GroupLens Research Project at the University of Minnesota. It consisted of about one million ratings of 3,952 movies from approximately 6,000 users who joined MovieLens in 2000. The data were released in 2003. Ratings were on a one-to-five integer (star) scale with five being the best. Each included user supplied at least 20 ratings.

To convert the data into a network format we aggregated differences in individual users' ratings to form a $3952 \times 3952$ ratings-difference matrix. To be explicit, entry $i, j$ in the matrix represented the sum of positive values of $rating(j) - rating(i)$ over users who rated both movies. (If $rating(i) > rating(j)$ the difference was added to entry $j,i$.) For example, if a user prefers movie $j$ to $i$ by one point, then one is added to entry $i, j$: $i$ ``sends'' a point to $j$. The corresponding network is positive-valued and directed. 
%note the data isn't quite counts, as the differences range between 1 and 4, but most are 1 or 2. Overdispersed?

To illustrate certain points without too much computational burden we restricted ourselves to a subset of the MovieLens data, retaining only movies assigned genre ``Action'', ``Crime'', ``Western'' or some combination therein. (Only two of the possible combinations are present in the data.) We removed a small number of isolates before modelling. The resulting network has 128 nodes and 11,393 edges.

\subsection{Latent space model}\label{Model2}

%full movie net is saved as movie_net.rds
<<r movie_data, eval = T, echo = F, cache = T>>=

movie = readRDS("~/Documents/citation/latent_ranking/movie_output.RDS")
movie_net_acw = movie$movie_net_acw
movie_net_acw_full = movie$movie_net_acw_full
movie_net_watch_full = movie$movie_net_watch_full
vc.acw = movie$vc.acw
latent_acw2 = movie$latent_acw2

acw_mat = as.sociomatrix(movie_net_acw_full, attrname = "ratings_diff", ignore.eval = F)
rownames(acw_mat) = movie_net_acw_full%v%"titles"
@

<<r movie_model, cache = T, eval = T, echo = F, results = 'hide', highlight = F, warning = F>>=

t1.acw2 = Sys.time()
latent_acw2 = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender, 
             response = "ratings_diff", family = "Poisson.log",
             control = control.ergmm(pilot.runs = 4,  burnin = 500000, 
             interval = 500, sample.size = 5000), seed = 123)
t2.acw2 = Sys.time()

@

% Settings too low, mcmc diagnostics bad
<<r movie_model_b, cache = T, eval = F, echo = F, results = 'hide', highlight = F, warning = F>>=

#try shorter burnin
t1.acw2b = Sys.time()
latent_acw2b = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender, 
             response = "ratings_diff", family = "Poisson.log",
             control = control.ergmm(pilot.runs = 8,  burnin = 100000, 
             interval = 100, sample.size = 1000), seed = 123)
t2.acw2b = Sys.time()

@

%even with a very long burning (5mill, took about 6hrs with interval 500, sample 10000) the final map only increases by a few over initial direct optimization

<<r movie_model_r, eval = T, echo = F, results = 'hide', highlight = F, cache = T, dependson = c("movie_data"), warning = F>>=

M = acw_mat
N = nrow(M); n = N; D= 2; d = 2; 
v_z = sqrt(N); s2_z = N/16
Z_dist = dist2(M)
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
sigma2_z = var(as.vector(Z))
a = rnorm(N)
b = rnorm(N)
sigma2_a = var(a)
sigma2_b = var(b)

t1.acw2.r = Sys.time()
latent_acw2.r = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender,
                      response = "ratings_diff", family = "Poisson.log",
                      control = control.ergmm(pilot.runs = 8,  burnin = 500000,
                      interval = 500, sample.size = 5000), seed = 123,
                      user.start = list(Z = Z, sender = a, receiver = b, 
                                        sender.var = sigma2_a,
                                        receiver.var = sigma2_b,
                                        Z.var = sigma2_z))
t2.acw2.r = Sys.time()
@

<<r movie_model_init_qn, eval = T, echo = F, results = 'hide', highlight = F, cache = T, dependson = c("movie_data"), warning = F>>=

t1.acw2.init.qn = Sys.time()
latent_acw2.init.qn = ergmm(movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender,
                      response = "ratings_diff", family = "Poisson.log",
                      control = control.ergmm(pilot.runs = 8,  burnin = 500000,
                      interval = 500, sample.size = 5000), seed = 123,
                      user.start = with(acw.qn.h$map, list(Z=Z, sender=sender, receiver=receiver,
                                             beta = beta, sender.var = sender.var,
                                             receiver.var = receiver.var, Z.var = Z.var))
                      )
t2.acw2.init.qn = Sys.time()
@

<<r movie_model_h, eval = T, echo = F, results = 'hide', highlight = F, cache = T, dependson = c("movie_data"), warning = F>>=

#adjustment to hyperpriors from default

t1.acw2.h = Sys.time()
latent_acw2.h = ergmm(  movie_net_acw_full ~ euclidean(d=2) + rreceiver + rsender, 
                        response = "ratings_diff", family = "Poisson.log", seed = 123,
             
                        prior = ergmm.prior(sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                        sender.var = 1,
                                        receiver.var = 1, 
                                        Z.var = 1,
                                        beta.var = 3),
                        
                        control = control.ergmm(pilot.runs = 8,  burnin = 500000, 
                        interval = 500, sample.size = 5000, mle.maxit = 100)

                       )
  
t2.acw2.h = Sys.time()


@

%for comparison tried without positions and all high-count films get pulled to the center, as expected
<<movie_model_noRE, eval = F, echo = F, warning = F, message = F>>=
M = acw_mat
N = nrow(M); n = N; D= 2; d = 2; 
v_z = sqrt(N); s2_z = N/16
Z_dist = dist2(M)
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
sigma2_z = var(as.vector(Z))

t1 = Sys.time()
latent_acw2.noRE = ergmm(movie_net_acw_full ~ euclidean(d=2),
                      response = "ratings_diff", family = "Poisson.log",
                      control = control.ergmm(pilot.runs = 8,  burnin = 500000,
                      interval = 500, sample.size = 5000), seed = 123,
                      user.start = list(Z = Z, Z.var = sigma2_z), verbose = 3)
t2 = Sys.time()
@

<<r movie_model_eval, eval = T, echo = F, message = F, warning = F, dependson = ("movie_data")>>=
library(latentnet)

#mcmc.diagnostics(latent_acw2)
#mcmc.diagnostics(latent_acw2b)
#mcmc.diagnostics(latent_acw2.r)
#mcmc.diagnostics(latent_acw2.h)

#par(mfrow = c(1,2))
#plot(latent_acw2$mkl$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16)
#plot(latent_acw2.h$mkl$Z, col = as.factor(movie_net_acw_full%v%"genres"), pch = 16)

@

By modelling the network of differences in ratings we capture the tendency for a movie to be frequently and consistently rated above movies that draw an overlapping audience. We apply the latent space Poisson model in two dimensions to facilitate visualization, which is of greater interest in this application than ratings precision. Results are presented in Table \ref{movie.table}. The methods presented in the first three rows are the same as in Table \ref{results.table}. The MCMC sampling parameters are also the same as above. We tried increasing them for this larger network, but found it not to be necessary. The MKL estimates are again the best of the estimates produced by the MCMC estimation. For comparison, we include in rows four and five the MLE and posterior mode estimated from the MCMC sample. Although the fit from MCMC estimation is not sensitive to initialization method in this example, %default, random, and quasi-Newton initialization all about the same
the time is somewhat affected. In addition, the quasi-Netwon method achieves better results in less time, about two minutes instead of ten, when using random initialization.

As above we consider adjustments to the hyperpriors that narrow the range of prior variance distributions and find the impact on ratings to be trivial (row 6). However, the hyperprior adjustments do benefit the visualization by limiting the amount that very low-connectivity nodes drift from the rest. These results are subsequently used for visualization and clustering analysis. For consistency, the calculations in columns three and four of Table \ref{movie.table} all assume the more diffuse priors.

<<r movie_table, echo = F, results = "asis", dependson = c("movie_model", "movie_model_r", "movie_model_qn", "movie_model_h")>>=
library(xtable)

movie.table = data.frame(row.names = c("QN (L-BFGS-B)",
                                       "MCMC (MKL)",
                                       "MCMC.init.r (MKL)",
                                       "MCMC.init.r (MLE)",
                                       "MCMC.init.r (P.mode)",
                                       "MCMC.h (MKL)"))
movie.table$"Time (min)" = c("2",  
                             as.character(round(difftime(t2.acw2, t1.acw2, units = "mins"))), 
                             as.character(round(difftime(t2.acw2.r, t1.acw2.r, units = "mins"))), "-", "-",
                             as.character(round(difftime(t2.acw2.h, t1.acw2.h, units = "mins"))))
movie.table$"$l(Y|\\hat{\\theta})$" = as.character(round(
                                c(
                                llik(latent_acw2.r$burnin.start, Y = acw_mat, est = "Y"),
                                llik(latent_acw2$mkl, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.r$mkl, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = "Y"),
                                llik(latent_acw2.h$mkl, Y = acw_mat, est = "Y"))))

movie.table$"$l(\\hat{\\theta})$" = as.character(round(
                                c(
                                llik(latent_acw2.r$burnin.start, Y = acw_mat, est = "theta"),
                                with(latent_acw2$mcmc.pmode, llik(latent_acw2$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")),
                                with(latent_acw2.r$mcmc.pmode, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")),
                                llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = "theta"),
                                llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = "theta"),
                                with(latent_acw2.r$mcmc.pmode, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta")))))

movie.table$"$l(\\hat{\\theta}|Y)$" = as.character(as.numeric(movie.table$"$l(Y|\\hat{\\theta})$") + as.numeric(movie.table$"$l(\\hat{\\theta})$"))

acw2 = latent_acw2.r$mkl$receiver - latent_acw2.r$mkl$sender
movie.table$"Rating Cor." = c(
                               cor(acw2, latent_acw2$burnin.start$receiver - 
                                            latent_acw2$burnin.start$sender), 
                               cor(acw2, latent_acw2$mkl$receiver - latent_acw2$mkl$sender),
                               cor(acw2, latent_acw2.r$mkl$receiver - 
                                            latent_acw2.r$mkl$sender),
                               cor(acw2, latent_acw2$mcmc.mle$receiver - latent_acw2$mcmc.mle$sender),
                               cor(acw2, latent_acw2$mcmc.pmode$receiver - latent_acw2$mcmc.pmode$sender),
                               cor(acw2, latent_acw2.h$mkl$receiver - 
                                            latent_acw2.h$mkl$sender)
                             )

print(xtable(movie.table, caption = "Comparison of stimation methods. The $\\ell$ function is the log of the probability.", digits = 4, label = "movie.table"), caption.placement = "top", sanitize.text.function=function(x){x})
@

<<r movie_results_likelihoods, eval = F, echo = F, results = "hide">>=

cor(latent_acw2.r$mkl$receiver - latent_acw2.r$mkl$sender, latent_acw2.h$mkl$receiver - latent_acw2.h$mkl$sender)
est = "Y"
llik(acw.qn$map, Y = acw_mat, est = est)
llik(acw.qn.h$map, Y = acw_mat, est = est)

llik(latent_acw2$burnin.start, Y = acw_mat, est = est) 
llik(latent_acw2.r$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.h$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$burnin.start, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.mle, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.pmode, Y = acw_mat, est = est)

llik(latent_acw2$mkl, Y = acw_mat, est = est) #second best
llik(latent_acw2.r$mkl, Y = acw_mat, est = est)
llik(latent_acw2.h$mkl, Y = acw_mat, est = est) #best
llik(latent_acw2.init.qn$mkl, Y = acw_mat, est = est)

est = "MAP"
llik(acw.qn$map, Y = acw_mat, est = est)
llik(acw.qn.h$map, Y = acw_mat, est = est)

llik(latent_acw2$burnin.start, Y = acw_mat, est = est) #worst
llik(latent_acw2.r$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.h$burnin.start, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$burnin.start, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.mle, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.mle, Y = acw_mat, est = est)

llik(latent_acw2$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.h$mcmc.pmode, Y = acw_mat, est = est)
llik(latent_acw2.init.qn$mcmc.pmode, Y = acw_mat, est = est)

with(latent_acw2$mcmc.pmode, llik(latent_acw2$mkl, Y = acw_mat, est = est, #best
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))
with(latent_acw2.r$mcmc.pmode, llik(latent_acw2$mkl, Y = acw_mat, est = est,
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))
with(latent_acw2.h$mcmc.pmode, llik(latent_acw2.h$mkl, Y = acw_mat, est = est,
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))
with(latent_acw2.init.qn$mcmc.pmode, llik(latent_acw2.init.qn$mkl, Y = acw_mat, est = est,
     sender.var = sender.var, receiver.var = receiver.var, Z.var = Z.var))

#compare p(theta) under adjusted hyperpriors
tmp = as.character(round(
                                c(llik(acw.qn.h$map, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                        prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                llik(latent_acw2.r$burnin.start, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                               prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                with(latent_acw2$mcmc.mle, llik(latent_acw2$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                               prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3)),
                                llik(latent_acw2.r$mcmc.mle, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                             prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                llik(latent_acw2.r$mcmc.pmode, Y = acw_mat, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                              prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3),
                                with(latent_acw2.r$mcmc.mle, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                                prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3)),
                                with(latent_acw2.r$mcmc.mle, llik(latent_acw2.r$mkl,
                                     Y = acw_mat, sender.var = sender.var,
                                     receiver.var = receiver.var, Z.var = Z.var, est = "theta", sender.var.df = 6,
                                        receiver.var.df = 6,
                                        Z.var.df = sqrt(nrow(acw_mat)/4),
                                               prior.sender.var = 1,
                                        prior.receiver.var = 1, 
                                        prior.Z.var = 1,
                                        beta.var = 3)))))

@
% larger movie network

<< latent_watch_init_r, eval = F, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=

W = as.sociomatrix(movie_net_watch_full, ignore.eval = F, attrname = "ratings_diff")
#random initation of sender and receiver, and scaling Z helps the model fit
D = 2; N = nrow(W)
Z_dist = dist2(t(W))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

#I tried this with 50k, 500, 5k and the sampling step took >  24 hours, stopped it
#mle.maxit 50 is probably enough

latent.watch.init.r = ergmm(movie_net_watch_full ~ euclidean(d = 2) + rsender + rreceiver,
                              response = "ratings_diff", family = "Poisson.log", seed = 30,
                              tofit = c("mcmc", "mkl", "procrustes", "mle"),
                              control = ergmm.control(burnin = 50000, interval = 50,
                                                      sample.size = 1000,
                                                      mle.maxit = 100),
                              user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                sender.var = var(a),
                                                receiver.var = var(b),
                                                Z.var = var(as.vector(Z))))
@

<< latent_watch_mle, eval = F, results = 'hide', echo = F, cache=T, cache.comments = FALSE, warning= FALSE>>=

W = as.sociomatrix(movie_net_watch_full, ignore.eval = F, attrname = "ratings_diff")
#random initation of sender and receiver, and scaling Z helps the model fit
D = 2; N = nrow(W)
Z_dist = dist2(t(W))
Z = cmdscale(Z_dist, k = D)
Z = Z/max(abs(Z))
a = rnorm(N) #rep(0, N) # #sender
b = rnorm(N) #rep(0, N) # #receiver

#default hyperprior, r.init ####
t1.latent.watch.mle = Sys.time() #about 75 minutes for optim to find mpe - could lower mle.maxit
latent.watch.mle = ergmm(movie_net_watch_full ~ euclidean(d = 2) + rsender + rreceiver,
                              response = "ratings_diff", family = "Poisson.log", seed = 30,
                              tofit = c("mle"),
                              control = ergmm.control(mle.maxit = 200),
                              user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                sender.var = var(a),
                                                receiver.var = var(b),
                                                Z.var = var(as.vector(Z))), verbose = 3)
t2.latent.watch.mle = Sys.time()
llik(latent.watch.mle$start, Y = W, est = "Y")

#hyperprior adjustment, r.init ####
t1.latent.watch.mle.h = Sys.time() #about 45 minutes with 100 maxit
latent.watch.mle.h = ergmm(movie_net_watch_full ~ euclidean(d = 2) + rsender + rreceiver,
                              response = "ratings_diff", family = "Poisson.log", seed = 30,
                              tofit = c("mle"),
                              control = ergmm.control(mle.maxit = 100),
                              user.start = list(Z = Z, sender = a, receiver = b, beta = 0,
                                                sender.var = var(a),
                                                receiver.var = var(b),
                                                Z.var = var(as.vector(Z))),
                                 prior = ergmm.prior(sender.var.df = 3,
                                        receiver.var.df = 3,
                                        Z.var.df = sqrt(N/8),
                                        sender.var = 1,
                                        receiver.var = 1, 
                                        Z.var = 1,
                                        beta.var = 3),verbose = 3)
t2.latent.watch.mle.h = Sys.time()
llik(latent.watch.mle.h$start, Y = W, est = "Y")
@

There is a strong correlation of $0.95$ (or $0.976$ when weighted by the log of co-review counts) between a film's average star rating and its ratings from the latent space model.
%?($x$ unweighted, $x$ when weighted by the logged counts of ratings).
Figure \ref{fig:r movie_compare} shows this correlation, plotting the latent space scores against the average star ratings and labeling points by review counts for each film. Although not pictured, the standard errors in the ratings are fairly consistent. They range from 0.13 to 0.26 (95th percentile) with a median of 0.14. The high-end outliers have very few co-review counts, and only points with few co-reviews deviate strongly from the overall linear trend. We take a closer look at a couple of the films that deviate from the trend but have more than $20$ reviews, highlighted in green. The corresponding films are, from left to right, \textit{Shaft in Africa (1973)} and \textit{Assassination (1987)}. These films have a potential ``cult'' following based on their lead actors, Richard Roundtree (as Shaft) and Charles Bronson respectively. That may boost some of the user reviews, but when comparative reviews are considered in the latent space model the derived rating is lower.
%also #79 with 32 reviews, Wanted: Dead or Alice (1987). Based on a three-season television show that launched steve mcqueen's career. 

<<r movie_compare, eval = T, echo = F, dependson = c("movie_data", "setup"), cache = T, message = FALSE, warning = FALSE, fig.width = 5, fig.height = 3, fig.align="center", results = "hide", fig.cap= "Latent space model scores vs. average star ratings. The plotting characters are the review counts for each film. The red points are strong outliers to the linear trend, have very few reviews, and are excluded in calculating the best fit line. The green points are discussed in the paper." >>=

library(ggplot2)

par(mai = c(.8,.8,.5,.8))
stars1 = movie_net_acw_full%v%"stars"
vc.acw = latent_acw2.h$mkl$receiver - latent_acw2.h$mkl$sender
# highlight deviant - include 79 too? Wanted dead or alive is the other 32 count
col1 = rep("black", length(stars1)); col1[c(78, 122)] = "green"; col1[ (movie_net_acw_full%v%"counts") < 5] = "red"
data1 = data.frame(stars = stars1, rating = vc.acw,
                   counts = movie_net_acw_full%v%"counts",
                   color = col1)
# to add trend line without outliers
x <- which(movie_net_acw_full%v%"counts"<5)
lm1 = lm(vc.acw[-x] ~ stars1[-x])

ggplot(data1, aes(x = stars, y = rating, color = color)) + 
  geom_abline(slope = lm1$coefficients[2], intercept = lm1$coefficients[1], col = "gray") +
  geom_text(aes(label=counts), size = 2, angle = -60) +
  theme_bw() +
  ggtitle("Ratings compared") +
  xlab("average star rating") + 
  ylab("2-D latent space rating") +
  theme(legend.position = "none", plot.title = element_text(size = 12)) +
  scale_colour_manual(values = c("black", "green", "red"))

#check unweighted and weighted correlations
cor(stars1 , vc.acw) #.942
#weighted correlation even higher
boot:::corr(cbind(stars1, vc.acw),
     w = log((movie_net_acw_full%v%"counts"))) #.975

@

<<r movie_uncertainty, eval = F, cache = T, echo = F, results = "hide">>=
movie_sd = apply(latent_acw2.h$sample$receiver - latent_acw2.h$sample$sender, 2, sd)
quantile(movie_sd, probs = c(0,.25,.5,.75,.90, .95, .97, 1))
(movie_net_acw_full%v%"counts")[which(movie_sd > .28)]
sd2[40]
@

It is somewhat surprising to find such a strong correlation between the two ratings methods pictured. This may be due in part to homogeneity in the MovieLens reviewer pool, which reduces the sources of bias discussed above. MovieLens was organized by a university and all reviewers in our data joined in 2000. The top four occupations of the reviewers are (in decreasing order): college/grad student, other/not specified, executive/managerial and academic/educator. This is certainly not representative of the population at large. The impact of our difference-based network model as compared to average star ratings may be more evident when reviewers are more heterogeneous. 

%Because the reviews are strongly correlated - almost all within a third of a point so overall impression the same - and we have no ground truth to compare to
We next consider the additional insight gained through latent space model positions and visualization. An interactive plot of the model output is included as an html file in the supplementary material, and available on the author's website (\url{http://www.stat.ucla.edu/~jane.carlen/pages/movie_net.html}). The nodes are colored by genre with size scaled to latent space model rating. For comparison, the average star ratings are listed next to the movie titles in the drop-down menu and when hovering over a node. For clear visualization, the nodes are limited to displaying their (at most) seven strongest out-edges. In-degree is not limited in the plot.
%An edge's arrow indicates the direction from lower to higher-rated film.

<<r movie_plot_setup, eval = T, echo = F, results = "hide", dependson = c("movie_model_r", "movie_model_h")>>=
write.csv(latent_acw2.h$mkl$Z, "~/Documents/citation/latent_ranking/movie_net_acwh_Z.csv")
write.csv(latent_acw2.r$mkl$Z, "~/Documents/citation/latent_ranking/movie_net_acwr_Z.csv")
@

<<r movie_plot, eval = F, echo = F, fig.keep = "none", dependson = c("movie_model_r", "movie_model_h")>>=

#not evaluated here. stored as movie_net.html and movie_neth.html
Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwh_Z.csv")[,2:3])

#to align with other picture
Z = -Z[,2:1]
movie_net = movie_net_acw #to see all edges use movie_net_acw_full
  
nodes <- data.frame(id = 1:length(movie_net%v%"titles"), 
                    label = "",
                    #label = movie_net_acw%v%"vertex.names",
                    title = paste(movie_net%v%"titles", round(movie_net%v%"stars",2)), #for selection dropdown
                    value = vc.acw, 
                    group = movie_net%v%"genres") #conveys node size, on a scale from 0-1

edges <- data.frame(from=data.frame(as.edgelist(movie_net))$X1, 
                    to=data.frame(as.edgelist(movie_net))$X2)
                    #value = movie_net_acw%e%"avg_diff")

acw_igraph <- igraph::graph.data.frame(edges, directed=TRUE, vertices=nodes)
  
visNetwork(nodes, edges, main = "Movie Network", submain = "latent space positions") %>%
  visIgraphLayout(acw_igraph, layout = "layout.norm",
                  layoutMatrix = Z, type = "full") %>%
  #visOptions(highlightNearest = list(enabled =TRUE, degree = 1)) %>%
  visNodes(#color = list(highlight = list(background = "black")),
           shape = "dot", #shape = "text"
           scaling = list(min = 5, max = 30, label = list(max=20)),
           labelHighlightBold = TRUE,
           borderWidth = .5,
           borderWidthSelected = 2,
           font= list(size = 20, color = "black", strokeWidth = 1)) %>%
  visEdges(shadow = FALSE,
           scaling = list(min = 1, max = 10),
           #selectionWidth = "function(x) {acw_avg_diff}",
           hidden = FALSE,
           #scaling = list(min = 1, max = 10),
           arrows = list(to = list(enabled = FALSE, scaleFactor = 5),
                         middle = list(enabled = TRUE, scaleFactor = 1)),
           color = list(inherit = TRUE, highlight = "red", opacity = 0.04), #, color = "white"
           smooth = list(type = "curvedCW")) %>%
  visOptions(highlightNearest = list(enabled = T, degree = 0.5),
             nodesIdSelection = FALSE, 
             selectedBy = "title") %>%
  visLegend(enabled = TRUE, useGroups=TRUE) %>%
  visInteraction(selectConnectedEdges = TRUE)

@

<<r movie_cloud, eval = F, echo = F, fig.height = 6, cache = T, fig.cap = "Uncertainty in estimated film positions illustrated by a sample of positions from the model posterior distribution. Colouring is due to pre-assigned genre labels.">>=
####  cloud/uncertainty plot  ####
n = nrow(acw_mat)
N = 1000
p = latent_acw2.h[["sample"]][["Z"]]
s = sample(5000, N)

#null plot:
plot(latent_acw2.h$mkl$Z, xlab = NA, ylab = NA, type ="n",
           bty="n", yaxt="n", xaxt="n",  main = NA,
           xlim = c(-4,5), ylim = c(-4,4))

legend("topright", pch = 16, col = rainbow(5, alpha = 1)[c(4,3,5,2,1)], legend = levels(as.factor(movie_net_acw_full%v%"genres")), bty = "n", legend.ce)

#add points
for (i in 1:N) {
  points(p[s[i],,], col = (rainbow(5, alpha = 1)[c(4,3,5,2,1)])[as.numeric(as.factor(movie_net_acw_full%v%"genres"))], pch = 15, cex = .2)
}
@

\subsection{Genre detection} \label{genres}
The visualizations shows that movies clearly cluster by genre even though genre was not a term in the model. Movies with hybrid genres are placed roughly between their two component genres. However, there are a few films that reside outside of their genre cluster. In those cases the plot can highlight incomplete or incorrect classification. For example, the film \textit{Coogan's Bluff (1968)} is categorized as a crime film by MovieLens, but its latent position is among actions and westerns. The Internet Movie Database (\textit{IMDb}) entry for this film describes it as ``An Arizona deputy goes to New York City to escort a fugitive back into custody,'' and the lead role is played by action/crime/western star Clint Eastwood \citep{coogan}. This film has heavy action and western influences which its latent position reveals. Another example of misclassification is the two ``action'' films \textit{The Kid (1921)} and \textit{Minnie and Moskowitz (1971)}, positioned between crimes and westerns. Their genres listed on \citet{imdb} are comedy/drama/family, and comedy/drama/romance, respectively. Neither is well-classified as an action film, and their positions close together and outside the action cluster reflect this, and also recognize their similarity.

The continuous positions from the latent space model are more precise identifiers of movie type than discrete cluster labels. Like the journal citation network visualized in Section \ref{Visualization}, the clusters are discernible but irregularly shaped and blend into each other. The positions aid in identifying sub-genres, which may be valuable for recommendation systems. For example, the westerns that tail into the crime cluster, \textit{Unforgiven (1992)}, \textit{Tombstone(1993)}, and \textit{Dead Man (1995)} reveal a "modern western" subgenre. These films are much newer than most westerns in the data, with median year 1968.

If no genre labels were given the fit positions provide rich input for a clustering algorithm. To illustrate, we apply k-means clustering with three clusters, which we expect to correspond roughly to the action, crime and western films (\citealt{kmeans}, \citealt{r}). We use pair-counting to measure accuracy, with the caveat that not all pre-assigned genres are correct, as discussed above. Films originally labeled with two genres are considered a match if assigned to either of those genres. Table \ref{class.table} compares the k-means output to pre-assigned genre labels. Of the 128 films in the network, 105 are assigned to matching labels. The most common change in classification is from action to crime. The comparison is visualized in Figure \ref{fig:movie_kmeans}. Most films that switch labels are positioned near the boundary of the three classes, indicating that a single label is probably insufficient. To put these results in perspective, we implemented two popular community detection methods to the film matrix, a Louvain modularity maximization method and infomap, as implemented in \pkg{igraph} \citep{igraph}. Both are unsupervised and require undirected networks, so we input the symmetric matrix of dyad totals. The Louvain method returned three communities, one that corresponded to westerns, one to both action and crime films, and a smaller third one to action films in the center of the plot. There were 38 total misclassifications. Infomap returned only one community, highlighting the lack of distinct divisions between genres that latent positions are able to capture. 

<<movie_kmeans, cache = T, echo = F, fig.height = 3, fig.width = 6, fig.align = "center", results = "asis", fig.cap = "Film network colored by k-means class with shape determined by pre-assigned genre. Centers of the k-means classes are labeled 1-3.", dependson = c("movie_model_r"), message = F>>=

library(stringr)

#as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwr_Z.csv")[,2:3])
fit.Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwr_Z.csv")[,2:3])
G = 3
fit.Kr = kmeans(fit.Z, centers =  G) #no variation in fit among the algorithms
data1 = data.frame(x = -fit.Z[,1], y = fit.Z[,2], kmeans_class = as.factor(fit.Kr$cluster), assigned_genre = as.factor(movie_net_acw_full%v%"genres"))
data2 = data.frame(x = fit.Kr$centers[,1], y = fit.Kr$centers[,2])
kmeans_r = ggplot(data1, aes(x = x, y = y, color = kmeans_class, shape = assigned_genre)) +
  geom_point(size= 3) +
    geom_point(data = data2, aes(x, y), inherit.aes = F, shape = c("1","2","3"), size = 3) + 
  scale_color_manual(values = rainbow(G, alpha = .7)) +
  theme_void()# + xlim(-3,3) + ylim(-2,2)

fit.Z = as.matrix(read.csv("~/Documents/citation/latent_ranking/movie_net_acwh_Z.csv")[,2:3])
G = 3
fit.Kh = kmeans(fit.Z, centers =  G) #no variation in fit among the algorithms
data1 = data.frame(x = fit.Z[,1], y = fit.Z[,2], kmeans_class = as.factor(fit.Kh$cluster), assigned_genre = as.factor(movie_net_acw_full%v%"genres"))
names(data1)[3:4] = c("k-means class", "assigned genre")
data2 = data.frame(x = fit.Kh$centers[,1], y = fit.Kh$centers[,2])
kmeans_h = ggplot(data1, aes(x = x, y = y, color = `k-means class`, shape = `assigned genre`)) +
  geom_point(size= 3) +
    geom_point(data = data2, aes(x, y), inherit.aes = F, shape = c("1","2","3"), size = 5) + 
  scale_color_manual(values = rainbow(G, alpha = .7)[c(1,2,3)]) +
  theme_void() + 
  theme(legend.text = element_text(size = 10), legend.title = element_text(size = 10))
  #ggtitle("Comparison of k-means clusters to pre-assigned genres")

kmeans_h

# pair counting
# let Action|Crime match action or crime. let Action|Western match action or wester
# see which matches which
kmeans.labels = as.factor(fit.Kh$cluster)
assigned.group  = aggregate(fit.Z, by = list(movie_net_acw_full%v%"genres"), mean)[c(1,4,5),]
kmeans.group = aggregate(fit.Z, by = list(kmeans.labels), mean)
assigned.group = assigned.group[order(assigned.group[,1])] #inspect this visually
levels(kmeans.labels) = assigned.group$Group.1[rank(-kmeans.group$V1)]
#sum(is.na(str_match(kmeans.labels, movie_net_acw_full%v%"genres")))
#table((movie_net_acw_full%v%"genres")[is.na(str_match(kmeans.labels, movie_net_acw_full%v%"genres"))])
table1 = t(table(kmeans.labels, movie_net_acw_full%v%"genres"))
print(xtable(table1[,c(2,1,3)], caption = "Classification of films by pre-assigned genres (row) vs. k-means cluster (column)", digits = 0, label = "class.table"))

# westerns by year
west_years = sort(sapply((movie_net_acw_full%v%"titles")[movie_net_acw_full%v%"genres"=="Western"], str_extract, "[0-9]{4}"))
crime_years = sort(sapply((movie_net_acw_full%v%"titles")[movie_net_acw_full%v%"genres"=="Crime"], str_extract, "[0-9]{4}"))
action_years = sort(sapply((movie_net_acw_full%v%"titles")[movie_net_acw_full%v%"genres"=="Action"], str_extract, "[0-9]{4}+"))
#median(as.numeric(west_years)); median(as.numeric(crime_years)); median(as.numeric(action_years)); 

@

% vs. an out-of-the-box clustering method on the (directed) graph, which does worse:
<<movie_louvain, eval = F, echo = F, results = "hide">>=

# vs. an out-of-the-box clustering method on the (directed) graph

# louvain ("multi-level modularity optimization") ####
tmp = igraph::graph.adjacency(acw_mat+t(acw_mat), weighted=T, mode = "undirected")
tmp.cl = igraph::cluster_louvain(tmp) #undirected only
plot(latent_acw2.h$mkl$Z, col = tmp.cl$membership, pch = as.numeric(as.factor(movie_net_acw_full%v%"genres")))
legend("topright", cex = .7, pch = 1:5, legend = levels(as.factor(movie_net_acw_full%v%"genres")))

# this method groups most action and crime films as one group, recognizes the westerns, and has a third group of mostly action and action|crime that's between those two.

tmp.table = table(tmp.cl$membership, as.factor(movie_net_acw_full%v%"genres"))
#group (1) is western -> 3 western misclassified (column), 6 others (row) misclassified
#group (2) is action, -> 37 + 1 misclassified
#group (3) is Crime -> no additional
#if group 3 action instead, 20 + 19 + 1 +1 misclassified,  (worse)

# infomap ####
tmp = igraph::graph.adjacency(acw_mat+t(acw_mat), weighted=T, mode = "undirected")
tmp.cl = igraph::cluster_infomap(tmp) #undirected only
plot(latent_acw2.h$mkl$Z, col = tmp.cl$membership, pch = as.numeric(as.factor(movie_net_acw_full%v%"genres")))
legend("topright", cex = .7, pch = 1:5, legend = levels(as.factor(movie_net_acw_full%v%"genres")))

tmp.table = table(tmp.cl$membership, as.factor(movie_net_acw_full%v%"genres"))

#Note, methods such as cluster_edge_betweenness and cluster_infomap return one community (forcing three communities from cluster_edge_betweenness gives two with size 1) and clutser_leading_eigin and cluster_fast greedy return two

@

%I tried  applying two-group and three group cluster models in latentnet. 2-group is not good (two central clusters, one with larger variance). 3-group hasn't run successfully yet on the full network and the results on the thinned network weren't very helpful. I may need to use the genres as inital labels, but would lose the "initial label" benefit. 
<<group_models, eval = F, echo = F>>=
#Even with initial postions the cluster model returns error.
latent_acw3 = ergmm(movie_net_acw_full ~ euclidean(d=2, G = 3) + rreceiver + rsender, 
             response = "ratings_diff", family = "Poisson.log", tofit = "mle",
             prior = ergmm.prior(Z.k = fit.K$cluster, Z.mean = fit.K$centers,
                                 Z.var = rep(.5,3)),
             control = control.ergmm(pilot.runs = 4,  burnin = 500000, 
             interval = 500, sample.size = 5000), seed = 123, verbose = 3)
=plot(latent_acw3$burnin.start$mkl$Z)
@

<< pagerank_movie, eval = F, cache = T, echo = F, results = 'hide', dependson = c("setup", "setup_jrss", "pagerank", "movie_data"), fig.keep = 'none', message = F, warning = F>>=

# apply page rank ####
acw_igraph = igraph::graph.adjacency(acw_mat, weighted=T)
#acw_igraph = igraph::graph.adjacency(log(acw_mat+1), weighted=T) <- stronger correlation with starts if take log first
movie.page.rank = igraph::page.rank(acw_igraph, damping = .95)$vector

# compare results to other ranks ####

## points in
cor(movie.page.rank, colSums(movie_rating_acw)) #.845

## to cited/citing ratio (cited is points in)
cor(movie.page.rank, colSums(movie_rating_acw)/rowSums(movie_rating_acw), use = "complete.obs") #.871 - better when normalized

## number of ratings
cor(movie.page.rank, movie_net_acw%v%"counts") #.64
plot(movie.page.rank, movie_net_acw%v%"counts", pch = "x")
cor(movie.page.rank, movie_net_acw%v%"stars") #.64
plot(movie.page.rank, movie_net_acw%v%"stars", pch = "x")
#text(movie.page.rank, movie_net_acw%v%"stars", labels = movie_net_acw%v%"titles", cex = .5, srt = 90)

### latent ranking scores
cor(movie.page.rank, vc.acw) #.66
plot(movie.page.rank, vc.acw, type = "n")
text(labels = movie_net_acw%v%"titles", movie.page.rank, vc.acw, cex = .5, srt = 90)

# takeaway: most page ranks extremely small (this is probably desiriable in web search results, but not here)
@

% collaborative filtering to generate individualized network plots?

\section{Discussion}\label{Discussion}

In this paper we have introduced a method for rating objects based on relational data using a latent space network model. Unlike standard network-based ranking methods such as PageRank, it is not a variant on a centrality measure. Instead, it weighs the relative in- and out-flow of an object while controlling for its position in space -- a position that is determined by its network ties. It is thereby an appropriate measure of an object's importance, influence or prestige within the network. In addition, the model provides estimates of uncertainty in the rankings that allow us to determine the significance of ranking differences. The quasi-Stigler model returns similar estimates of uncertainty in ratings, but as a network-generating model it underestimates uncertainty because it is conditioned on dyad totals.

The latent space model is also valuable for its meaningful visualization of network data, which facilitates user exploration of the network itself. One could use the dynamic plots we present to find journals to submit articles to or films to watch that are similar to a given one.
%i checked a standard plot and it's OK but not nearly as good:
%plot(movie_net_acw, vertex.col = as.numeric(as.factor(movie_net_acw%v%"genres")), cex = 1.3, edge.col ="grey")
While the latent space rankings are very similar to those of the quasi-Stigler model, the visualization helps us better understand what is driving each journal's rating. Furthermore, the visualizations allow for nuanced genre detection. Though our models do not include a term to capture genre, the latent space positions have proven to cluster in agreement with pre-assigned genres. In the case where labels are not provided, we showed that k-means clustering on the estimated positions can produce good results. In preliminary comparisons this technique has performed better than clustering algorithms on the raw data, but further work is needed to establish these results.

The latent space ranking method is especially advantageous when elements to be rated have some underlying, but difficult to measure, similarities. In our applications these were the statistics journals with one or more type (theoretical, computational, etc.) and films of one or more genre. However, the overall patterns of activity were fairly homogeneous. The journal set was restricted to a subset of a single field to be suitable for the quasi-Stigler model. Unfortunately, we are not currently able to obtain and present citation data for related fields of journals, e.g. economics, applied mathematics. A subject for future work is to extend the data and model to additional fields with different citation patterns. One general strategy to do this is to estimate clusters in the model and add covariates that capture the different levels of activity between them, or alter the random effects structure to have cluster-specific means. However, in the data we have considered the clusters are fuzzy and irregularly shaped, and the latent position cluster model of \citet{handcock07} fails to fit meaningful clusters. Our preferred avenue is to retain the current model structure but allow multidimensional sender and receiver coefficients, which capture the multi-faceted relationships in the network. This would build on the work of \citet{hoff09}, who proposed multiplicative latent factor models with multi-dimensional sender and receiver coefficients. Multidimensional random effects can also be visualized with the latent positions. 

% \section{Data Expansion} - ON HOLD, this would take a lot of data scraping work and/or consent/release from JCR. Varin et al don't have it, said the data they did share was hard to negotiate.
% - \citet{varinetal} Data comes from 2010 Web of Science published by Thomson Reuters. Want to expand to include all of Statistics and Probability (?) and Mathematics, Applied ( 253 journals) and Agricultural, Economics and Policy (? 17 journals)
% Note JCR can give related journals, e.g.  http://admin-apps.webofknowledge.com/JCR/JCR?RQ=RELATED_JOURNALS&rank=6&journal=CAN+J+AGR+ECON&query_new=true, and this could be another use of the network model

Another type of heterogeneity that may bias our results is due to nodes that are very poorly connected. We saw in the MovieLens example that films with very few reviews are most likely to deviate from an overall ratings correlation. One way to combat this is to make prior variance distributions less diffuse, which stabilizes the position estimates for low-connectivity nodes without sacrificing ratings estimates, in our experience. To further combat this we could incorporate a film's average star rating as a prior expectation for its receiver coefficient.
%while keeping the sender coefficients at prior mean zero
%Although this is not currently implemented in \pkg{latentnet} it could be incorporated in the future.

We have established that quasi-Newton estimation for the latent space rating model can perform as well as Bayesian MCMC estimation. The speed of quasi-Newton estimation enables us to fit larger networks than were practical with MCMC estimation. Although not discussed in the paper, we were able to fit an expanded film network of 510 nodes in roughly 45 minutes, whereas it would take over 24 hours with MCMC estimation. However, we still face the scaling problem of the number of edges growing $O(n^2)$. An area of future work is to integrate a local dependence structure into the model. For many large networks we expect that the positions and random effects for each node depend primarily on a neighborhood around the node and a few high-activity nodes with wide reach. This would reduce the storage space and number number of calculations needed for each update step, assuming we can develop a simple method for identifying the nodal neighborhoods. The number of edges to consider would grow only $O(n)$ in the case that mean degree were not an increasing function of graph size, which is a realistic assumption of many social networks. \citet{raftery12} took such an approach for their case-control approximate likelihood model, which they incorporated into MCMC estimation. By integrating a local dependence structure approach into quasi-Netwon estimation we could multiply the gains in speed. Finally, we note that many of the arguments in favor of quasi-Newton estimation apply to other direct optimization methods, and comparing competing optimization methods is a subject of future work. 

The variational Bayesian inference methods developed for latent space network models and implemented in the \pkg{R} package \pkg{VBLPCM} currently only apply to binary networks, not valued networks of the type discussed here. Given the speed of quasi-Newton estimation and uncertainty in the level of bias introduced by variational techniques we did not undertake to expand the variational methods to valued networks. Another line of work would be to implement that extension and compare the speed and results to the optimization methods described here.

\break

\newpage


\bibliographystyle{apacite}
\bibliography{latent_ranking}

\end{document}